{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RBCWealth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tweets_rbc = pd.read_csv('/Users/houhiroshisakai/Desktop/Schulich/Term 2/MBAN 6090/RBCwealth_since_2018_01_01.csv')\n",
    "\n",
    "texts_rbc = tweets_rbc['Tweet'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/houhiroshisakai/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/houhiroshisakai/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "#Define cleaning\n",
    "def clean_text(text):\n",
    "    text = str(text)\n",
    "    text = text.lower()  # convert to lower case\n",
    "    text = re.sub(r'\\d+', '', text)  # remove number\n",
    "    text = re.sub(r'http\\S+', '', text)  # remove url\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # remove punctuation mark\n",
    "    tokens = word_tokenize(text)  \n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]  # remove stop words\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "clean_texts_rbc = [clean_text(text) for text in texts_rbc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.09989678 0.1        1.09999893 ... 0.1000005  0.10002835 0.1       ]\n",
      " [0.10001541 0.10000643 0.1        ... 0.10002092 0.1        0.1       ]\n",
      " [0.1        1.09994308 0.1        ... 0.10000588 0.1        0.1       ]\n",
      " ...\n",
      " [1.09999845 2.74435988 1.10000107 ... 0.1        0.1        0.1       ]\n",
      " [2.01888095 0.1        0.1        ... 2.0999727  0.10000505 2.1       ]\n",
      " [1.10000726 2.10000735 0.1        ... 0.1        1.56236583 0.1       ]]\n"
     ]
    }
   ],
   "source": [
    "# Latent Dirichlet Allocation\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# ContextVectorize\n",
    "vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "X_rbc = vectorizer.fit_transform(clean_texts_rbc)\n",
    "\n",
    "# Apply LDA\n",
    "lda_rbc = LatentDirichletAllocation(n_components=10)\n",
    "lda_rbc.fit(X_rbc)\n",
    "\n",
    "print(lda_rbc.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names_rbc = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "global learn insight latest read investors weekly markets economic market\n",
      "Topic #1:\n",
      "rbc learn read support wealth jersey art mental partner delighted\n",
      "Topic #2:\n",
      "learn wealth business financial read family rbc women make management\n",
      "Topic #3:\n",
      "learn watch help wealthy barber financial chilton david executor women\n",
      "Topic #4:\n",
      "learn financial plan help impact make read aging retirement time\n",
      "Topic #5:\n",
      "rbc read wealth management support learn work ceo young thank\n",
      "Topic #6:\n",
      "learn family market global investors income investing fixed rbc despite\n",
      "Topic #7:\n",
      "learn wealth plan retirement planning consider read health financial care\n",
      "Topic #8:\n",
      "learn rbc read impact look partnership support signs wealth career\n",
      "Topic #9:\n",
      "investing learn read rbc women impacted start health wealth helped\n"
     ]
    }
   ],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "\n",
    "print_top_words(lda_rbc, feature_names_rbc, n_top_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top words per topic\n",
    "top_words_per_topic = []\n",
    "for topic_idx, topic in enumerate(lda_rbc.components_):\n",
    "    top_features_ind = topic.argsort()[:-11:-1]\n",
    "    top_features = [feature_names_rbc[i] for i in top_features_ind]\n",
    "    top_words_per_topic.append(top_features)\n",
    "\n",
    "# Create a DataFrame with the top words for each topic\n",
    "df_top_words_rbc = pd.DataFrame(top_words_per_topic)\n",
    "df_top_words_transposed_rbc = df_top_words_rbc.T \n",
    "\n",
    "# Save the DataFrame to an Excel file\n",
    "#excel_path = '/Users/houhiroshisakai/Desktop/Schulich/Term 2/MBAN 6090/ResultIntegration.xlsx'\n",
    "#df_top_words_transposed_rbc.to_excel(excel_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rbc_prefixed = df_top_words_transposed_rbc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_cibc = pd.read_csv('/Users/houhiroshisakai/Desktop/Schulich/Term 2/MBAN 6090/CIBCwealth_since_2018_01_01.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_cibc = tweets_cibc['Tweet'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_texts_cibc = [clean_text(text) for text in texts_cibc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.1        2.10000407 0.1        ... 1.09998658 0.1        0.1000386 ]\n",
      " [1.10000231 0.1        0.1        ... 1.18906372 1.09999003 0.1       ]\n",
      " [1.09999769 0.1        0.1        ... 0.1        0.1        0.10003843]\n",
      " ...\n",
      " [0.1        1.09999593 0.1        ... 0.10003323 3.60307106 0.1       ]\n",
      " [0.1        0.1        3.10000417 ... 4.10006921 2.09994143 0.1       ]\n",
      " [0.1        0.1        4.09999071 ... 0.1000185  9.59698151 0.1001207 ]]\n"
     ]
    }
   ],
   "source": [
    "# ContextVectorize\n",
    "X_cibc = vectorizer.fit_transform(clean_texts_cibc)\n",
    "\n",
    "# Apply LDA\n",
    "lda_cibc = LatentDirichletAllocation(n_components=10)\n",
    "topics_cibc = lda_cibc.fit(X_cibc)\n",
    "\n",
    "print(lda_cibc.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names_cibc = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "reserve federal tax global recession interestrates investment perspectives cibcs economy\n",
      "Topic #1:\n",
      "says chief interestrates markets durantaye la luc higher cibcs rate\n",
      "Topic #2:\n",
      "register join tax pm et family hear experts cibcfamilyoffice cibc\n",
      "Topic #3:\n",
      "bank rate canada rates today economy cibc help inflation learn\n",
      "Topic #4:\n",
      "inflation cibcs oil tal market dyk housing benjamin investors canadian\n",
      "Topic #5:\n",
      "week discuss available watch investment roundup weekly economic markets reading\n",
      "Topic #6:\n",
      "day happy wishing tax ones loved celebrate today time investing\n",
      "Topic #7:\n",
      "canadians investors know golombek jamie canada like end cibcs shares\n",
      "Topic #8:\n",
      "cibc jamie month investing team year golombek cibcfamilyoffice community contributions\n",
      "Topic #9:\n",
      "wealth wood cibc gundy advisors clients women advisor experience industry\n"
     ]
    }
   ],
   "source": [
    "print_top_words(lda_cibc, feature_names_cibc, n_top_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top words per topic\n",
    "top_words_per_topic = []\n",
    "for topic_idx, topic in enumerate(lda_cibc.components_):\n",
    "    top_features_ind = topic.argsort()[:-11:-1]\n",
    "    top_features = [feature_names_cibc[i] for i in top_features_ind]\n",
    "    top_words_per_topic.append(top_features)\n",
    "\n",
    "# Create a DataFrame with the top words for each topic\n",
    "df_top_words_cibc = pd.DataFrame(top_words_per_topic)\n",
    "df_top_words_transposed_cibc = df_top_words_cibc.T \n",
    "\n",
    "df_cibc_prefixed = df_top_words_transposed_cibc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>reserve</td>\n",
       "      <td>says</td>\n",
       "      <td>register</td>\n",
       "      <td>bank</td>\n",
       "      <td>inflation</td>\n",
       "      <td>week</td>\n",
       "      <td>day</td>\n",
       "      <td>canadians</td>\n",
       "      <td>cibc</td>\n",
       "      <td>wealth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>federal</td>\n",
       "      <td>chief</td>\n",
       "      <td>join</td>\n",
       "      <td>rate</td>\n",
       "      <td>cibcs</td>\n",
       "      <td>discuss</td>\n",
       "      <td>happy</td>\n",
       "      <td>investors</td>\n",
       "      <td>jamie</td>\n",
       "      <td>wood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tax</td>\n",
       "      <td>interestrates</td>\n",
       "      <td>tax</td>\n",
       "      <td>canada</td>\n",
       "      <td>oil</td>\n",
       "      <td>available</td>\n",
       "      <td>wishing</td>\n",
       "      <td>know</td>\n",
       "      <td>month</td>\n",
       "      <td>cibc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>global</td>\n",
       "      <td>markets</td>\n",
       "      <td>pm</td>\n",
       "      <td>rates</td>\n",
       "      <td>tal</td>\n",
       "      <td>watch</td>\n",
       "      <td>tax</td>\n",
       "      <td>golombek</td>\n",
       "      <td>investing</td>\n",
       "      <td>gundy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>recession</td>\n",
       "      <td>durantaye</td>\n",
       "      <td>et</td>\n",
       "      <td>today</td>\n",
       "      <td>market</td>\n",
       "      <td>investment</td>\n",
       "      <td>ones</td>\n",
       "      <td>jamie</td>\n",
       "      <td>team</td>\n",
       "      <td>advisors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>interestrates</td>\n",
       "      <td>la</td>\n",
       "      <td>family</td>\n",
       "      <td>economy</td>\n",
       "      <td>dyk</td>\n",
       "      <td>roundup</td>\n",
       "      <td>loved</td>\n",
       "      <td>canada</td>\n",
       "      <td>year</td>\n",
       "      <td>clients</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>investment</td>\n",
       "      <td>luc</td>\n",
       "      <td>hear</td>\n",
       "      <td>cibc</td>\n",
       "      <td>housing</td>\n",
       "      <td>weekly</td>\n",
       "      <td>celebrate</td>\n",
       "      <td>like</td>\n",
       "      <td>golombek</td>\n",
       "      <td>women</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>perspectives</td>\n",
       "      <td>higher</td>\n",
       "      <td>experts</td>\n",
       "      <td>help</td>\n",
       "      <td>benjamin</td>\n",
       "      <td>economic</td>\n",
       "      <td>today</td>\n",
       "      <td>end</td>\n",
       "      <td>cibcfamilyoffice</td>\n",
       "      <td>advisor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>cibcs</td>\n",
       "      <td>cibcs</td>\n",
       "      <td>cibcfamilyoffice</td>\n",
       "      <td>inflation</td>\n",
       "      <td>investors</td>\n",
       "      <td>markets</td>\n",
       "      <td>time</td>\n",
       "      <td>cibcs</td>\n",
       "      <td>community</td>\n",
       "      <td>experience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>economy</td>\n",
       "      <td>rate</td>\n",
       "      <td>cibc</td>\n",
       "      <td>learn</td>\n",
       "      <td>canadian</td>\n",
       "      <td>reading</td>\n",
       "      <td>investing</td>\n",
       "      <td>shares</td>\n",
       "      <td>contributions</td>\n",
       "      <td>industry</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0              1                 2          3          4   \n",
       "0        reserve           says          register       bank  inflation  \\\n",
       "1        federal          chief              join       rate      cibcs   \n",
       "2            tax  interestrates               tax     canada        oil   \n",
       "3         global        markets                pm      rates        tal   \n",
       "4      recession      durantaye                et      today     market   \n",
       "5  interestrates             la            family    economy        dyk   \n",
       "6     investment            luc              hear       cibc    housing   \n",
       "7   perspectives         higher           experts       help   benjamin   \n",
       "8          cibcs          cibcs  cibcfamilyoffice  inflation  investors   \n",
       "9        economy           rate              cibc      learn   canadian   \n",
       "\n",
       "            5          6          7                 8           9  \n",
       "0        week        day  canadians              cibc      wealth  \n",
       "1     discuss      happy  investors             jamie        wood  \n",
       "2   available    wishing       know             month        cibc  \n",
       "3       watch        tax   golombek         investing       gundy  \n",
       "4  investment       ones      jamie              team    advisors  \n",
       "5     roundup      loved     canada              year     clients  \n",
       "6      weekly  celebrate       like          golombek       women  \n",
       "7    economic      today        end  cibcfamilyoffice     advisor  \n",
       "8     markets       time      cibcs         community  experience  \n",
       "9     reading  investing     shares     contributions    industry  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cibc_prefixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store RBC to excel file\n",
    "df_rbc_with_name = pd.DataFrame([['RBC']], columns=[0])  # Only a cell includes \"RBC\"\n",
    "df_rbc_with_topics = pd.concat([df_rbc_with_name, df_rbc_prefixed], ignore_index=True)\n",
    "\n",
    "\n",
    "df_final = pd.concat([df_final, df_rbc_with_topics], ignore_index=True)\n",
    "#Store CIBC to excel file\n",
    "df_cibc_with_name = pd.DataFrame([['CIBC']], columns=[0])  # 只有一个单元格包含\"CIBC\"\n",
    "df_cibc_with_topics = pd.concat([df_cibc_with_name, df_cibc_prefixed], ignore_index=True)\n",
    "\n",
    "excel_path = '/Users/houhiroshisakai/Desktop/Schulich/Term 2/MBAN 6090/ResultIntegration.xlsx'\n",
    "\n",
    "df_final = pd.concat([df_final, df_cibc_with_topics], ignore_index=True)\n",
    "df_final.to_excel(excel_path, sheet_name='Bank_Topics', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_bmo = pd.read_csv('/Users/houhiroshisakai/Desktop/Schulich/Term 2/MBAN 6090/BMO_since_2018_01_01.csv')\n",
    "\n",
    "texts_bmo = tweets_bmo['Tweet'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_texts_bmo = [clean_text(text) for text in texts_bmo]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.1         0.1         0.10000352 ...  0.10017287  0.10005787\n",
      "   0.10001447]\n",
      " [ 0.10001566  0.10000315  1.0882488  ...  0.1         0.10000057\n",
      "   0.1       ]\n",
      " [ 0.1000122   9.78425734 10.90067421 ...  2.09978383  0.10000062\n",
      "   0.1       ]\n",
      " ...\n",
      " [ 0.10000465  0.10001246  3.47368833 ...  0.1         0.1\n",
      "   0.1       ]\n",
      " [ 0.1         0.1         8.61447523 ...  0.1         0.1\n",
      "   0.1       ]\n",
      " [ 0.1         0.10000091  0.10000057 ...  0.10003046  0.1\n",
      "   3.09997869]]\n"
     ]
    }
   ],
   "source": [
    "# ContextVectorize\n",
    "X_bmo = vectorizer.fit_transform(clean_texts_bmo)\n",
    "\n",
    "# Apply LDA\n",
    "lda_bmo = LatentDirichletAllocation(n_components=10)\n",
    "topics_bmo = lda_bmo.fit_transform(X_bmo)\n",
    "\n",
    "print(lda_bmo.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names_bmo = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "bmo wethenorth northovereverything game proud support bmogrowthegood today day learn\n",
      "Topic #1:\n",
      "dm apologize wait hi send need feel times free questions\n",
      "Topic #2:\n",
      "working wait apologize times possible thank soon patience calls high\n",
      "Topic #3:\n",
      "thank feedback dm experience help hello thanks time sharing nb\n",
      "Topic #4:\n",
      "bmo card account information credit visit branch hi mastercard debit\n",
      "Topic #5:\n",
      "nc know let great thanks investsmart thank youre help welcome\n",
      "Topic #6:\n",
      "banking online dm hi app youre mobile sorry learn hello\n",
      "Topic #7:\n",
      "message send private sorry learn im hi assist like phone\n",
      "Topic #8:\n",
      "thank dm send hi sorry assist help look hello reaching\n",
      "Topic #9:\n",
      "bmo nous vous business join et bmoforwomen financial investing episode\n"
     ]
    }
   ],
   "source": [
    "print_top_words(lda_bmo, feature_names_bmo, n_top_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top words per topic\n",
    "top_words_per_topic = []\n",
    "for topic_idx, topic in enumerate(lda_bmo.components_):\n",
    "    top_features_ind = topic.argsort()[:-11:-1]\n",
    "    top_features = [feature_names_bmo[i] for i in top_features_ind]\n",
    "    top_words_per_topic.append(top_features)\n",
    "\n",
    "# Create a DataFrame with the top words for each topic\n",
    "df_top_words_bmo = pd.DataFrame(top_words_per_topic)\n",
    "df_top_words_transposed_bmo = df_top_words_bmo.T \n",
    "\n",
    "df_bmo_prefixed = df_top_words_transposed_bmo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scotiabank "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.10000015  1.09999119 13.04354861 ...  0.1         0.1\n",
      "   0.10000067]\n",
      " [ 2.09989139  0.1         0.1        ...  0.1         0.1\n",
      "   0.1       ]\n",
      " [ 0.1         0.1         0.10000927 ...  1.09995578  0.10001578\n",
      "   0.1       ]\n",
      " ...\n",
      " [ 0.1         1.09996378  0.1        ...  1.10000215  0.1\n",
      "   0.1       ]\n",
      " [ 0.1         0.10002873  1.15641712 ...  0.1         0.1\n",
      "   0.1       ]\n",
      " [ 0.10010056  1.09999444  0.1        ...  1.09994736  0.1\n",
      "   0.1       ]]\n"
     ]
    }
   ],
   "source": [
    "tweets_scotia = pd.read_csv('/Users/houhiroshisakai/Desktop/Schulich/Term 2/MBAN 6090/scotiabank_since_2018_01_01.csv')\n",
    "\n",
    "texts_scotia = tweets_scotia['Tweet'].tolist()\n",
    "\n",
    "clean_texts_scotia = [clean_text(text) for text in texts_scotia]\n",
    "\n",
    "# ContextVectorize\n",
    "vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "X_scotia = vectorizer.fit_transform(clean_texts_scotia)\n",
    "\n",
    "# Apply LDA\n",
    "lda_scotia = LatentDirichletAllocation(n_components=10)\n",
    "topics_scotia = lda_scotia.fit(X_scotia)\n",
    "\n",
    "print(lda_scotia.components_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names_scotia = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "hockey rate scotiabank inflation make inclusive work home accessible game\n",
      "Topic #1:\n",
      "listen follow podcasts spotify apple scene rate canada points scotiabank\n",
      "Topic #2:\n",
      "canada credit canadians chief rate scotiabanks inflation investment latest rates\n",
      "Topic #3:\n",
      "year account blackhistorymonth blackvoices story women savings celebrate black business\n",
      "Topic #4:\n",
      "hockey canada financial hockeyforall day help like jersey money new\n",
      "Topic #5:\n",
      "nous la scotiabank et scotia sa gatineau célébrons vous autochtones\n",
      "Topic #6:\n",
      "la les et des le taux que du en pour\n",
      "Topic #7:\n",
      "scotiabank ont little depuis pas canada pour ne new employees\n",
      "Topic #8:\n",
      "latest episode scotiabank podcast canadas help canada read new canadian\n",
      "Topic #9:\n",
      "financial holiday advisor scotia goals season plan new learn retirement\n"
     ]
    }
   ],
   "source": [
    "print_top_words(lda_scotia, feature_names_scotia, n_top_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top words per topic\n",
    "top_words_per_topic = []\n",
    "for topic_idx, topic in enumerate(lda_scotia.components_):\n",
    "    top_features_ind = topic.argsort()[:-11:-1]\n",
    "    top_features = [feature_names_scotia[i] for i in top_features_ind]\n",
    "    top_words_per_topic.append(top_features)\n",
    "\n",
    "# Create a DataFrame with the top words for each topic\n",
    "df_top_words_scotia = pd.DataFrame(top_words_per_topic)\n",
    "df_top_words_transposed_scotia = df_top_words_scotia.T \n",
    "\n",
    "df_scotia_prefixed = df_top_words_transposed_scotia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD_US_News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.1         0.1         0.1        ...  2.50906681  0.1\n",
      "   0.10000743]\n",
      " [ 0.1         1.09999135  0.10005518 ...  0.798476    0.1\n",
      "   0.1       ]\n",
      " [ 0.1         0.10001627  0.1        ...  4.5319798   1.09999346\n",
      "   0.1       ]\n",
      " ...\n",
      " [ 1.10000038  0.1         0.1        ...  0.1         1.10000406\n",
      "   0.1       ]\n",
      " [ 0.1         0.1         0.1        ... 11.11640123  0.1\n",
      "   0.10000833]\n",
      " [ 0.1         0.1         1.09994031 ...  1.63148437  0.1\n",
      "   3.94859763]]\n"
     ]
    }
   ],
   "source": [
    "tweets_tdus = pd.read_csv('/Users/houhiroshisakai/Desktop/Schulich/Term 2/MBAN 6090/TDNews_US_since_2018_01_01.csv')\n",
    "\n",
    "texts_tdus = tweets_tdus['Tweet'].tolist()\n",
    "\n",
    "clean_texts_tdus = [clean_text(text) for text in texts_tdus]\n",
    "\n",
    "# ContextVectorize\n",
    "X_tdus = vectorizer.fit_transform(clean_texts_tdus)\n",
    "\n",
    "# Apply LDA\n",
    "lda_tdus = LatentDirichletAllocation(n_components=10)\n",
    "topics_tdus = lda_tdus.fit(X_tdus)\n",
    "\n",
    "print(lda_tdus.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "wealth financial tdbank_us home td investment strategist equity head chief\n",
      "Topic #1:\n",
      "tdbank_us credit head lending td learn day home card important\n",
      "Topic #2:\n",
      "td community bank learn new housing business million ready lgbtq\n",
      "Topic #3:\n",
      "td learn tds year tdbank_us read inclusion disability colleagues proud\n",
      "Topic #4:\n",
      "td support learn communities local community provide help organizations efforts\n",
      "Topic #5:\n",
      "read tdbank_us covid shares time tds new learn journey officer\n",
      "Topic #6:\n",
      "check tips moneymattersmonday make help business budget new learn year\n",
      "Topic #7:\n",
      "td ceo president customers learn tdbank_us braca greg tdbank_uss ppp\n",
      "Topic #8:\n",
      "survey money tdbank_us love according results data relationship check couples\n",
      "Topic #9:\n",
      "td financial business tdbank_us bank owners survey finance best learn\n"
     ]
    }
   ],
   "source": [
    "feature_names_tdus = vectorizer.get_feature_names_out()\n",
    "\n",
    "print_top_words(lda_tdus, feature_names_tdus, n_top_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top words per topic\n",
    "top_words_per_topic = []\n",
    "for topic_idx, topic in enumerate(lda_tdus.components_):\n",
    "    top_features_ind = topic.argsort()[:-11:-1]\n",
    "    top_features = [feature_names_tdus[i] for i in top_features_ind]\n",
    "    top_words_per_topic.append(top_features)\n",
    "\n",
    "# Create a DataFrame with the top words for each topic\n",
    "df_top_words_tdus = pd.DataFrame(top_words_per_topic)\n",
    "df_top_words_transposed_tdus = df_top_words_tdus.T \n",
    "\n",
    "df_tdus_prefixed = df_top_words_transposed_tdus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_tdus1 = pd.read_csv('/Users/houhiroshisakai/Desktop/Schulich/Term 2/MBAN 6090/TDBank_US_since_2018_01_01.csv')\n",
    "\n",
    "texts_tdus1 = tweets_tdus1['Tweet'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_texts_tdus1 = [clean_text(text) for text in texts_tdus1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.00000000e-01 1.00000000e-01 5.14249111e+00 ... 1.00000001e-01\n",
      "  1.00000000e-01 1.00000000e-01]\n",
      " [1.00000000e-01 1.00000000e-01 9.82422869e+01 ... 1.00000001e-01\n",
      "  1.00010916e-01 1.00010916e-01]\n",
      " [1.00000000e-01 1.00000000e-01 5.91359643e+02 ... 1.00000001e-01\n",
      "  2.09989325e+00 2.09989325e+00]\n",
      " ...\n",
      " [1.00000000e-01 1.00000000e-01 1.67483334e-01 ... 1.00000000e-01\n",
      "  1.00000000e-01 1.00000000e-01]\n",
      " [1.00000000e-01 1.00000000e-01 1.00011753e-01 ... 1.00024404e-01\n",
      "  1.00048297e-01 1.00048297e-01]\n",
      " [1.00000000e-01 1.00000000e-01 3.53510168e+02 ... 3.09989080e+00\n",
      "  1.00000000e-01 1.00000000e-01]]\n"
     ]
    }
   ],
   "source": [
    "# ContextVectorize\n",
    "X_tdus1 = vectorizer.fit_transform(clean_texts_tdus1)\n",
    "\n",
    "# Apply LDA\n",
    "lda_tdus1 = LatentDirichletAllocation(n_components=10)\n",
    "topics_tdus1 = lda_tdus1.fit_transform(X_tdus1)\n",
    "\n",
    "print(lda_tdus1.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "apologize online issues inconvenience working banking app experiencing issue sorry\n",
      "Topic #1:\n",
      "appreciate patience customers time working possible td thank feedback thanks\n",
      "Topic #2:\n",
      "dm noaccts like send best thank lf details assist good\n",
      "Topic #3:\n",
      "td card bank hope store visit information debit hi hey\n",
      "Topic #4:\n",
      "free feel dm noaccts tw happy send noacct great know\n",
      "Topic #5:\n",
      "dm account send numbers hi like chat concerns thanks hey\n",
      "Topic #6:\n",
      "ka hey concerns happy know fees questions account address understand\n",
      "Topic #7:\n",
      "account dm send numbers good details hear morning like ask\n",
      "Topic #8:\n",
      "dm accts help hi lw sorry plz hold times assist\n",
      "Topic #9:\n",
      "dm like send account details feel numbers saw learn tweet\n"
     ]
    }
   ],
   "source": [
    "feature_names_tdus1 = vectorizer.get_feature_names_out()\n",
    "\n",
    "print_top_words(lda_tdus1, feature_names_tdus1, n_top_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top words per topic\n",
    "top_words_per_topic = []\n",
    "for topic_idx, topic in enumerate(lda_tdus1.components_):\n",
    "    top_features_ind = topic.argsort()[:-11:-1]\n",
    "    top_features = [feature_names_bmo[i] for i in top_features_ind]\n",
    "    top_words_per_topic.append(top_features)\n",
    "\n",
    "# Create a DataFrame with the top words for each topic\n",
    "df_top_words_tdus1 = pd.DataFrame(top_words_per_topic)\n",
    "df_top_words_transposed_tdus1 = df_top_words_tdus1.T \n",
    "\n",
    "df_tdus1_prefixed = df_top_words_transposed_tdus1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD_Canada_New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_tdcanada = pd.read_csv('/Users/houhiroshisakai/Desktop/Schulich/Term 2/MBAN 6090/TDNews_Canada_since_2018_01_01.csv')\n",
    "\n",
    "texts_tdcanada = tweets_tdcanada['Tweet'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_texts_tdcanada = [clean_text(text) for text in texts_tdcanada]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.1        2.1000061  0.1        ... 2.10001324 0.10005523 0.1       ]\n",
      " [0.10000191 0.1        1.09999694 ... 0.1        1.09990892 0.1       ]\n",
      " [0.1        0.1        0.1        ... 1.09999186 0.1        0.1       ]\n",
      " ...\n",
      " [0.1        0.1        0.1        ... 0.1        1.27352652 0.1       ]\n",
      " [0.1        0.1        0.1        ... 0.1        0.10000702 0.10000446]\n",
      " [0.1        0.1        0.1        ... 0.1        0.1        2.10000449]]\n"
     ]
    }
   ],
   "source": [
    "# ContextVectorize\n",
    "X_tdcanada = vectorizer.fit_transform(clean_texts_tdcanada)\n",
    "\n",
    "# Apply LDA\n",
    "lda_tdcanada = LatentDirichletAllocation(n_components=10)\n",
    "topics = lda_tdcanada.fit_transform(X_tdcanada)\n",
    "\n",
    "print(lda_tdcanada.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "td digital tds customers experiences canada new help customer learn\n",
      "Topic #1:\n",
      "td news banking heres indigenous weve bank financial shares digital\n",
      "Topic #2:\n",
      "td covid help customers ceo bharat masrani learn canadians pandemic\n",
      "Topic #3:\n",
      "fraud financial help black learn money women tips td work\n",
      "Topic #4:\n",
      "financial td tds new help canadians global canada canadian protect\n",
      "Topic #5:\n",
      "td report read new annual economics innovation canadians proud group\n",
      "Topic #6:\n",
      "help financial mortgage season tips holiday ways canadians retirement home\n",
      "Topic #7:\n",
      "td help heres canadian support intelligence insurance tips make money\n",
      "Topic #8:\n",
      "brucecooper_td tdassetmanagement weeks podcast marketperspectives canada growth markets market economy\n",
      "Topic #9:\n",
      "td kids financial customers check tds sure changing heres parents\n"
     ]
    }
   ],
   "source": [
    "feature_names_tdcanada = vectorizer.get_feature_names_out()\n",
    "\n",
    "print_top_words(lda_tdcanada, feature_names_tdcanada, n_top_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top words per topic\n",
    "top_words_per_topic = []\n",
    "for topic_idx, topic in enumerate(lda_tdcanada.components_):\n",
    "    top_features_ind = topic.argsort()[:-11:-1]\n",
    "    top_features = [feature_names_tdcanada[i] for i in top_features_ind]\n",
    "    top_words_per_topic.append(top_features)\n",
    "\n",
    "# Create a DataFrame with the top words for each topic\n",
    "df_top_words_tdcanada = pd.DataFrame(top_words_per_topic)\n",
    "df_top_words_transposed_tdcanada = df_top_words_tdcanada.T \n",
    "\n",
    "df_tdcanada_prefixed = df_top_words_transposed_tdcanada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD Canada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.1        1.09987438 0.1        ... 0.1        4.87673378 0.1       ]\n",
      " [0.1        0.1        0.1        ... 0.1        0.1        1.10000142]\n",
      " [0.1        0.1        0.1        ... 1.09999922 0.71148895 0.1       ]\n",
      " ...\n",
      " [1.09999936 0.10010723 0.1        ... 2.10000397 0.1000092  1.10000117]\n",
      " [0.1        0.1        0.1        ... 0.1        1.09999901 0.1       ]\n",
      " [1.10000063 0.1        0.1        ... 0.1        0.1        0.1       ]]\n"
     ]
    }
   ],
   "source": [
    "tweets_tdcanada1 = pd.read_csv('/Users/houhiroshisakai/Desktop/Schulich/Term 2/MBAN 6090/TDNews_Canada_since_2018_01_01.csv')\n",
    "\n",
    "texts_tdcanada1 = tweets_tdcanada1['Tweet'].tolist()\n",
    "\n",
    "clean_texts_tdcanada1 = [clean_text(text) for text in texts_tdcanada]\n",
    "\n",
    "# ContextVectorize\n",
    "X_tdcanada1 = vectorizer.fit_transform(clean_texts_tdcanada1)\n",
    "\n",
    "# Apply LDA\n",
    "lda_tdcanada1 = LatentDirichletAllocation(n_components=10)\n",
    "topics_tdcanada1 = lda_tdcanada1.fit_transform(X_tdcanada1)\n",
    "\n",
    "print(lda_tdcanada1.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "brucecooper_td tdassetmanagement weeks podcast marketperspectives digital explains td tds markets\n",
      "Topic #1:\n",
      "td ceo bharat masrani tds group financial bank new canadian\n",
      "Topic #2:\n",
      "financial canadians td report advice money finances ai online dyk\n",
      "Topic #3:\n",
      "mortgage td home retirement financial canadians covid tips help common\n",
      "Topic #4:\n",
      "td help learn customers read support financial canada communities colleagues\n",
      "Topic #5:\n",
      "td community economic canada lgbtq tds covid business economist read\n",
      "Topic #6:\n",
      "td help protect loved financial canadians ones fraud insurance learn\n",
      "Topic #7:\n",
      "td help new heres financial ways dont canadians weve future\n",
      "Topic #8:\n",
      "customers customer td new experiences experience black innovation inclusive digital\n",
      "Topic #9:\n",
      "td tds season tdnewsroom heres year canadian news holiday business\n"
     ]
    }
   ],
   "source": [
    "feature_names_tdcanada1 = vectorizer.get_feature_names_out()\n",
    "\n",
    "print_top_words(lda_tdcanada1, feature_names_tdcanada1, n_top_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top words per topic\n",
    "top_words_per_topic = []\n",
    "for topic_idx, topic in enumerate(lda_tdcanada1.components_):\n",
    "    top_features_ind = topic.argsort()[:-11:-1]\n",
    "    top_features = [feature_names_tdcanada1[i] for i in top_features_ind]\n",
    "    top_words_per_topic.append(top_features)\n",
    "\n",
    "# Create a DataFrame with the top words for each topic\n",
    "df_top_words_tdcanada1 = pd.DataFrame(top_words_per_topic)\n",
    "df_top_words_transposed_tdcanada1 = df_top_words_tdcanada1.T \n",
    "\n",
    "df_tdcanada1_prefixed = df_top_words_transposed_tdcanada1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# US Company"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Morgan Stanley"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.1        0.1        0.10000169 ... 0.1        0.1        0.1       ]\n",
      " [0.1        0.1        0.1        ... 0.1        0.1        0.1       ]\n",
      " [0.1        0.1        0.1        ... 0.1        0.1        0.1       ]\n",
      " ...\n",
      " [0.1        0.1        2.09998567 ... 0.1        2.09989402 0.1       ]\n",
      " [0.10005864 4.09994799 0.1        ... 2.1        0.1        0.10000675]\n",
      " [0.1        0.10003885 0.1        ... 0.1        0.1        0.10000966]]\n"
     ]
    }
   ],
   "source": [
    "tweets_ms = pd.read_csv('/Users/houhiroshisakai/Desktop/Schulich/Term 2/MBAN 6090/MorganStanley_since_2018_01_01.csv')\n",
    "\n",
    "texts_ms = tweets_ms['Tweet'].tolist()\n",
    "\n",
    "clean_texts_ms = [clean_text(text) for text in texts_ms]\n",
    "\n",
    "# ContextVectorize\n",
    "X_ms = vectorizer.fit_transform(clean_texts_ms)\n",
    "\n",
    "# Apply LDA\n",
    "lda_ms = LatentDirichletAllocation(n_components=10)\n",
    "topics_ms = lda_ms.fit(X_ms)\n",
    "\n",
    "print(lda_ms.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "markets chief investors global market equity read investment episode policy\n",
      "Topic #1:\n",
      "morgan stanley health mental msgivesback children head childrens management support\n",
      "Topic #2:\n",
      "help learn morgan market stanley sichallenge fund make money financial\n",
      "Topic #3:\n",
      "ms earnings net billion management multicultural investment revenues year innovation\n",
      "Topic #4:\n",
      "eagleup wealth lisa shalett morgan management investment year eagles booktrustusa\n",
      "Topic #5:\n",
      "investing sustainable morgan stanley change investors learn climate sustainability new\n",
      "Topic #6:\n",
      "morgan podcast year opportunity new access episode carlaannharris team stanley\n",
      "Topic #7:\n",
      "morgan new stanley asia firm annual opportunities summit pride lgbt\n",
      "Topic #8:\n",
      "morgan stanley learn msgivesback employees women program day career london\n",
      "Topic #9:\n",
      "morgan stanley learn advisors veterans named clients congratulations make wealth\n"
     ]
    }
   ],
   "source": [
    "feature_names_ms = vectorizer.get_feature_names_out()\n",
    "print_top_words(lda_ms, feature_names_ms, n_top_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top words per topic\n",
    "top_words_per_topic = []\n",
    "for topic_idx, topic in enumerate(lda_ms.components_):\n",
    "    top_features_ind = topic.argsort()[:-11:-1]\n",
    "    top_features = [feature_names_ms[i] for i in top_features_ind]\n",
    "    top_words_per_topic.append(top_features)\n",
    "\n",
    "# Create a DataFrame with the top words for each topic\n",
    "df_top_words_ms = pd.DataFrame(top_words_per_topic)\n",
    "df_top_words_transposed_ms = df_top_words_ms.T \n",
    "\n",
    "df_ms_prefixed = df_top_words_transposed_ms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UBS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.1         0.1         0.1        ...  0.1         0.1\n",
      "  19.47331979]\n",
      " [ 0.1         0.1         0.1        ...  0.1         0.1\n",
      "   2.21210432]\n",
      " [ 0.1         0.10002408  0.10006523 ...  0.1         1.09984244\n",
      "   0.10000936]\n",
      " ...\n",
      " [ 0.10001673  2.09996231  0.10000357 ...  0.1         0.1\n",
      "   0.1       ]\n",
      " [ 0.1         0.1         0.1        ...  0.10000353  0.1\n",
      "   3.08937569]\n",
      " [ 0.1         0.1         0.1        ...  2.09999647  0.10015356\n",
      "   1.92865036]]\n"
     ]
    }
   ],
   "source": [
    "tweets_ubs = pd.read_csv('/Users/houhiroshisakai/Desktop/Schulich/Term 2/MBAN 6090/UBS_since_2018_01_01.csv')\n",
    "\n",
    "texts_ubs = tweets_ubs['Tweet'].tolist()\n",
    "\n",
    "clean_texts_ubs = [clean_text(text) for text in texts_ubs]\n",
    "\n",
    "# ContextVectorize\n",
    "X_ubs = vectorizer.fit_transform(clean_texts_ubs)\n",
    "\n",
    "# Apply LDA\n",
    "lda_ubs = LatentDirichletAllocation(n_components=10)\n",
    "topics_ubs = lda_ubs.fit_transform(X_ubs)\n",
    "\n",
    "print(lda_ubs.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "shareubs togetherband thetogetherband iwd sdg support awareness help goal choosetochallenge\n",
      "Topic #1:\n",
      "shareubs ubs chief economist paul donovan global gwm year video\n",
      "Topic #2:\n",
      "shareubs wef paper learn white world read sdg education ubs\n",
      "Topic #3:\n",
      "women shareubs financial ownyourworth ubss money learn decisions finances watch\n",
      "Topic #4:\n",
      "results business ubs ceo shareubs group ermotti sergio owners quarter\n",
      "Topic #5:\n",
      "shareubs ubs finance impact sustainability future china best bank ubsgcc\n",
      "Topic #6:\n",
      "shareubs ubsresearch investors report latest global market explore new ubs\n",
      "Topic #7:\n",
      "shareubs ubs head ubsconf china conference global ubsevidencelab watch new\n",
      "Topic #8:\n",
      "shareubs ubs join watch episode trending learn conversation tune today\n",
      "Topic #9:\n",
      "shareubs live results starts stay tomorrow presentation sustainable tuned cest\n"
     ]
    }
   ],
   "source": [
    "feature_names_ubs = vectorizer.get_feature_names_out()\n",
    "print_top_words(lda_ubs, feature_names_ubs, n_top_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top words per topic\n",
    "top_words_per_topic = []\n",
    "for topic_idx, topic in enumerate(lda_ubs.components_):\n",
    "    top_features_ind = topic.argsort()[:-11:-1]\n",
    "    top_features = [feature_names_ubs[i] for i in top_features_ind]\n",
    "    top_words_per_topic.append(top_features)\n",
    "\n",
    "# Create a DataFrame with the top words for each topic\n",
    "df_top_words_ubs = pd.DataFrame(top_words_per_topic)\n",
    "df_top_words_transposed_ubs = df_top_words_ubs.T \n",
    "\n",
    "df_ubs_prefixed = df_top_words_transposed_ubs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Citi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.1        3.09998441 1.37222363 ... 0.1        0.10002176 0.1       ]\n",
      " [0.1        0.1        1.82764451 ... 1.09994834 0.1        0.10001872]\n",
      " [0.1        0.1        0.1        ... 0.1        0.1        0.1       ]\n",
      " ...\n",
      " [0.1        0.10009384 1.10000406 ... 0.1        0.10012225 0.10000249]\n",
      " [0.10006412 0.10005269 0.10005344 ... 0.10004853 0.10008005 0.1       ]\n",
      " [0.1        0.1        0.10000367 ... 1.09994772 0.1        2.09997068]]\n"
     ]
    }
   ],
   "source": [
    "tweets_citi = pd.read_csv('/Users/houhiroshisakai/Desktop/Schulich/Term 2/MBAN 6090/Citi_since_2018_01_01.csv')\n",
    "\n",
    "texts_citi = tweets_citi['Tweet'].tolist()\n",
    "\n",
    "clean_texts_citi = [clean_text(text) for text in texts_citi]\n",
    "\n",
    "# ContextVectorize\n",
    "X_citi = vectorizer.fit_transform(clean_texts_citi)\n",
    "\n",
    "# Apply LDA\n",
    "lda_citi = LatentDirichletAllocation(n_components=10)\n",
    "topics_citi = lda_citi.fit(X_citi)\n",
    "\n",
    "print(lda_citi.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "citi bank global best private day head citivolunteers awards services\n",
      "Topic #1:\n",
      "citi learn clients treasury new digital citis payments solutions help\n",
      "Topic #2:\n",
      "citi teamciti citis athletes tomorrow news et results stareatgreatness disabilities\n",
      "Topic #3:\n",
      "ceo citi jane fraser global information year financial discusses citis\n",
      "Topic #4:\n",
      "citi learn pathwaysprogress today proud youth support foundation social impact\n",
      "Topic #5:\n",
      "citi digital home money th join citidigimoney proud forward people\n",
      "Topic #6:\n",
      "ceo corbat mike watch citis citi global growth live tune\n",
      "Topic #7:\n",
      "citi learn help communities support housing proud racial cities foundation\n",
      "Topic #8:\n",
      "citi standforprogress women citis diversity proud learn community support colleagues\n",
      "Topic #9:\n",
      "report read citi new gps latest payments opportunities global supply\n"
     ]
    }
   ],
   "source": [
    "feature_names_citi = vectorizer.get_feature_names_out()\n",
    "\n",
    "print_top_words(lda_citi, feature_names_citi, n_top_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top words per topic\n",
    "top_words_per_topic = []\n",
    "for topic_idx, topic in enumerate(lda_citi.components_):\n",
    "    top_features_ind = topic.argsort()[:-11:-1]\n",
    "    top_features = [feature_names_citi[i] for i in top_features_ind]\n",
    "    top_words_per_topic.append(top_features)\n",
    "\n",
    "# Create a DataFrame with the top words for each topic\n",
    "df_top_words_citi = pd.DataFrame(top_words_per_topic)\n",
    "df_top_words_transposed_citi = df_top_words_citi.T \n",
    "\n",
    "df_citi_prefixed = df_top_words_transposed_citi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wells Fargo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.1         0.1         6.09986492 ...  0.16606249  0.10001067\n",
      "   0.10000342]\n",
      " [ 7.09983689  0.1         0.10007202 ...  0.10000016  0.10000058\n",
      "   0.10000427]\n",
      " [ 0.1         0.1         0.1        ...  0.1000047   0.10000349\n",
      "   0.100015  ]\n",
      " ...\n",
      " [ 0.1         0.1         0.1        ...  0.10000059  0.1\n",
      "   0.10001733]\n",
      " [ 0.1         0.1         0.1        ...  0.10000021  0.1\n",
      "  25.47519338]\n",
      " [ 0.10000118  0.1         0.10000037 ...  1.74198748  0.10000318\n",
      "  27.42453022]]\n"
     ]
    }
   ],
   "source": [
    "tweets_wf = pd.read_csv('/Users/houhiroshisakai/Desktop/Schulich/Term 2/MBAN 6090/WellsFargo_since_2018_01_01.csv')\n",
    "\n",
    "texts_wf = tweets_wf['Tweet'].tolist()\n",
    "\n",
    "clean_texts_wf = [clean_text(text) for text in texts_wf]\n",
    "\n",
    "# ContextVectorize\n",
    "X_wf = vectorizer.fit_transform(clean_texts_wf)\n",
    "\n",
    "# Apply LDA\n",
    "lda_wf = LatentDirichletAllocation(n_components=10)\n",
    "topics_wf = lda_wf.fit(X_wf)\n",
    "\n",
    "print(lda_wf.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "deposit inconvenience caused apologize wells fargo account direct fees issue\n",
      "Topic #1:\n",
      "thank happy support great chris glad ddg opportunity thanks help\n",
      "Topic #2:\n",
      "assistance sorry need account hello banker speak thank hear alex\n",
      "Topic #3:\n",
      "fargo wells issue apologize branch working visit thank hi technical\n",
      "Topic #4:\n",
      "youre team support help online jules try feedback able sorry\n",
      "Topic #5:\n",
      "thanks dm thank message hi review email forward received information\n",
      "Topic #6:\n",
      "thank proud hi support communities customers time help work pm\n",
      "Topic #7:\n",
      "account numbers details dm help tweet situation send hi like\n",
      "Topic #8:\n",
      "account numbers dm make sorry provide help details like sure\n",
      "Topic #9:\n",
      "number dm phone account numbers send like address details sorry\n"
     ]
    }
   ],
   "source": [
    "feature_names_wf = vectorizer.get_feature_names_out()\n",
    "print_top_words(lda_wf, feature_names_wf, n_top_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top words per topic\n",
    "top_words_per_topic = []\n",
    "for topic_idx, topic in enumerate(lda_wf.components_):\n",
    "    top_features_ind = topic.argsort()[:-11:-1]\n",
    "    top_features = [feature_names_wf[i] for i in top_features_ind]\n",
    "    top_words_per_topic.append(top_features)\n",
    "\n",
    "# Create a DataFrame with the top words for each topic\n",
    "df_top_words_wf = pd.DataFrame(top_words_per_topic)\n",
    "df_top_words_transposed_wf = df_top_words_wf.T \n",
    "\n",
    "df_wf_prefixed = df_top_words_transposed_wf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BOA (Bank of America)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.1        0.1        0.1        ... 0.1        0.1        0.10014002]\n",
      " [0.1        0.10000257 0.1        ... 9.09999328 0.1        0.1       ]\n",
      " [6.06619449 0.10004111 0.10002638 ... 0.1        0.1        0.1       ]\n",
      " ...\n",
      " [0.10000199 0.1        0.1        ... 0.10000022 0.1        0.1       ]\n",
      " [0.13353118 2.09998966 4.09996893 ... 0.1        0.1000993  0.1       ]\n",
      " [0.1        1.09993734 0.1        ... 0.10000119 0.1000006  0.1       ]]\n"
     ]
    }
   ],
   "source": [
    "tweets_boa = pd.read_csv('/Users/houhiroshisakai/Desktop/Schulich/Term 2/MBAN 6090/BankofAmerica_since_2018_01_01.csv')\n",
    "\n",
    "texts_boa = tweets_boa['Tweet'].tolist()\n",
    "\n",
    "clean_texts_boa = [clean_text(text) for text in texts_boa]\n",
    "\n",
    "# ContextVectorize\n",
    "X_boa = vectorizer.fit_transform(clean_texts_boa)\n",
    "\n",
    "# Apply LDA\n",
    "lda_boa = LatentDirichletAllocation(n_components=10)\n",
    "topics_boa = lda_boa.fit(X_boa)\n",
    "\n",
    "print(lda_boa.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "proud help support bettermoneyhabits partnership year partner prices red fight\n",
      "Topic #1:\n",
      "link help like connect hi hello account use click dm\n",
      "Topic #2:\n",
      "business small help money app owners banking businesses resources bank\n",
      "Topic #3:\n",
      "women program change positive thanks partnership creating conversation leaders globalambassadors\n",
      "Topic #4:\n",
      "mobile banking app love pay card month deposit free let\n",
      "Topic #5:\n",
      "weve help supporting important young thats organizations women provide thanks\n",
      "Topic #6:\n",
      "bank america cash rewards earn shopping way holiday chicagomarathon credit\n",
      "Topic #7:\n",
      "help financial bettermoneyhabits erica tips youre life new plan bank\n",
      "Topic #8:\n",
      "digital tools using fans impressive cantstopbanking proud debit like wallet\n",
      "Topic #9:\n",
      "thank communities work thanks appreciate great glad shout sharing support\n"
     ]
    }
   ],
   "source": [
    "feature_names_boa = vectorizer.get_feature_names_out()\n",
    "print_top_words(lda_boa, feature_names_boa, n_top_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top words per topic\n",
    "top_words_per_topic = []\n",
    "for topic_idx, topic in enumerate(lda_boa.components_):\n",
    "    top_features_ind = topic.argsort()[:-11:-1]\n",
    "    top_features = [feature_names_boa[i] for i in top_features_ind]\n",
    "    top_words_per_topic.append(top_features)\n",
    "\n",
    "# Create a DataFrame with the top words for each topic\n",
    "df_top_words_boa = pd.DataFrame(top_words_per_topic)\n",
    "df_top_words_transposed_boa = df_top_words_boa.T \n",
    "\n",
    "df_boa_prefixed = df_top_words_transposed_boa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JP Morgan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.1        0.1        0.1        ... 0.1        2.09999669 0.1       ]\n",
      " [0.1        0.1        0.1        ... 0.1        0.1        0.1       ]\n",
      " [0.1        0.1        0.10001658 ... 0.1        0.1        0.1       ]\n",
      " ...\n",
      " [0.1        0.1        0.10000353 ... 0.10001258 0.1        0.1       ]\n",
      " [0.1        0.1        0.10001947 ... 2.09998742 0.1        0.1       ]\n",
      " [0.1        0.1        1.09996306 ... 0.1        0.1        2.1       ]]\n"
     ]
    }
   ],
   "source": [
    "tweets_jp = pd.read_csv('/Users/houhiroshisakai/Desktop/Schulich/Term 2/MBAN 6090/jpmorgan_since_2018_01_01.csv')\n",
    "\n",
    "texts_jp = tweets_jp['Tweet'].tolist()\n",
    "\n",
    "clean_texts_jp = [clean_text(text) for text in texts_jp]\n",
    "\n",
    "# ContextVectorize\n",
    "X_jp = vectorizer.fit_transform(clean_texts_jp)\n",
    "\n",
    "# Apply LDA\n",
    "lda_jp = LatentDirichletAllocation(n_components=10)\n",
    "topics_jp = lda_jp.fit_transform(X_jp)\n",
    "\n",
    "print(lda_jp.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "tech conference investors better career jpm companies robinhoodnyc hear beer\n",
      "Topic #1:\n",
      "jp morgan summer list tech reading new solutions business challenge\n",
      "Topic #2:\n",
      "business new leaders women businesses jpms future techtrends podcast entrepreneurs\n",
      "Topic #3:\n",
      "jpmcc learn technology energy curators participants jpmcartcollection featured look choice\n",
      "Topic #4:\n",
      "jpmorgan chase jpm commercial consumers collection banking art business jpms\n",
      "Topic #5:\n",
      "jpm payments dimon jamie income ceo net eps reports chairman\n",
      "Topic #6:\n",
      "investment company women false forex asj leadership industry leaders day\n",
      "Topic #7:\n",
      "jp global morgans market morgan head outlook shares markets discusses\n",
      "Topic #8:\n",
      "jp morgan global research visit markets jpm thank team blockchain\n",
      "Topic #9:\n",
      "new year jpms erdoes asset management jp growth mary work\n"
     ]
    }
   ],
   "source": [
    "feature_names_jp = vectorizer.get_feature_names_out()\n",
    "print_top_words(lda_jp, feature_names_jp, n_top_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top words per topic\n",
    "top_words_per_topic = []\n",
    "for topic_idx, topic in enumerate(lda_jp.components_):\n",
    "    top_features_ind = topic.argsort()[:-11:-1]\n",
    "    top_features = [feature_names_jp[i] for i in top_features_ind]\n",
    "    top_words_per_topic.append(top_features)\n",
    "\n",
    "# Create a DataFrame with the top words for each topic\n",
    "df_top_words_jp = pd.DataFrame(top_words_per_topic)\n",
    "df_top_words_transposed_jp = df_top_words_jp.T \n",
    "\n",
    "df_jp_prefixed = df_top_words_transposed_jp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raymond James"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.1        0.1        0.1        ... 3.92364415 0.1        0.1       ]\n",
      " [0.1        0.10000288 2.46068976 ... 0.1        0.1        0.10001084]\n",
      " [0.1        0.1        0.10000151 ... 0.1        1.09994368 0.1       ]\n",
      " ...\n",
      " [0.1        0.1        0.10000013 ... 0.10000154 0.10000525 0.1       ]\n",
      " [0.1        0.1        0.1        ... 0.1        0.10004062 0.1       ]\n",
      " [0.1        0.1        0.1        ... 0.1        0.1        0.1       ]]\n"
     ]
    }
   ],
   "source": [
    "tweets_rj = pd.read_csv('/Users/houhiroshisakai/Desktop/Schulich/Term 2/MBAN 6090/RaymondJames_since_2018_01_01.csv')\n",
    "\n",
    "texts_rj = tweets_rj['Tweet'].tolist()\n",
    "\n",
    "clean_texts_rj = [clean_text(text) for text in texts_rj]\n",
    "\n",
    "# ContextVectorize\n",
    "X_rj = vectorizer.fit_transform(clean_texts_rj)\n",
    "\n",
    "# Apply LDA\n",
    "lda_rj = LatentDirichletAllocation(n_components=10)\n",
    "topics_rj = lda_rj.fit_transform(X_rj)\n",
    "\n",
    "print(lda_rj.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "learn james raymond community career important firm inclusion tips honor\n",
      "Topic #1:\n",
      "discuss tune et policy mills ed markets pm analyst raymond\n",
      "Topic #2:\n",
      "financial learn help business james giving raymond discover rjwomen students\n",
      "Topic #3:\n",
      "markets heres market larryadamrj economy cio year investors experts look\n",
      "Topic #4:\n",
      "tax heres learn highlights financial jobsreport new sustainability consider yearend\n",
      "Topic #5:\n",
      "discuss tune et chris raymond james change paul subject ceo\n",
      "Topic #6:\n",
      "pavel molchanov energy financial art oil like important celebrate life\n",
      "Topic #7:\n",
      "planning plan help retirement important financial family consider make youre\n",
      "Topic #8:\n",
      "james raymond financial year new home rjf nyse data learn\n",
      "Topic #9:\n",
      "rjcares associates james raymond advisors learn scott brown economist month\n"
     ]
    }
   ],
   "source": [
    "feature_names_rj = vectorizer.get_feature_names_out()\n",
    "print_top_words(lda_rj, feature_names_rj, n_top_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top words per topic\n",
    "top_words_per_topic = []\n",
    "for topic_idx, topic in enumerate(lda_rj.components_):\n",
    "    top_features_ind = topic.argsort()[:-11:-1]\n",
    "    top_features = [feature_names_rj[i] for i in top_features_ind]\n",
    "    top_words_per_topic.append(top_features)\n",
    "\n",
    "# Create a DataFrame with the top words for each topic\n",
    "df_top_words_rj = pd.DataFrame(top_words_per_topic)\n",
    "df_top_words_transposed_rj = df_top_words_rj.T \n",
    "\n",
    "df_rj_prefixed = df_top_words_transposed_rj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Africa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quant Africa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.1        2.10001212 1.10000064 ... 0.1        0.1        0.1       ]\n",
      " [0.1        0.10000307 0.10000494 ... 0.1        0.1        2.09997195]\n",
      " [0.1        0.1        0.10000319 ... 0.1        0.1        0.1       ]\n",
      " ...\n",
      " [0.1        0.1        0.1        ... 0.1        6.10001962 1.10000319]\n",
      " [0.1        0.10003166 8.09998964 ... 1.09999703 0.10000212 0.10000804]\n",
      " [2.1        1.09995315 0.1        ... 0.1        0.1        0.1       ]]\n"
     ]
    }
   ],
   "source": [
    "tweets_qa = pd.read_csv('/Users/houhiroshisakai/Desktop/Schulich/Term 2/MBAN 6090/quantafrica_since_2018_01_01.csv')\n",
    "\n",
    "texts_qa = tweets_qa['Tweet'].tolist()\n",
    "\n",
    "clean_texts_qa = [clean_text(text) for text in texts_qa]\n",
    "\n",
    "# ContextVectorize\n",
    "X_qa = vectorizer.fit_transform(clean_texts_qa)\n",
    "\n",
    "# Apply LDA\n",
    "lda_qa = LatentDirichletAllocation(n_components=10)\n",
    "topics_qa = lda_qa.fit(X_qa)\n",
    "\n",
    "print(lda_qa.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "javascript community learning learn tech great resource covers provides design\n",
      "Topic #1:\n",
      "quanta tech new africa filled innovation growth happy month link\n",
      "Topic #2:\n",
      "tech join tomorrow pm upskilling day guest today google sharing\n",
      "Topic #3:\n",
      "tech session career pm learning opportunities learn time join hangout\n",
      "Topic #4:\n",
      "startup idea pitch session tuesday product join moving implementing market\n",
      "Topic #5:\n",
      "sir love happy olorunsheyi cheers birthday happybirthday dr sunday new\n",
      "Topic #6:\n",
      "today web language page miss pm used css create shouldnt\n",
      "Topic #7:\n",
      "yes talentdev learn frontend developer months dont create javascript online\n",
      "Topic #8:\n",
      "tech quanta free alimosho ideation community quantaafrica lagos techbros technology\n",
      "Topic #9:\n",
      "data analysis book html provides visualization official visualizations website powerful\n"
     ]
    }
   ],
   "source": [
    "feature_names_qa = vectorizer.get_feature_names_out()\n",
    "print_top_words(lda_qa, feature_names_qa, n_top_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top words per topic\n",
    "top_words_per_topic = []\n",
    "for topic_idx, topic in enumerate(lda_qa.components_):\n",
    "    top_features_ind = topic.argsort()[:-11:-1]\n",
    "    top_features = [feature_names_qa[i] for i in top_features_ind]\n",
    "    top_words_per_topic.append(top_features)\n",
    "\n",
    "# Create a DataFrame with the top words for each topic\n",
    "df_top_words_qa = pd.DataFrame(top_words_per_topic)\n",
    "df_top_words_transposed_qa = df_top_words_qa.T \n",
    "\n",
    "df_qa_prefixed = df_top_words_transposed_qa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.1000158  0.1        0.1        ... 0.1        0.10001266 0.1       ]\n",
      " [0.1        0.1        0.1        ... 1.09999438 0.1        0.1       ]\n",
      " [0.10002195 2.09988241 0.10001399 ... 0.1        0.10001533 0.10000414]\n",
      " ...\n",
      " [0.1        0.1        0.1        ... 1.10000032 0.1        0.1       ]\n",
      " [0.10000554 0.1        2.09995008 ... 0.1        0.1        2.09999586]\n",
      " [0.1        0.1        0.1        ... 0.1        0.1        0.1       ]]\n"
     ]
    }
   ],
   "source": [
    "tweets_sb = pd.read_csv('/Users/houhiroshisakai/Desktop/Schulich/Term 2/MBAN 6090/StandardBankZA_since_2018_01_01.csv')\n",
    "\n",
    "texts_sb = tweets_sb['Tweet'].tolist()\n",
    "\n",
    "clean_texts_sb = [clean_text(text) for text in texts_sb]\n",
    "\n",
    "# ContextVectorize\n",
    "X_sb = vectorizer.fit_transform(clean_texts_sb)\n",
    "\n",
    "# Apply LDA\n",
    "lda_sb = LatentDirichletAllocation(n_components=10)\n",
    "topics_sb = lda_sb.fit_transform(X_sb)\n",
    "\n",
    "print(lda_sb.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "account bank standard hi funds link card hey transaction need\n",
      "Topic #1:\n",
      "sblovessummer sblove financial beatthescam thats goals great year budget youre\n",
      "Topic #2:\n",
      "hi know let getting dm need message thank error screenshot\n",
      "Topic #3:\n",
      "hi experience service details dm branch inconvenience contact like sorry\n",
      "Topic #4:\n",
      "app banking hi try issues know let device thank experiencing\n",
      "Topic #5:\n",
      "thank love sblove appreciate thanks glad feedback happy taking day\n",
      "Topic #6:\n",
      "dm team hi assist number contact details thank send id\n",
      "Topic #7:\n",
      "money instantmoneymondays sblove instant wallet using sblovessummer right welcome thing\n",
      "Topic #8:\n",
      "like dm hi look help send details assist want way\n",
      "Topic #9:\n",
      "sblove instantmoneymondays thats itcanbe answer sure correct got right challenge\n"
     ]
    }
   ],
   "source": [
    "feature_names_sb = vectorizer.get_feature_names_out()\n",
    "print_top_words(lda_sb, feature_names_sb, n_top_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top words per topic\n",
    "top_words_per_topic = []\n",
    "for topic_idx, topic in enumerate(lda_sb.components_):\n",
    "    top_features_ind = topic.argsort()[:-11:-1]\n",
    "    top_features = [feature_names_sb[i] for i in top_features_ind]\n",
    "    top_words_per_topic.append(top_features)\n",
    "\n",
    "# Create a DataFrame with the top words for each topic\n",
    "df_top_words_sb = pd.DataFrame(top_words_per_topic)\n",
    "df_top_words_transposed_sb = df_top_words_sb.T \n",
    "\n",
    "df_sb_prefixed = df_top_words_transposed_sb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## North Thern Trust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.1        0.1        0.1        ... 0.1        3.10002792 0.1       ]\n",
      " [0.1        0.1        0.1        ... 0.1        1.09999223 2.84859821]\n",
      " [2.1        1.09996982 1.09999652 ... 0.1        0.1        0.1       ]\n",
      " ...\n",
      " [0.1        0.1        0.1        ... 4.09971562 1.34055227 0.10001667]\n",
      " [0.1        0.1        0.1        ... 3.09981027 0.1        1.99849218]\n",
      " [0.1        0.1        0.1        ... 0.10044582 1.09998962 0.1       ]]\n"
     ]
    }
   ],
   "source": [
    "tweets_nt = pd.read_csv('/Users/houhiroshisakai/Desktop/Schulich/Term 2/MBAN 6090/NTWealth_since_2018_01_01.csv')\n",
    "\n",
    "texts_nt = tweets_nt['Tweet'].tolist()\n",
    "\n",
    "clean_texts_nt = [clean_text(text) for text in texts_nt]\n",
    "\n",
    "# ContextVectorize\n",
    "X_nt = vectorizer.fit_transform(clean_texts_nt)\n",
    "\n",
    "# Apply LDA\n",
    "lda_nt = LatentDirichletAllocation(n_components=10)\n",
    "topics_nt = lda_nt.fit(X_nt)\n",
    "\n",
    "print(lda_nt.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "wealth plan help learn planning estate retirement market attractive artists\n",
      "Topic #1:\n",
      "tax expochicago suzanne shier art learn exchangebynortherntrust digital arts changes\n",
      "Topic #2:\n",
      "trust northern expochicago learn sponsor management wealth proud family presenting\n",
      "Topic #3:\n",
      "officer fiduciary chief president northerntrust lucina wealth read women named\n",
      "Topic #4:\n",
      "katie nixon inflation investors cio management fed outlook wealth market\n",
      "Topic #5:\n",
      "policy investment tax view business strategies future bank wealth private\n",
      "Topic #6:\n",
      "wealth management nixon katie cio discusses market tax northerntrust investors\n",
      "Topic #7:\n",
      "northern trust wealth family team families planning trusts art executive\n",
      "Topic #8:\n",
      "family new business learn planning tax wealth strategies philanthropy read\n",
      "Topic #9:\n",
      "trust northern proud advice event president experts texas northerntrust wealth\n"
     ]
    }
   ],
   "source": [
    "feature_names_nt = vectorizer.get_feature_names_out()\n",
    "print_top_words(lda_nt, feature_names_nt, n_top_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top words per topic\n",
    "top_words_per_topic = []\n",
    "for topic_idx, topic in enumerate(lda_nt.components_):\n",
    "    top_features_ind = topic.argsort()[:-11:-1]\n",
    "    top_features = [feature_names_nt[i] for i in top_features_ind]\n",
    "    top_words_per_topic.append(top_features)\n",
    "\n",
    "# Create a DataFrame with the top words for each topic\n",
    "df_top_words_nt = pd.DataFrame(top_words_per_topic)\n",
    "df_top_words_transposed_nt = df_top_words_nt.T \n",
    "\n",
    "df_nt_prefixed = df_top_words_transposed_nt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UBA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0999921  0.1        0.10004868 ... 0.1        0.1        0.1       ]\n",
      " [0.1        0.1        0.1        ... 0.10001628 0.1        0.1       ]\n",
      " [0.1        0.1        0.1        ... 0.1        0.1        2.09983807]\n",
      " ...\n",
      " [0.1        0.1        0.1        ... 0.10002886 2.09997987 0.1       ]\n",
      " [0.10005046 0.10035278 2.09995132 ... 2.09978082 0.10001164 0.1000032 ]\n",
      " [0.1        2.09929672 0.1        ... 0.10005318 0.1        0.10003174]]\n"
     ]
    }
   ],
   "source": [
    "tweets_uba = pd.read_csv('/Users/houhiroshisakai/Desktop/Schulich/Term 2/MBAN 6090/UBAGroup_since_2018_01_01.csv')\n",
    "\n",
    "texts_uba = tweets_uba['Tweet'].tolist()\n",
    "\n",
    "clean_texts_uba = [clean_text(text) for text in texts_uba]\n",
    "\n",
    "# ContextVectorize\n",
    "X_uba = vectorizer.fit_transform(clean_texts_uba)\n",
    "\n",
    "# Apply LDA\n",
    "lda_uba = LatentDirichletAllocation(n_components=10)\n",
    "topics = lda_uba.fit_transform(X_uba)\n",
    "\n",
    "print(lda_uba.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "thank hello kindly dm response informed provided bitlymlzbnp number enable\n",
      "Topic #1:\n",
      "leo chat uba account open mmeubachatbanking visit whatsapp using hello\n",
      "Topic #2:\n",
      "africasglobalbank join ubaat ubamarketplace live register session ubaafricanentrepreneurs pm click\n",
      "Topic #3:\n",
      "hello account thank ubacares open dial visit migrate ubabumperaccount bumper\n",
      "Topic #4:\n",
      "uba informed hello thank emanate response number kindly whatsapp dm\n",
      "Topic #5:\n",
      "africasglobalbank ubaat new week stay day like help make work\n",
      "Topic #6:\n",
      "win africasglobalbank winners love better uba airtime million retweet draw\n",
      "Topic #7:\n",
      "africa africasglobalbank african uba ubaafricaday happy world ubaafricaconversations tonyoelumelu africaday\n",
      "Topic #8:\n",
      "link click dm thank sharing hello avoid public bitlymlzbnp information\n",
      "Topic #9:\n",
      "uba africasglobalbank banking group chairman bank tonyoelumelu app today director\n"
     ]
    }
   ],
   "source": [
    "feature_names_uba = vectorizer.get_feature_names_out()\n",
    "print_top_words(lda_uba, feature_names_uba, n_top_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top words per topic\n",
    "top_words_per_topic = []\n",
    "for topic_idx, topic in enumerate(lda_uba.components_):\n",
    "    top_features_ind = topic.argsort()[:-11:-1]\n",
    "    top_features = [feature_names_uba[i] for i in top_features_ind]\n",
    "    top_words_per_topic.append(top_features)\n",
    "\n",
    "# Create a DataFrame with the top words for each topic\n",
    "df_top_words_uba = pd.DataFrame(top_words_per_topic)\n",
    "df_top_words_transposed_uba = df_top_words_uba.T \n",
    "\n",
    "df_uba_prefixed = df_top_words_transposed_uba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Asia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HSBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.1        0.1000038  0.1        ... 1.09999882 0.10001795 0.1       ]\n",
      " [1.10000044 0.1        0.1        ... 0.1        0.1        0.1       ]\n",
      " [0.1        0.1        1.09999401 ... 0.1        1.10000273 0.1       ]\n",
      " ...\n",
      " [0.1        0.1        1.09998613 ... 0.1        0.1        1.09999782]\n",
      " [0.1000039  0.1        1.10000066 ... 0.1        0.1        0.1       ]\n",
      " [0.1        0.1        2.1000084  ... 1.10000117 2.09997932 0.1       ]]\n"
     ]
    }
   ],
   "source": [
    "tweets_hsbc = pd.read_csv('/Users/houhiroshisakai/Desktop/Schulich/Term 2/MBAN 6090/HSBC_since_2018_01_01.csv')\n",
    "\n",
    "texts_hsbc = tweets_hsbc['Tweet'].tolist()\n",
    "\n",
    "clean_texts_hsbc = [clean_text(text) for text in texts_hsbc]\n",
    "\n",
    "# ContextVectorize\n",
    "X_hsbc = vectorizer.fit_transform(clean_texts_hsbc)\n",
    "\n",
    "# Apply LDA\n",
    "lda_hsbc = LatentDirichletAllocation(n_components=10)\n",
    "topics = lda_hsbc.fit(X_hsbc)\n",
    "\n",
    "print(lda_hsbc.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "global finance transition trade sustainable hsbc world hsbcs bank netzero\n",
      "Topic #1:\n",
      "hsbc new world people support action group hong technology partnership\n",
      "Topic #2:\n",
      "hsbc global business climate solutions world help customers passed new\n",
      "Topic #3:\n",
      "hsbc future read trade global sustainable hsbcresults banking businesses growth\n",
      "Topic #4:\n",
      "today results hsbcresults learn weve announced set trade hsbc world\n",
      "Topic #5:\n",
      "hsbc financial business year partnership people iwd businesses looking work\n",
      "Topic #6:\n",
      "entrepreneurs latest report podcast business read hsbc entrepreneur listen founder\n",
      "Topic #7:\n",
      "hsbc business global proud bank customers weve world new banking\n",
      "Topic #8:\n",
      "hsbcnavigator businesses hsbc china companies supply ciie sustainability business chinas\n",
      "Topic #9:\n",
      "digital health mental customers group support hsbc help covid year\n"
     ]
    }
   ],
   "source": [
    "feature_names_hsbc = vectorizer.get_feature_names_out()\n",
    "print_top_words(lda_hsbc, feature_names_hsbc, n_top_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top words per topic\n",
    "top_words_per_topic = []\n",
    "for topic_idx, topic in enumerate(lda_hsbc.components_):\n",
    "    top_features_ind = topic.argsort()[:-11:-1]\n",
    "    top_features = [feature_names_hsbc[i] for i in top_features_ind]\n",
    "    top_words_per_topic.append(top_features)\n",
    "\n",
    "# Create a DataFrame with the top words for each topic\n",
    "df_top_words_hsbc = pd.DataFrame(top_words_per_topic)\n",
    "df_top_words_transposed_hsbc = df_top_words_hsbc.T \n",
    "\n",
    "df_hsbc_prefixed = df_top_words_transposed_hsbc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OCBC (Singapore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.1        1.09996532 0.1        ... 1.00154074 0.10000694 0.1       ]\n",
      " [2.38407348 0.1        0.10000242 ... 0.1        0.1        0.10010147]\n",
      " [1.5904087  0.10000799 0.10002953 ... 1.19845925 0.1        0.10016341]\n",
      " ...\n",
      " [0.10000772 0.1        0.1        ... 0.1        0.1        0.1       ]\n",
      " [0.1        0.1        0.10002676 ... 0.1        0.1        0.1       ]\n",
      " [0.1        0.1        0.1        ... 0.1        0.1        0.10002146]]\n"
     ]
    }
   ],
   "source": [
    "tweets_ocbc = pd.read_csv('/Users/houhiroshisakai/Desktop/Schulich/Term 2/MBAN 6090/OCBCBank_since_2018_01_01.csv')\n",
    "\n",
    "texts_ocbc = tweets_ocbc['Tweet'].tolist()\n",
    "\n",
    "clean_texts_ocbc = [clean_text(text) for text in texts_ocbc]\n",
    "\n",
    "# ContextVectorize\n",
    "X_ocbc = vectorizer.fit_transform(clean_texts_ocbc)\n",
    "\n",
    "# Apply LDA\n",
    "lda_ocbc = LatentDirichletAllocation(n_components=10)\n",
    "topics_ocbc = lda_ocbc.fit(X_ocbc)\n",
    "\n",
    "print(lda_ocbc.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "ocbc account hi singapore sh touch malaysia looking dg reached\n",
      "Topic #1:\n",
      "app hi try dg banking mobile reinstalling deleting delete issues\n",
      "Topic #2:\n",
      "dg hi able number service mobile drop dm executive assist\n",
      "Topic #3:\n",
      "sh card xf form update hi thank new youre afraid\n",
      "Topic #4:\n",
      "know hi xf transfer let cash ocbc thank inconvenience caused\n",
      "Topic #5:\n",
      "app version mobile banking phone ocbc following bank model details\n",
      "Topic #6:\n",
      "dm thank hi contact xf share number details sorry drop\n",
      "Topic #7:\n",
      "hi access dg banking online pin sh token sorry look\n",
      "Topic #8:\n",
      "thank team feedback working xf hi inconvenience relevant dg soon\n",
      "Topic #9:\n",
      "sh thanks email sorry dm free feel glad good hi\n"
     ]
    }
   ],
   "source": [
    "feature_names_ocbc = vectorizer.get_feature_names_out()\n",
    "print_top_words(lda_ocbc, feature_names_ocbc, n_top_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top words per topic\n",
    "top_words_per_topic = []\n",
    "for topic_idx, topic in enumerate(lda_ocbc.components_):\n",
    "    top_features_ind = topic.argsort()[:-11:-1]\n",
    "    top_features = [feature_names_ocbc[i] for i in top_features_ind]\n",
    "    top_words_per_topic.append(top_features)\n",
    "\n",
    "# Create a DataFrame with the top words for each topic\n",
    "df_top_words_ocbc = pd.DataFrame(top_words_per_topic)\n",
    "df_top_words_transposed_ocbc = df_top_words_ocbc.T \n",
    "\n",
    "df_ocbc_prefixed = df_top_words_transposed_ocbc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bank of Singapore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.10000537 0.10003381 0.1        ... 0.1        0.10001324 4.91601106]\n",
      " [0.1        1.09996214 0.10005    ... 0.1        0.10007186 0.33883278]\n",
      " [2.10000006 1.09996103 1.09994606 ... 0.1        5.16034967 0.1       ]\n",
      " ...\n",
      " [0.1        0.1        0.1        ... 0.1        0.10000527 0.10000181]\n",
      " [0.1        1.09997473 0.1        ... 0.1        0.10004081 1.04514537]\n",
      " [0.10002537 0.1        0.1        ... 1.10000166 0.1        0.10000898]]\n"
     ]
    }
   ],
   "source": [
    "tweets_bos = pd.read_csv('/Users/houhiroshisakai/Desktop/Schulich/Term 2/MBAN 6090/bankofSG_since_2018_01_01.csv')\n",
    "\n",
    "texts_bos = tweets_bos['Tweet'].tolist()\n",
    "\n",
    "clean_texts_bos = [clean_text(text) for text in texts_bos]\n",
    "\n",
    "# ContextVectorize\n",
    "X_bos = vectorizer.fit_transform(clean_texts_bos)\n",
    "\n",
    "# Apply LDA\n",
    "lda_bos = LatentDirichletAllocation(n_components=10)\n",
    "topics_bos = lda_bos.fit(X_bos)\n",
    "\n",
    "print(lda_bos.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "chief mansoor inflation mohiuddin economist federal rate investment central reserve\n",
      "Topic #1:\n",
      "strategist moh siong sim currency says usd likely fx outlook\n",
      "Topic #2:\n",
      "chief economist mohiuddin mansoor fed policy inflation says assets risk\n",
      "Topic #3:\n",
      "chief economist fed says jerram richard mansoor mohiuddin rate bank\n",
      "Topic #4:\n",
      "china market global head greater markets asia singapore north research\n",
      "Topic #5:\n",
      "private wealth investment senior bank cheo james strategist global market\n",
      "Topic #6:\n",
      "management portfolio global research jean head chia office year investment\n",
      "Topic #7:\n",
      "investment head lee market strategy eli global says months watch\n",
      "Topic #8:\n",
      "head fixed income global expected says products van walle marc\n",
      "Topic #9:\n",
      "investment year singapore bank guests outlook research change world term\n"
     ]
    }
   ],
   "source": [
    "feature_names_bos = vectorizer.get_feature_names_out()\n",
    "print_top_words(lda_bos, feature_names_bos, n_top_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top words per topic\n",
    "top_words_per_topic = []\n",
    "for topic_idx, topic in enumerate(lda_bos.components_):\n",
    "    top_features_ind = topic.argsort()[:-11:-1]\n",
    "    top_features = [feature_names_bos[i] for i in top_features_ind]\n",
    "    top_words_per_topic.append(top_features)\n",
    "\n",
    "# Create a DataFrame with the top words for each topic\n",
    "df_top_words_bos = pd.DataFrame(top_words_per_topic)\n",
    "df_top_words_transposed_bos = df_top_words_bos.T \n",
    "\n",
    "df_bos_prefixed = df_top_words_transposed_bos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hana Bank (South Korea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.1         0.1         0.1         0.1         0.10000445  0.1\n",
      "   0.1         0.1         0.1         2.10000658  0.1         0.1\n",
      "   0.10000686  0.1         1.09996357  0.1         0.1         2.09996948\n",
      "   0.1         0.10000532  0.1         0.1         0.1         0.10001433\n",
      "   0.1         0.1         0.1         0.1         0.1000528   0.1\n",
      "   0.10000441  0.1         0.1         2.09998477  1.09995771  0.1\n",
      "   1.36533975  0.1         0.1         1.09997508  0.10001972  0.1\n",
      "   1.09999166  0.1         0.1         0.1         0.1         2.10001903\n",
      "   0.1         5.10002669  0.10000549  0.1         0.1         0.10000901\n",
      "   1.09999627  0.1         0.10002832  0.1         0.1         0.10000532\n",
      "   0.10000675  0.1         0.1         2.10004161  1.09996406  0.1\n",
      "   0.10000882  0.1         1.10000405  0.1         1.0999726   0.1\n",
      "   0.1         0.10000328  0.10001322  1.09996304  0.10000315  0.1\n",
      "   2.34739198  0.1         0.1         1.09997314  0.1         0.1\n",
      "   0.1         3.09998083  2.10000374  0.10004199  1.09997067  1.09997314\n",
      "   0.1         0.1         0.1         0.10000308  0.64645712  0.1\n",
      "   0.1         0.10000532  0.1         0.1       ]\n",
      " [ 0.1         0.1         1.09999099  0.1         0.1         0.1\n",
      "   1.10001426  0.1         0.1         0.1         1.10000887  0.1\n",
      "   0.10000198  0.1         0.1         0.1         0.1         0.10001186\n",
      "   0.1         0.1         1.09999852  0.1         1.09996834  0.1\n",
      "   0.1         0.1         0.1         0.1         0.1         0.1\n",
      "   0.1         0.1         1.09999664  0.1         1.10001468  0.1\n",
      "   0.10002116  1.10000119  0.1         0.1         0.1         0.1\n",
      "   0.1         0.1         0.1         0.10000001  0.1         0.1\n",
      "   0.1         1.09998405  0.1         1.10000381  0.1         0.100001\n",
      "   1.10002729  0.1         2.09998823  0.1         0.1         0.1\n",
      "   0.1         0.1         0.1         0.1         1.10000999  0.1\n",
      "   0.10000177  0.1         0.1         2.09999573  0.1         0.1\n",
      "   0.1         0.1         0.1         0.1         1.13510301  0.1\n",
      "   0.1         0.1         1.10000886  0.1         0.1         0.1\n",
      "   0.10007916  0.1         0.1         0.10002716  0.1         0.1\n",
      "   0.1         0.1         0.1         0.1         0.10000249  0.1\n",
      "   0.1         0.1         1.10002369  0.1       ]\n",
      " [ 0.1         1.09998368  0.1         0.1         2.10002996  0.1\n",
      "   3.02694159  0.1         0.1         1.09999341  1.09999112  0.1\n",
      "   2.09997242  3.1         0.1         2.099999    0.1         0.1\n",
      "   0.1         0.1         0.1         0.1         1.10002509  1.09997473\n",
      "   3.09999014  0.1         0.1         1.09999368  0.1         0.1\n",
      "   0.1         1.09999746  0.1         0.1         0.1         1.09999421\n",
      "   0.1         0.1         2.09992472  1.09998647  0.1         0.10009834\n",
      "   0.1         1.09998831  0.1         0.10011669  1.09999304  0.1\n",
      "   0.1         0.1         0.1         5.09997706  0.1         0.10000719\n",
      "   1.09997581  1.09999886  1.09997107  1.09999813  0.1         0.1\n",
      "   0.1         2.09995068  0.1         2.10000783  0.1         2.09999499\n",
      "   0.10001272  1.09999546  1.09999594  0.1         1.09999978  0.10000971\n",
      "   1.09999455  0.1         0.1         0.1         0.1         0.1\n",
      "   1.09993678  0.10002602  1.09999113  0.1         2.10000727  1.09999467\n",
      "   0.1         0.1         1.24527586  0.1         2.0999991   0.1\n",
      "   0.1         1.09999368  0.1         0.1         0.10001793  0.1\n",
      "   0.10000971  0.1         0.1         1.09999872]\n",
      " [ 0.1         0.1         0.1         0.1         1.21543877  0.1\n",
      "   0.1         0.1         1.100001    0.1         0.1         0.1\n",
      "   0.1         0.1         0.10002196  0.1         1.0999984   0.1\n",
      "   0.1         0.10000608  0.1         0.1         0.1         1.1720343\n",
      "   0.1         0.1         0.1         0.1         0.1         0.1\n",
      "   0.10000503  1.10000253  0.1         0.1         0.1         1.10000578\n",
      "   0.10001347  1.0999988   3.10004193  0.1         0.1         0.1\n",
      "   0.1         1.10003574  0.1         0.10008194  2.10004656  0.1\n",
      "   0.1         1.09997958  2.38676581  0.10000455  1.10001085  0.10000163\n",
      "   0.1         0.1         0.1         0.1         0.1         0.1\n",
      "   0.1         0.10001291  0.10001618  0.1         0.1         0.1\n",
      "   0.100004    0.1         0.1         0.1         0.1         0.10000684\n",
      "   1.10001828  0.10000374  0.10000164  1.10000934  0.10001057  0.1\n",
      "   0.1         2.09992268  0.1         1.0999637   0.1         0.1\n",
      "   0.1         0.1         0.10000384  0.1         1.09996104  1.0999637\n",
      "   0.1         1.10000631  0.1         0.10000352  0.1         0.1\n",
      "   0.10000684  0.10000608  0.1         0.1       ]\n",
      " [ 0.1         0.1         0.1         0.1         0.10000132  0.1\n",
      "   0.10000772  0.1         0.1         0.1         0.1         0.1\n",
      "   8.10001873  0.1         5.09997601  0.1         0.1         0.1\n",
      "   0.1         0.10000179  0.1         0.1         0.10015647  4.91951085\n",
      "   0.10000311  0.1         0.1         0.1         7.37099939  0.1\n",
      "   0.10000148  0.1         0.1         0.1000322   0.10002761  0.1\n",
      "   0.10000396  0.1         0.1         0.10003844  0.1         0.10009891\n",
      "   0.1         0.1         0.1         0.1         0.1         0.10001809\n",
      "   0.1         0.10000368  0.10000131  0.10000176  0.1        15.6204135\n",
      "   0.1         0.1         0.1         0.1         0.1         0.1\n",
      "   0.1         0.1         0.10000476  0.1         0.10002594  0.10000501\n",
      "   9.03507799  0.1         0.1         0.1         0.10002761  0.1\n",
      "   0.1         0.10000217 11.24582773  0.10002761 10.06486186  0.1\n",
      "   0.1         0.1         0.1         0.10001338  0.1         0.1\n",
      "   0.1         0.1         0.1         0.1         0.1000184   0.10001338\n",
      "   0.1         0.1         0.1         0.10000204  6.76647705  9.81548498\n",
      "   0.1         0.10000179  0.1         0.1       ]\n",
      " [ 0.1         0.1         1.09998928  1.10003638  0.1         2.0999927\n",
      "   0.1         0.1         1.099999    0.1         0.1         1.09999518\n",
      "   0.1         0.1         0.1         0.1         1.1000016   0.10000801\n",
      "   1.09998956  0.1         0.1         3.10000657  0.1         0.1\n",
      "   0.1         1.09999003  1.1000008   1.1000063   0.1         0.1\n",
      "   0.1         0.1         0.1         0.1         0.1         0.1\n",
      "   0.10001428  0.1         1.09998449  0.1         0.1         0.1\n",
      "   0.1         0.1         0.1         0.1         0.1         0.1\n",
      "   0.1         0.1         0.1         0.1         0.1         0.1\n",
      "   1.10000062  1.10000114  0.10001237  1.10000186  1.09999726  0.1\n",
      "   3.09998945  0.1         0.1         1.10001572  0.1         0.1\n",
      "   0.1         1.10000562  0.1         0.1         0.1         0.1\n",
      "   0.1         0.1         0.1         0.1         0.1         0.1\n",
      "   3.85267122  1.09999955  0.1         0.1         1.09999273  0.1\n",
      "   0.1         0.10001916  0.1         2.09993084  0.1         0.1\n",
      "   0.1         0.1         1.1000008   0.1         0.1         0.1\n",
      "   0.1         0.1         0.1         0.1       ]\n",
      " [ 2.09999522  1.10001631  2.099999    0.1         0.1         0.1\n",
      "   0.1         0.1         0.1         0.1         0.1         0.1\n",
      "   0.1         0.1         0.1         0.1         0.1         0.1\n",
      "   1.10001043  0.1         0.1         1.09999342  0.1         0.1\n",
      "   0.1         2.09999705  1.0999992   0.1         0.1         0.1\n",
      "   0.1         0.1         1.099992    0.1         0.1         0.1\n",
      "   0.1         0.1         3.10000141  0.1         2.09998027  0.1\n",
      "   2.10000834  0.1         2.1         0.10022238  1.09996039  0.1\n",
      "   0.1         0.100006    2.09996958  0.10001217  0.1         0.1\n",
      "   0.1         0.1         0.1         0.1         0.1         0.1\n",
      "   1.09998754  0.10003461  0.1         1.10000725  0.1         0.1\n",
      "   0.1         1.09999891  0.1         0.10000426  0.1         2.09998249\n",
      "   1.09998716  0.1         0.1         0.1         0.10001008  2.1\n",
      "   0.1         0.10004918  0.1         0.1         0.1         0.1\n",
      "   2.09992083  0.1         2.95471601  0.1         0.1         0.1\n",
      "   0.1         0.1         1.0999992   0.1         0.1         0.1\n",
      "   2.09998249  0.1         0.1         1.10000128]\n",
      " [ 1.10000478  0.1         1.10001611  0.10000001  0.1         0.1\n",
      "   0.1         0.10000001  0.1         0.1         0.1         1.10000482\n",
      "   0.1         0.1         0.1         0.1         0.1         0.1\n",
      "   0.1         0.1         0.1         0.1         0.10000001  0.1\n",
      "   0.1         1.10001092  0.1         0.1         0.1         0.1\n",
      "   0.1         0.1         1.10001136  0.1         0.1         0.1\n",
      "   0.10000001  0.1         1.10003379  0.1         0.1         0.10000001\n",
      "   0.1         0.1         0.1         0.10000001  0.1         0.1\n",
      "   0.1         0.1         0.1         0.1         0.1         0.1\n",
      "   0.1         0.1         0.1         0.1         0.1         0.1\n",
      "   1.10001625  0.1         0.1         0.1         0.1         0.1\n",
      "   0.1         0.1         0.1         0.1         0.1         0.1\n",
      "   0.1         0.1         0.1         0.1         0.1         0.1\n",
      "   0.1         0.1         0.1         0.1         0.1         0.1\n",
      "   0.1         0.1         0.1         0.1         0.1         0.1\n",
      "   0.1         0.1         0.1         0.1         0.1         0.1\n",
      "   0.1         0.1         0.1         0.1       ]\n",
      " [ 0.1         0.1         0.1         1.09996359 11.98452548  0.1\n",
      "   4.17303642  1.09993187  0.1         0.1         0.1         0.1\n",
      "   0.1         0.1         0.10003844  0.100001    0.1         0.10001065\n",
      "   0.1        10.09998681  0.1         0.1         1.09985006 16.20846577\n",
      "   0.1         0.1         0.1         0.1        10.82894779  3.09999999\n",
      "  12.09998908  0.1         0.1         5.09998303  0.1         0.1\n",
      "   4.83461566  0.1         0.100001    0.1         0.1         6.09980271\n",
      "   0.1         1.09997594  0.1         3.09957895  0.1         2.09996286\n",
      "   0.1         0.1         9.8132578   0.10000064  2.09998914 27.57956767\n",
      "   0.1         0.1         0.1         0.1         0.1        10.09999467\n",
      "   0.1         0.1000018   4.09997906  1.09987681  0.1         0.1\n",
      "  10.16489469  0.1         0.1         0.1         0.1         0.10000096\n",
      "   0.1        16.09999081 29.95415741  0.1         0.10001134  0.1\n",
      "   0.1         0.10000256  0.1         0.10004978  0.1         0.1\n",
      "   0.1         0.1         0.10000054  0.1         0.10005078  0.10004978\n",
      "   3.09999999  0.1         0.1        17.09999137  3.8870454   3.384515\n",
      "   0.10000096 10.09998681  1.0999763   0.1       ]\n",
      " [ 0.1         0.1         1.10000462  0.1         0.1         1.10000729\n",
      "   0.1         1.10006811  0.1         0.1         0.1         0.1\n",
      "   0.1         0.1         0.1         0.1         0.1         0.1\n",
      "   0.1         0.1         1.10000148  0.1         0.10000001  0.1\n",
      "   1.10000675  1.100002    0.1         0.1         0.1         0.1\n",
      "   0.1         0.1         0.1         0.1         0.1         0.1\n",
      "   1.09999172  0.1         1.10001266  0.1         0.1         0.10000001\n",
      "   0.1         0.1         0.1         0.10000001  0.1         0.1\n",
      "   2.1         0.1         0.1         0.1         0.1         0.1\n",
      "   0.1         0.1         0.1         0.1         1.10000274  0.1\n",
      "   0.1         0.1         0.1         1.10005078  0.1         0.1\n",
      "   0.1         0.1         0.1         0.1         0.1         0.1\n",
      "   0.1         0.1         0.1         0.1         0.1         0.1\n",
      "   0.1         0.1         0.1         0.1         0.1         1.10000532\n",
      "   0.1         0.1         0.1         0.1         0.1         0.1\n",
      "   0.1         0.1         0.1         0.1         0.1         0.1\n",
      "   0.1         0.1         0.1         0.1       ]]\n"
     ]
    }
   ],
   "source": [
    "tweets_hana = pd.read_csv('/Users/houhiroshisakai/Desktop/Schulich/Term 2/MBAN 6090/HanaBank4expats_since_2018_01_01.csv')\n",
    "\n",
    "texts_hana = tweets_hana['Tweet'].tolist()\n",
    "\n",
    "clean_texts_hana = [clean_text(text) for text in texts_hana]\n",
    "\n",
    "# ContextVectorize\n",
    "X_hana = vectorizer.fit_transform(clean_texts_hana)\n",
    "\n",
    "# Apply LDA\n",
    "lda_hana = LatentDirichletAllocation(n_components=10)\n",
    "topics = lda_hana.fit(X_hana)\n",
    "\n",
    "print(lda_hana.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "korea tips seoul new kebhana card travel fx day good\n",
      "Topic #1:\n",
      "parent local report like year gives banks news change starting\n",
      "Topic #2:\n",
      "koreas coast expats banks bank new sunday tuesday cool online\n",
      "Topic #3:\n",
      "hana korean keb st bank exchange hope pyeongchang krw rates\n",
      "Topic #4:\n",
      "krwusd rate report weeks outlook check forecast weekly complete exchange\n",
      "Topic #5:\n",
      "seoul easy money banking trip arent new fly overseas locations\n",
      "Topic #6:\n",
      "hana travel home send journey app ez abroad win prizes\n",
      "Topic #7:\n",
      "hana money app foreigners ez cheaper abroad july english heres\n",
      "Topic #8:\n",
      "rate krwusd week exchange range forecasts bank forecast outlook monday\n",
      "Topic #9:\n",
      "know beaches new hana banking expats tell app mobile ez\n"
     ]
    }
   ],
   "source": [
    "feature_names_hana = vectorizer.get_feature_names_out()\n",
    "print_top_words(lda_hana, feature_names_hana, n_top_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top words per topic\n",
    "top_words_per_topic = []\n",
    "for topic_idx, topic in enumerate(lda_hana.components_):\n",
    "    top_features_ind = topic.argsort()[:-11:-1]\n",
    "    top_features = [feature_names_hana[i] for i in top_features_ind]\n",
    "    top_words_per_topic.append(top_features)\n",
    "\n",
    "# Create a DataFrame with the top words for each topic\n",
    "df_top_words_hana = pd.DataFrame(top_words_per_topic)\n",
    "df_top_words_transposed_hana = df_top_words_hana.T \n",
    "\n",
    "df_hana_prefixed = df_top_words_transposed_hana"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UFJ (Japan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mizuho Bank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oceania"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Australia and New Zealand Banking Group Limited  (ANZ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.00000000e-01 1.57288192e+02 1.00000306e-01 ... 1.33508027e+00\n",
      "  1.00000000e-01 1.00000000e-01]\n",
      " [1.00000001e-01 4.65041634e+00 1.20641942e+00 ... 1.00000001e-01\n",
      "  1.00015900e-01 1.00000001e-01]\n",
      " [1.00000001e-01 1.38001472e-01 1.00000000e-01 ... 1.00071767e-01\n",
      "  1.00000000e-01 1.00103423e-01]\n",
      " ...\n",
      " [1.00165812e-01 5.20725220e+01 1.00032045e-01 ... 1.09983720e+00\n",
      "  3.00930633e+00 1.00026929e-01]\n",
      " [1.00000001e-01 1.00018668e-01 1.00000000e-01 ... 5.55890794e+00\n",
      "  1.00000000e-01 2.09986697e+00]\n",
      " [1.00000001e-01 1.00014294e-01 1.00051793e-01 ... 1.00008386e-01\n",
      "  1.00000000e-01 1.00030829e-01]]\n"
     ]
    }
   ],
   "source": [
    "tweets_anz = pd.read_csv('/Users/houhiroshisakai/Desktop/Schulich/Term 2/MBAN 6090/ANZ_AU_since_2018_01_01.csv')\n",
    "\n",
    "texts_anz = tweets_anz['Tweet'].tolist()\n",
    "\n",
    "clean_texts_anz = [clean_text(text) for text in texts_anz]\n",
    "\n",
    "# ContextVectorize\n",
    "X_anz = vectorizer.fit_transform(clean_texts_anz)\n",
    "\n",
    "# Apply LDA\n",
    "lda_anz = LatentDirichletAllocation(n_components=10)\n",
    "topics_anz = lda_anz.fit(X_anz)\n",
    "\n",
    "print(lda_anz.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "hi inconvenience anz issues sorry app issue banking caused internet\n",
      "Topic #1:\n",
      "hi wait long thanks sorry times team message hoax email\n",
      "Topic #2:\n",
      "card hi team ampm credit aest account contact cards like\n",
      "Topic #3:\n",
      "hi thanks sorry hear feedback dm help team know message\n",
      "Topic #4:\n",
      "banking internet anz pm app hi team aest contact try\n",
      "Topic #5:\n",
      "hi dm branch contact number postcode sorry send details like\n",
      "Topic #6:\n",
      "link dont thanks hi click delete anz sure sms details\n",
      "Topic #7:\n",
      "anz hi account payments funds business pay transfer payment card\n",
      "Topic #8:\n",
      "dm number post code send hi contact phone assist sorry\n",
      "Topic #9:\n",
      "hi thanks darren payments team sorry kindly customer days pm\n"
     ]
    }
   ],
   "source": [
    "feature_names_anz = vectorizer.get_feature_names_out()\n",
    "print_top_words(lda_anz, feature_names_anz, n_top_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top words per topic\n",
    "top_words_per_topic = []\n",
    "for topic_idx, topic in enumerate(lda_anz.components_):\n",
    "    top_features_ind = topic.argsort()[:-11:-1]\n",
    "    top_features = [feature_names_anz[i] for i in top_features_ind]\n",
    "    top_words_per_topic.append(top_features)\n",
    "\n",
    "# Create a DataFrame with the top words for each topic\n",
    "df_top_words_anz = pd.DataFrame(top_words_per_topic)\n",
    "df_top_words_transposed_anz = df_top_words_anz.T \n",
    "\n",
    "df_anz_prefixed = df_top_words_transposed_anz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commonwealth Bank of Australia (CBA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.05159575 0.1        1.11780792 ... 1.09999723 0.10000173 4.10001254]\n",
      " [0.10000714 1.1000023  0.1        ... 0.1        1.88172015 0.10001116]\n",
      " [0.1        0.1        0.1        ... 0.1        1.28875754 0.1       ]\n",
      " ...\n",
      " [0.1        0.1        1.14729049 ... 1.10000277 0.10000108 1.09999188]\n",
      " [0.1        0.1        0.1        ... 0.1        3.58485722 0.1       ]\n",
      " [0.10000499 1.0999977  0.1        ... 0.1        0.10002469 1.11299113]]\n"
     ]
    }
   ],
   "source": [
    "tweets_cba = pd.read_csv('/Users/houhiroshisakai/Desktop/Schulich/Term 2/MBAN 6090/CommBank_since_2018_01_01.csv')\n",
    "\n",
    "texts_cba = tweets_cba['Tweet'].tolist()\n",
    "\n",
    "clean_texts_cba = [clean_text(text) for text in texts_cba]\n",
    "\n",
    "# ContextVectorize\n",
    "X_cba = vectorizer.fit_transform(clean_texts_cba)\n",
    "\n",
    "# Apply LDA\n",
    "lda_cba = LatentDirichletAllocation(n_components=10)\n",
    "topics_cba = lda_cba.fit_transform(X_cba)\n",
    "\n",
    "print(lda_cba.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "hi know reach card nat team help information dm concerns\n",
      "Topic #1:\n",
      "dm hi help send enquiry provide assist understand like information\n",
      "Topic #2:\n",
      "footprint hi carbon accounts data scam business based industry calculated\n",
      "Topic #3:\n",
      "scams information report need security account forwarding future hoaxcbacomau customers\n",
      "Topic #4:\n",
      "message send hi dm commbank app details help thank assist\n",
      "Topic #5:\n",
      "dm hi like send details sorry best discuss assistance reach\n",
      "Topic #6:\n",
      "send like hi message number contact private hear understand sorry\n",
      "Topic #7:\n",
      "hi customers information cryptocurrency app commbank thanks personal team scams\n",
      "Topic #8:\n",
      "message thanks hi team scam text forward card security details\n",
      "Topic #9:\n",
      "team christine visit sorry check hello hear know suspicious hi\n"
     ]
    }
   ],
   "source": [
    "feature_names_cba = vectorizer.get_feature_names_out()\n",
    "print_top_words(lda_cba, feature_names_cba, n_top_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top words per topic\n",
    "top_words_per_topic = []\n",
    "for topic_idx, topic in enumerate(lda_cba.components_):\n",
    "    top_features_ind = topic.argsort()[:-11:-1]\n",
    "    top_features = [feature_names_cba[i] for i in top_features_ind]\n",
    "    top_words_per_topic.append(top_features)\n",
    "\n",
    "# Create a DataFrame with the top words for each topic\n",
    "df_top_words_cba = pd.DataFrame(top_words_per_topic)\n",
    "df_top_words_transposed_cba = df_top_words_cba.T \n",
    "\n",
    "df_cba_prefixed = df_top_words_transposed_cba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## National Australia Bank (NAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.10001587 0.1        0.1        ... 0.1        0.10010085 0.1       ]\n",
      " [0.10000956 4.07950768 0.1        ... 3.09995381 3.09980832 0.10013924]\n",
      " [0.1        0.10000701 0.1        ... 0.10000651 0.1        0.1       ]\n",
      " ...\n",
      " [0.1        0.10003732 2.09999911 ... 0.1        0.1        0.1       ]\n",
      " [1.09997821 0.10000616 0.1        ... 0.10001724 0.1        0.1       ]\n",
      " [1.09996565 5.33766783 0.10000089 ... 0.1        0.1        1.09991184]]\n"
     ]
    }
   ],
   "source": [
    "tweets_nab = pd.read_csv('/Users/houhiroshisakai/Desktop/Schulich/Term 2/MBAN 6090/NAB_since_2018_01_01.csv')\n",
    "\n",
    "texts_nab = tweets_nab['Tweet'].tolist()\n",
    "\n",
    "clean_texts_nab = [clean_text(text) for text in texts_nab]\n",
    "\n",
    "# ContextVectorize\n",
    "X_nab = vectorizer.fit_transform(clean_texts_nab)\n",
    "\n",
    "# Apply LDA\n",
    "lda_nab = LatentDirichletAllocation(n_components=10)\n",
    "topics_nab = lda_nab.fit_transform(X_nab)\n",
    "\n",
    "print(lda_nab.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "pay nab apple know hi let make payments contactless customers\n",
      "Topic #1:\n",
      "dm send hi chat im help sorry hear rg like\n",
      "Topic #2:\n",
      "hi sorry working inconvenience issues banking app internet aware issue\n",
      "Topic #3:\n",
      "nab read australia atm systems new nabs fee cash scams\n",
      "Topic #4:\n",
      "rl card thats hi able cj youre message account lh\n",
      "Topic #5:\n",
      "hear feedback sorry thanks hi team need im glad know\n",
      "Topic #6:\n",
      "hi ive like team lodge pm time feedback replied youd\n",
      "Topic #7:\n",
      "hi nab pm rate customers payments home loan understand aestaedt\n",
      "Topic #8:\n",
      "nab business hi customers support accounts need banking customer branch\n",
      "Topic #9:\n",
      "thanks message delete team hi messages security scam letting know\n"
     ]
    }
   ],
   "source": [
    "feature_names_nab = vectorizer.get_feature_names_out()\n",
    "print_top_words(lda_nab, feature_names_nab, n_top_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top words per topic\n",
    "top_words_per_topic = []\n",
    "for topic_idx, topic in enumerate(lda_nab.components_):\n",
    "    top_features_ind = topic.argsort()[:-11:-1]\n",
    "    top_features = [feature_names_nab[i] for i in top_features_ind]\n",
    "    top_words_per_topic.append(top_features)\n",
    "\n",
    "# Create a DataFrame with the top words for each topic\n",
    "df_top_words_nab = pd.DataFrame(top_words_per_topic)\n",
    "df_top_words_transposed_nab = df_top_words_nab.T \n",
    "\n",
    "df_nab_prefixed = df_top_words_transposed_nab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Westpac Banking Corporation (WBC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.10001007 0.1        5.32199946 ... 0.10002027 0.1000326  0.1       ]\n",
      " [0.1000539  0.10000352 2.86758631 ... 0.10022302 0.1        0.1       ]\n",
      " [0.1        0.1        2.8346606  ... 0.1        1.28933772 3.09997072]\n",
      " ...\n",
      " [0.1        0.10000452 8.82877994 ... 0.10013353 0.10000206 0.1       ]\n",
      " [0.10000091 2.09998376 0.10004913 ... 0.1        0.10002904 0.1       ]\n",
      " [0.10009707 0.1        0.10000349 ... 0.1        0.1        0.1       ]]\n"
     ]
    }
   ],
   "source": [
    "tweets_wbc = pd.read_csv('/Users/houhiroshisakai/Desktop/Schulich/Term 2/MBAN 6090/Westpac_since_2018_01_01.csv')\n",
    "\n",
    "texts_wbc = tweets_wbc['Tweet'].tolist()\n",
    "\n",
    "clean_texts_wbc = [clean_text(text) for text in texts_wbc]\n",
    "\n",
    "# ContextVectorize\n",
    "X_wbc = vectorizer.fit_transform(clean_texts_wbc)\n",
    "\n",
    "# Apply LDA\n",
    "lda_wbc = LatentDirichletAllocation(n_components=10)\n",
    "topics_wbc = lda_wbc.fit_transform(X_wbc)\n",
    "\n",
    "print(lda_wbc.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "send dm like hi help sorry hear wed concerns whats\n",
      "Topic #1:\n",
      "dm responded know hi weve message send look tweet forward\n",
      "Topic #2:\n",
      "westpac sms delete forward banking scam hoax hi text email\n",
      "Topic #3:\n",
      "im sorry youre hear hi dm help send youve able\n",
      "Topic #4:\n",
      "banking online hi inconvenience caused apologise issues resolved services working\n",
      "Topic #5:\n",
      "thanks hi team reaching details share feedback know card attention\n",
      "Topic #6:\n",
      "pay apple hi open future customers remain offering options westpac\n",
      "Topic #7:\n",
      "help team hi contact know thanks branch happy let reach\n",
      "Topic #8:\n",
      "westpac help customers hi australia new helps loan hope support\n",
      "Topic #9:\n",
      "complaints information dont process feedback complaint public hesitate link including\n"
     ]
    }
   ],
   "source": [
    "feature_names_wbc = vectorizer.get_feature_names_out()\n",
    "print_top_words(lda_wbc, feature_names_wbc, n_top_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top words per topic\n",
    "top_words_per_topic = []\n",
    "for topic_idx, topic in enumerate(lda_wbc.components_):\n",
    "    top_features_ind = topic.argsort()[:-11:-1]\n",
    "    top_features = [feature_names_wbc[i] for i in top_features_ind]\n",
    "    top_words_per_topic.append(top_features)\n",
    "\n",
    "# Create a DataFrame with the top words for each topic\n",
    "df_top_words_wbc = pd.DataFrame(top_words_per_topic)\n",
    "df_top_words_transposed_wbc = df_top_words_wbc.T \n",
    "\n",
    "df_wbc_prefixed = df_top_words_transposed_wbc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Europe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BNP Paribas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.1         0.10000414  0.1        ...  0.1         0.1\n",
      "   0.10000153]\n",
      " [ 0.10019259  0.10000117  0.10000172 ...  0.1         0.10000292\n",
      "   0.10000143]\n",
      " [ 0.10013365  0.10000066  0.1        ...  0.1         0.1\n",
      "   0.10000152]\n",
      " ...\n",
      " [ 0.1        33.75441093  0.1        ...  0.1         0.1\n",
      "   0.1       ]\n",
      " [ 0.10015281  0.1         4.0999961  ...  2.09999844 11.0999843\n",
      "   6.0999574 ]\n",
      " [ 2.09935048  0.10000017  0.1        ...  0.10000156  0.10000019\n",
      "   0.10003508]]\n"
     ]
    }
   ],
   "source": [
    "tweets_bnp = pd.read_csv('/Users/houhiroshisakai/Desktop/Schulich/Term 2/MBAN 6090/BNPParibas_since_2018_01_01.csv')\n",
    "\n",
    "texts_bnp = tweets_bnp['Tweet'].tolist()\n",
    "\n",
    "clean_texts_bnp = [clean_text(text) for text in texts_bnp]\n",
    "\n",
    "# ContextVectorize\n",
    "X_bnp = vectorizer.fit_transform(clean_texts_bnp)\n",
    "\n",
    "# Apply LDA\n",
    "lda_bnp = LatentDirichletAllocation(n_components=10)\n",
    "topics_bnp = lda_bnp.fit(X_bnp)\n",
    "\n",
    "print(lda_bnp.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "clients head sustainable answer order good social day account wish\n",
      "Topic #1:\n",
      "tennis young paribas bnp players teamjeunestalents team support fftennis help\n",
      "Topic #2:\n",
      "rolandgarros support vrarlesfestival paribas bnp vr young bnppresults know new\n",
      "Topic #3:\n",
      "la et le pour les du des en qui avec\n",
      "Topic #4:\n",
      "bnppcsr greenreflex women bfmbusiness watch business paribas bnp energy data\n",
      "Topic #5:\n",
      "positivebanking climate sustainable climatechange bank fondationbnpp research bnp paribas projects\n",
      "Topic #6:\n",
      "women heforshe bnppcoalitions jblefevre diversity wfgm bnppgenderequality jbonnel ym genderequality\n",
      "Topic #7:\n",
      "vivatech positivebanking bnppadvance startups new discover tech innovation mobility day\n",
      "Topic #8:\n",
      "la pour le des les et en nous du sur\n",
      "Topic #9:\n",
      "vous nous bonjour dm contacter par afin que bonne des\n"
     ]
    }
   ],
   "source": [
    "feature_names_bnp = vectorizer.get_feature_names_out()\n",
    "print_top_words(lda_bnp, feature_names_bnp, n_top_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top words per topic\n",
    "top_words_per_topic = []\n",
    "for topic_idx, topic in enumerate(lda_bnp.components_):\n",
    "    top_features_ind = topic.argsort()[:-11:-1]\n",
    "    top_features = [feature_names_bnp[i] for i in top_features_ind]\n",
    "    top_words_per_topic.append(top_features)\n",
    "\n",
    "# Create a DataFrame with the top words for each topic\n",
    "df_top_words_bnp = pd.DataFrame(top_words_per_topic)\n",
    "df_top_words_transposed_bnp = df_top_words_bnp.T \n",
    "\n",
    "df_bnp_prefixed = df_top_words_transposed_bnp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BNP Asset Mgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.1        0.1        0.1        ... 2.09997851 0.1        0.1       ]\n",
      " [0.1        0.1        0.1        ... 0.1        0.10000053 0.1       ]\n",
      " [0.1        0.10001162 0.10000161 ... 0.1        0.10000233 0.1       ]\n",
      " ...\n",
      " [0.1        0.1        2.20238222 ... 0.10001318 0.10001432 0.1       ]\n",
      " [0.1        0.1        0.1        ... 2.09940056 0.10000419 2.1       ]\n",
      " [0.1        2.09992381 2.09999419 ... 0.10060906 8.09996894 0.1       ]]\n"
     ]
    }
   ],
   "source": [
    "tweets_bnp1 = pd.read_csv('/Users/houhiroshisakai/Desktop/Schulich/Term 2/MBAN 6090/BNPPAM_COM_since_2018_01_01.csv')\n",
    "\n",
    "texts_bnp1 = tweets_bnp1['Tweet'].tolist()\n",
    "\n",
    "clean_texts_bnp1 = [clean_text(text) for text in texts_bnp1]\n",
    "\n",
    "# ContextVectorize\n",
    "X_bnp1 = vectorizer.fit_transform(clean_texts_bnp1)\n",
    "\n",
    "# Apply LDA\n",
    "lda_bnp1 = LatentDirichletAllocation(n_components=10)\n",
    "topics_bnp1 = lda_bnp1.fit(X_bnp1)\n",
    "\n",
    "print(lda_bnp1.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "read article outlook latest market inflation markets update investment weekly\n",
      "Topic #1:\n",
      "asset investment investors management sustainable paribas bnp new food whats\n",
      "Topic #2:\n",
      "bnppam_com china greaterchina head economist chi lo chinas senior global\n",
      "Topic #3:\n",
      "sustainability bnppam_com esg global head investing investment investors key strategy\n",
      "Topic #4:\n",
      "bnppam_com awards year investment fund esg asset best manager management\n",
      "Topic #5:\n",
      "bnppam_com stand podcast discover sustainable markets market week solutions covid\n",
      "Topic #6:\n",
      "markets read income fixed financial article energy transition insights economy\n",
      "Topic #7:\n",
      "bnppam_com head tells china debt investment investors trade senior emergingmarkets\n",
      "Topic #8:\n",
      "learn sri investors asset investing discover equities people read sustainable\n",
      "Topic #9:\n",
      "bnppam_com paul apac head asia tells sandhu investors esg client\n"
     ]
    }
   ],
   "source": [
    "feature_names_bnp1 = vectorizer.get_feature_names_out()\n",
    "print_top_words(lda_bnp1, feature_names_bnp1, n_top_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top words per topic\n",
    "top_words_per_topic = []\n",
    "for topic_idx, topic in enumerate(lda_bnp1.components_):\n",
    "    top_features_ind = topic.argsort()[:-11:-1]\n",
    "    top_features = [feature_names_bnp1[i] for i in top_features_ind]\n",
    "    top_words_per_topic.append(top_features)\n",
    "\n",
    "# Create a DataFrame with the top words for each topic\n",
    "df_top_words_bnp1 = pd.DataFrame(top_words_per_topic)\n",
    "df_top_words_transposed_bnp1 = df_top_words_bnp1.T \n",
    "\n",
    "df_bnp1_prefixed = df_top_words_transposed_bnp1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crédit Agricole Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.09998483 0.1        0.1        ... 0.1        0.1        1.09998893]\n",
      " [0.1        0.1        0.1        ... 2.09996272 1.30936212 0.1       ]\n",
      " [0.1        0.10000291 0.1        ... 0.1        0.10000292 0.1       ]\n",
      " ...\n",
      " [0.1        3.09995676 2.09999632 ... 0.1        2.49839579 0.1       ]\n",
      " [0.1        0.10003214 0.1        ... 0.10002815 2.49222364 1.09992242]\n",
      " [2.09999379 0.10000634 0.10000368 ... 0.1        0.1        0.1000102 ]]\n"
     ]
    }
   ],
   "source": [
    "tweets_cag = pd.read_csv('/Users/houhiroshisakai/Desktop/Schulich/Term 2/MBAN 6090/more_banks/Credit_Agricole_since_2018_01_01.csv')\n",
    "\n",
    "texts_cag = tweets_cag['Tweet'].tolist()\n",
    "\n",
    "clean_texts_cag = [clean_text(text) for text in texts_cag]\n",
    "\n",
    "# ContextVectorize\n",
    "X_cag = vectorizer.fit_transform(clean_texts_cag)\n",
    "\n",
    "# Apply LDA\n",
    "lda_cag = LatentDirichletAllocation(n_components=10)\n",
    "topics_cag = lda_cag.fit(X_cag)\n",
    "\n",
    "print(lda_cag.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "et le en la des les pour nous du dans\n",
      "Topic #1:\n",
      "pour en nous et sia sur des le creditagricole votre\n",
      "Topic #2:\n",
      "ukraine companies financing russian agricole russia credit group hi activities\n",
      "Topic #3:\n",
      "la le dans analyse pour et du des eco qui\n",
      "Topic #4:\n",
      "et du pour les nos la le résultats aca des\n",
      "Topic #5:\n",
      "la pour des le les et en sur avec du\n",
      "Topic #6:\n",
      "vous en le ce la nous et bonjour les plus\n",
      "Topic #7:\n",
      "aca results la plasticodyssey lefebvre time dominique le financial président\n",
      "Topic #8:\n",
      "les la des et nous en une nos notre dans\n",
      "Topic #9:\n",
      "directeur crédit général agricole du sa suspended cc russia le\n"
     ]
    }
   ],
   "source": [
    "feature_names_cag = vectorizer.get_feature_names_out()\n",
    "print_top_words(lda_cag, feature_names_cag, n_top_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top words per topic\n",
    "top_words_per_topic = []\n",
    "for topic_idx, topic in enumerate(lda_cag.components_):\n",
    "    top_features_ind = topic.argsort()[:-11:-1]\n",
    "    top_features = [feature_names_cag[i] for i in top_features_ind]\n",
    "    top_words_per_topic.append(top_features)\n",
    "\n",
    "# Create a DataFrame with the top words for each topic\n",
    "df_top_words_cag = pd.DataFrame(top_words_per_topic)\n",
    "df_top_words_transposed_cag = df_top_words_cag.T \n",
    "\n",
    "df_cag_prefixed = df_top_words_transposed_cag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Barclays PLC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5.18690744  0.1         0.1000089  ... 30.37046102  0.1\n",
      "   1.15796717]\n",
      " [ 9.81361676  0.1         0.10000222 ...  0.23624142  0.10001133\n",
      "   1.10000551]\n",
      " [11.75047246  0.1         0.10000113 ... 14.96146138  0.10000306\n",
      "   0.1       ]\n",
      " ...\n",
      " [ 0.10000293  0.1         0.1        ...  0.88703341  0.1000072\n",
      "   0.1       ]\n",
      " [ 1.151914    0.1         0.1        ...  0.10000116  2.29027644\n",
      "   0.1       ]\n",
      " [ 5.50614454  0.1         0.10000121 ...  0.10001739  0.1\n",
      "   2.7919697 ]]\n"
     ]
    }
   ],
   "source": [
    "tweets_barclays = pd.read_csv('/Users/houhiroshisakai/Desktop/Schulich/Term 2/MBAN 6090/more_banks/Barclays_since_2018_01_01.csv')\n",
    "\n",
    "texts_barclays = tweets_barclays['Tweet'].tolist()\n",
    "\n",
    "clean_texts_barclays = [clean_text(text) for text in texts_barclays]\n",
    "\n",
    "# ContextVectorize\n",
    "X_barclays = vectorizer.fit_transform(clean_texts_barclays)\n",
    "\n",
    "# Apply LDA\n",
    "lda_barclays = LatentDirichletAllocation(n_components=10)\n",
    "topics_barclays = lda_barclays.fit(X_barclays)\n",
    "\n",
    "print(lda_barclays.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "youve available pm youre thanks askbarclaysus im team sorry need\n",
      "Topic #1:\n",
      "group cs venkatakrishnan results barclaysresults read executive ceo announcement chief\n",
      "Topic #2:\n",
      "barclaysukhelp thanks dm hi help tweet hey great weve reaching\n",
      "Topic #3:\n",
      "barclays info note left new day read years ive leave\n",
      "Topic #4:\n",
      "sorry youre im hi thanks help know hear hope let\n",
      "Topic #5:\n",
      "complaint log like youd fully include complaints link concerns investigate\n",
      "Topic #6:\n",
      "dm number postcode pop contact help support know ill like\n",
      "Topic #7:\n",
      "barclays banking post office local uk theyre know new app\n",
      "Topic #8:\n",
      "colleagues barclays customers clients year happy world proud work communities\n",
      "Topic #9:\n",
      "customers need rewards reach like app colleagues team youll month\n"
     ]
    }
   ],
   "source": [
    "feature_names_barclays = vectorizer.get_feature_names_out()\n",
    "print_top_words(lda_barclays, feature_names_barclays, n_top_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top words per topic\n",
    "top_words_per_topic = []\n",
    "for topic_idx, topic in enumerate(lda_barclays.components_):\n",
    "    top_features_ind = topic.argsort()[:-11:-1]\n",
    "    top_features = [feature_names_barclays[i] for i in top_features_ind]\n",
    "    top_words_per_topic.append(top_features)\n",
    "\n",
    "# Create a DataFrame with the top words for each topic\n",
    "df_top_words_barclays = pd.DataFrame(top_words_per_topic)\n",
    "df_top_words_transposed_barclays = df_top_words_barclays.T \n",
    "\n",
    "df_barclays_prefixed = df_top_words_transposed_barclays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Banco Santander SA (BSSA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.1        1.10000265 1.10000579 ... 2.09999022 3.09999393 0.10006897]\n",
      " [0.1        0.1        0.1        ... 0.1        0.1        0.1       ]\n",
      " [0.1        1.09999734 0.1        ... 0.1        0.1        1.0999961 ]\n",
      " ...\n",
      " [0.1        0.1        0.1        ... 0.1        0.1        0.1       ]\n",
      " [0.1        0.1        0.1        ... 0.1        0.1        0.1       ]\n",
      " [1.10000402 0.1        0.1        ... 0.1        0.1        0.1       ]]\n"
     ]
    }
   ],
   "source": [
    "tweets_bssa = pd.read_csv('/Users/houhiroshisakai/Desktop/Schulich/Term 2/MBAN 6090/more_banks/bancosantander_since_2018_01_01.csv')\n",
    "\n",
    "texts_bssa = tweets_bssa['Tweet'].tolist()\n",
    "\n",
    "clean_texts_bssa = [clean_text(text) for text in texts_bssa]\n",
    "\n",
    "# ContextVectorize\n",
    "X_bssa = vectorizer.fit_transform(clean_texts_bssa)\n",
    "\n",
    "# Apply LDA\n",
    "lda_bssa = LatentDirichletAllocation(n_components=10)\n",
    "topics_bssa = lda_bssa.fit(X_bssa)\n",
    "\n",
    "print(lda_bssa.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "team know work dont phishinggruposantanderes best tips happy congratulations thank\n",
      "Topic #1:\n",
      "la para santander en al las esta resultados por es\n",
      "Topic #2:\n",
      "know million bank customers digital want year new global profit\n",
      "Topic #3:\n",
      "en la el los para más del que las santander\n",
      "Topic #4:\n",
      "san madrid acción cierre bancosantander really mn investors analysts broadcast\n",
      "Topic #5:\n",
      "que la por en para te gracias el lo tu\n",
      "Topic #6:\n",
      "em santander_br te tu una twitter mucho poder banco uma\n",
      "Topic #7:\n",
      "santander digital new financial anabotin today banking international group world\n",
      "Topic #8:\n",
      "para en por que el las es gracias la los\n",
      "Topic #9:\n",
      "convocatoria nómina tu euros esta santander proyecto la ganadora universia\n"
     ]
    }
   ],
   "source": [
    "feature_names_bssa = vectorizer.get_feature_names_out()\n",
    "print_top_words(lda_bssa, feature_names_bssa, n_top_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top words per topic\n",
    "top_words_per_topic = []\n",
    "for topic_idx, topic in enumerate(lda_bssa.components_):\n",
    "    top_features_ind = topic.argsort()[:-11:-1]\n",
    "    top_features = [feature_names_bssa[i] for i in top_features_ind]\n",
    "    top_words_per_topic.append(top_features)\n",
    "\n",
    "# Create a DataFrame with the top words for each topic\n",
    "df_top_words_bssa = pd.DataFrame(top_words_per_topic)\n",
    "df_top_words_transposed_bssa = df_top_words_bssa.T \n",
    "\n",
    "df_bssa_prefixed = df_top_words_transposed_bssa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group BPCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.10001282 3.74075116 1.09995037 ... 2.54932992 0.1        0.10002715]\n",
      " [0.1        0.1        0.1        ... 0.1000042  0.10001    0.1       ]\n",
      " [0.1        0.1        0.1        ... 3.13513329 0.1        0.1       ]\n",
      " ...\n",
      " [0.1        0.1        1.10000665 ... 3.09989623 0.1        0.1       ]\n",
      " [3.09998152 2.41609647 0.1        ... 0.10006254 0.1        1.0999737 ]\n",
      " [0.1        3.06423958 0.1        ... 1.75492456 0.1        0.1       ]]\n"
     ]
    }
   ],
   "source": [
    "tweets_bpce = pd.read_csv('/Users/houhiroshisakai/Desktop/Schulich/Term 2/MBAN 6090/more_banks/GroupeBPCE_since_2018_01_01.csv')\n",
    "\n",
    "texts_bpce = tweets_bpce['Tweet'].tolist()\n",
    "\n",
    "clean_texts_bpce = [clean_text(text) for text in texts_bpce]\n",
    "\n",
    "# ContextVectorize\n",
    "X_bpce = vectorizer.fit_transform(clean_texts_bpce)\n",
    "\n",
    "# Apply LDA\n",
    "lda_bpce = LatentDirichletAllocation(n_components=10)\n",
    "topics_bpce = lda_bpce.fit(X_bpce)\n",
    "\n",
    "print(lda_bpce.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "et les la des le en pour du sur avec\n",
      "Topic #1:\n",
      "du les des groupe bpce résultats par le et sur\n",
      "Topic #2:\n",
      "du et le les pour des sport en la groupe\n",
      "Topic #3:\n",
      "la voilebanquepop et avec en le pour les populaire une\n",
      "Topic #4:\n",
      "la le et par en des pour une france est\n",
      "Topic #5:\n",
      "la et du les le pour groupe bpce des en\n",
      "Topic #6:\n",
      "le la et des en les avec bpce groupe pour\n",
      "Topic #7:\n",
      "du le groupe bpce la paris et des jeux laurent\n",
      "Topic #8:\n",
      "vous le bien bonjour cordialement je dm si nous votre\n",
      "Topic #9:\n",
      "et des du le en bpce la groupe sur les\n"
     ]
    }
   ],
   "source": [
    "feature_names_bpce = vectorizer.get_feature_names_out()\n",
    "print_top_words(lda_bpce, feature_names_bpce, n_top_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top words per topic\n",
    "top_words_per_topic = []\n",
    "for topic_idx, topic in enumerate(lda_bpce.components_):\n",
    "    top_features_ind = topic.argsort()[:-11:-1]\n",
    "    top_features = [feature_names_bpce[i] for i in top_features_ind]\n",
    "    top_words_per_topic.append(top_features)\n",
    "\n",
    "# Create a DataFrame with the top words for each topic\n",
    "df_top_words_bpce = pd.DataFrame(top_words_per_topic)\n",
    "df_top_words_transposed_bpce = df_top_words_bpce.T \n",
    "\n",
    "df_bpce_prefixed = df_top_words_transposed_bpce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.DataFrame()\n",
    "#Canada\n",
    "df_canada = pd.DataFrame([['Canada']], columns = [0])\n",
    "#Store RBC to excel file\n",
    "df_rbc_with_name = pd.DataFrame([['RBC']], columns=[0])  # Only a cell includes \"RBC\"\n",
    "df_rbc_with_topics = pd.concat([df_canada, df_rbc_with_name, df_rbc_prefixed], ignore_index=True)\n",
    "\n",
    "\n",
    "df_final = pd.concat([df_final, df_rbc_with_topics], ignore_index=True)\n",
    "#Store CIBC to excel file\n",
    "df_cibc_with_name = pd.DataFrame([['CIBC']], columns=[0]) \n",
    "df_cibc_with_topics = pd.concat([df_cibc_with_name, df_cibc_prefixed], ignore_index=True)\n",
    "df_final = pd.concat([df_final, df_cibc_with_topics], ignore_index=True)\n",
    "\n",
    "#Store Scotiabank to excel file\n",
    "df_scotia_with_name = pd.DataFrame([['ScotiaBank']], columns =[0])\n",
    "df_scotia_with_topics = pd.concat([df_scotia_with_name,df_scotia_prefixed], ignore_index= True)\n",
    "df_final = pd.concat([df_final, df_scotia_with_topics], ignore_index=True)\n",
    "\n",
    "#Store TD Canada News to excel file\n",
    "df_tdcanada_with_name = pd.DataFrame([['TD_Canada_News']], columns = [0])\n",
    "df_tdcanada_with_topics = pd.concat([df_tdcanada_with_name, df_tdcanada_prefixed], ignore_index = True)\n",
    "df_final = pd.concat([df_final, df_tdcanada_with_topics], ignore_index=True)\n",
    "\n",
    "#Store TD Canada Official Bank to excel file\n",
    "df_tdcanada1_with_name = pd.DataFrame([['TD_Canada']], columns = [0])\n",
    "df_tdcanada1_with_topics = pd.concat([df_tdcanada1_with_name, df_tdcanada1_prefixed], ignore_index = True)\n",
    "df_final = pd.concat([df_final, df_tdcanada1_with_topics], ignore_index=True)\n",
    "\n",
    "#US Area\n",
    "df_us = pd.DataFrame([['US']], columns = [0])\n",
    "#Store TD US News to excel file\n",
    "df_tdus_with_name = pd.DataFrame([['TD_US_News']], columns = [0])\n",
    "df_tdus_with_topics = pd.concat([df_us, df_tdus_with_name, df_tdus_prefixed], ignore_index = True)\n",
    "df_final = pd.concat([df_final, df_tdus_with_topics], ignore_index=True)\n",
    "\n",
    "#Store TD US Official Account to excel file\n",
    "df_tdus1_with_name = pd.DataFrame([['TD_US']], columns = [0])\n",
    "df_tdus1_with_topics = pd.concat([df_tdus1_with_name, df_tdus1_prefixed], ignore_index = True)\n",
    "df_final = pd.concat([df_final, df_tdus1_with_topics], ignore_index=True)\n",
    "\n",
    "#Store Morgan Stanley to excel file\n",
    "df_ms_with_name = pd.DataFrame([['Morgan Stanley']], columns = [0])\n",
    "df_ms_with_topics = pd.concat([df_ms_with_name, df_ms_prefixed], ignore_index = True)\n",
    "df_final = pd.concat([df_final, df_ms_with_topics], ignore_index=True)\n",
    "\n",
    "#Store UBS\n",
    "df_ubs_with_name = pd.DataFrame([['UBS']], columns = [0])\n",
    "df_ubs_with_topics = pd.concat([df_ubs_with_name, df_ubs_prefixed], ignore_index = True)\n",
    "df_final = pd.concat([df_final, df_ubs_with_topics], ignore_index=True)\n",
    "#Citi\n",
    "df_citi_with_name = pd.DataFrame([['Citi']], columns = [0])\n",
    "df_citi_with_topics = pd.concat([df_citi_with_name, df_citi_prefixed], ignore_index = True)\n",
    "df_final = pd.concat([df_final, df_citi_with_topics], ignore_index=True)\n",
    "#Wells Fargo\n",
    "df_wf_with_name = pd.DataFrame([['Wells Fargo']], columns = [0])\n",
    "df_wf_with_topics = pd.concat([df_wf_with_name, df_wf_prefixed], ignore_index = True)\n",
    "df_final = pd.concat([df_final, df_wf_with_topics], ignore_index=True)\n",
    "#BOA (Bank of America)\n",
    "df_boa_with_name = pd.DataFrame([['Bank of America']], columns = [0])\n",
    "df_boa_with_topics = pd.concat([df_boa_with_name, df_boa_prefixed], ignore_index = True)\n",
    "df_final = pd.concat([df_final, df_boa_with_topics], ignore_index=True)\n",
    "#JP Morgan\n",
    "df_jp_with_name = pd.DataFrame([['JP Morgan']], columns = [0])\n",
    "df_jp_with_topics = pd.concat([df_jp_with_name, df_jp_prefixed], ignore_index = True)\n",
    "df_final = pd.concat([df_final, df_jp_with_topics], ignore_index=True)\n",
    "#Raymond James\n",
    "df_rj_with_name = pd.DataFrame([['Raymond James']], columns = [0])\n",
    "df_rj_with_topics = pd.concat([df_rj_with_name, df_rj_prefixed], ignore_index = True)\n",
    "df_final = pd.concat([df_final, df_rj_with_topics], ignore_index=True)\n",
    "\n",
    "#Africa\n",
    "df_africa = pd.DataFrame([['Africa']], columns = [0])\n",
    "#Quant Africa\n",
    "df_qa_with_name = pd.DataFrame([['Quant Africa']], columns = [0])\n",
    "df_qa_with_topics = pd.concat([df_africa, df_qa_with_name, df_qa_prefixed], ignore_index = True)\n",
    "df_final = pd.concat([df_final, df_qa_with_topics], ignore_index=True)\n",
    "#Standard Bank\n",
    "df_sb_with_name = pd.DataFrame([['Standard Bank']], columns = [0])\n",
    "df_sb_with_topics = pd.concat([df_sb_with_name, df_sb_prefixed], ignore_index = True)\n",
    "df_final = pd.concat([df_final, df_sb_with_topics], ignore_index=True)\n",
    "#Northern Trust\n",
    "df_nt_with_name = pd.DataFrame([['Standard Bank']], columns = [0])\n",
    "df_nt_with_topics = pd.concat([df_nt_with_name, df_nt_prefixed], ignore_index = True)\n",
    "df_final = pd.concat([df_final, df_nt_with_topics], ignore_index=True)\n",
    "#UBA\n",
    "df_uba_with_name = pd.DataFrame([['UBA']], columns = [0])\n",
    "df_uba_with_topics = pd.concat([df_uba_with_name, df_uba_prefixed], ignore_index = True)\n",
    "df_final = pd.concat([df_final, df_uba_with_topics], ignore_index=True)\n",
    "\n",
    "#Asia\n",
    "df_asia = pd.DataFrame([['Asia']], columns = [0])\n",
    "#HSBC\n",
    "df_hsbc_with_name = pd.DataFrame([['HSBC']], columns = [0])\n",
    "df_hsbc_with_topics = pd.concat([df_asia, df_hsbc_with_name, df_hsbc_prefixed], ignore_index = True)\n",
    "df_final = pd.concat([df_final, df_hsbc_with_topics], ignore_index=True)\n",
    "#OCBC Premier banking\n",
    "df_ocbc_with_name = pd.DataFrame([['OCBC Premier Banking']], columns = [0])\n",
    "df_ocbc_with_topics = pd.concat([df_ocbc_with_name, df_ocbc_prefixed], ignore_index = True)\n",
    "df_final = pd.concat([df_final, df_ocbc_with_topics], ignore_index=True)\n",
    "#Bank of Singapore\n",
    "df_bos_with_name = pd.DataFrame([['Bank of Singapore']], columns = [0])\n",
    "df_bos_with_topics = pd.concat([df_bos_with_name, df_bos_prefixed], ignore_index = True)\n",
    "df_final = pd.concat([df_final, df_bos_with_topics], ignore_index=True)\n",
    "#Hana Bank\n",
    "df_hana_with_name = pd.DataFrame([['Hana Bank']], columns = [0])\n",
    "df_hana_with_topics = pd.concat([df_hana_with_name, df_hana_prefixed], ignore_index = True)\n",
    "df_final = pd.concat([df_final, df_hana_with_topics], ignore_index=True)\n",
    "\n",
    "#Oceania\n",
    "df_oceania = pd.DataFrame([['Oceania']], columns = [0])\n",
    "#ANZ\n",
    "df_anz_with_name = pd.DataFrame([['ANZ']], columns = [0])\n",
    "df_anz_with_topics = pd.concat([df_oceania, df_anz_with_name, df_anz_prefixed], ignore_index = True)\n",
    "df_final = pd.concat([df_final, df_anz_with_topics], ignore_index=True)\n",
    "#CBA\n",
    "df_cba_with_name = pd.DataFrame([['CBA']], columns = [0])\n",
    "df_cba_with_topics = pd.concat([df_cba_with_name, df_cba_prefixed], ignore_index = True)\n",
    "df_final = pd.concat([df_final, df_cba_with_topics], ignore_index=True)\n",
    "#NAB\n",
    "df_nab_with_name = pd.DataFrame([['NAB']], columns = [0])\n",
    "df_nab_with_topics = pd.concat([df_nab_with_name, df_nab_prefixed], ignore_index = True)\n",
    "df_final = pd.concat([df_final, df_nab_with_topics], ignore_index=True)\n",
    "#WBC\n",
    "df_wbc_with_name = pd.DataFrame([['WBC']], columns = [0])\n",
    "df_wbc_with_topics = pd.concat([df_wbc_with_name, df_wbc_prefixed], ignore_index = True)\n",
    "df_final = pd.concat([df_final, df_wbc_with_topics], ignore_index=True)\n",
    "\n",
    "#Europe\n",
    "df_europe = pd.DataFrame([['Europe']], columns = [0])\n",
    "#BNP Paribas\n",
    "df_bnp_with_name = pd.DataFrame([['BNP Paribas']], columns = [0])\n",
    "df_bnp_with_topics = pd.concat([df_europe, df_bnp_with_name, df_bnp_prefixed], ignore_index = True)\n",
    "df_final = pd.concat([df_final, df_bnp_with_topics], ignore_index=True)\n",
    "#Crédit Agricole Group\n",
    "df_cag_with_name = pd.DataFrame([['Crédit Agricole Group']], columns = [0])\n",
    "df_cag_with_topics = pd.concat([df_cag_with_name, df_cag_prefixed], ignore_index = True)\n",
    "df_final = pd.concat([df_final, df_cag_with_topics], ignore_index=True)\n",
    "#Barclays PLC\n",
    "df_barclays_with_name = pd.DataFrame([['Barclays PLC']], columns = [0])\n",
    "df_barclays_with_topics = pd.concat([df_barclays_with_name, df_barclays_prefixed], ignore_index = True)\n",
    "df_final = pd.concat([df_final, df_barclays_with_topics], ignore_index=True)\n",
    "#Banco Santander SA\n",
    "df_bssa_with_name = pd.DataFrame([['Banco Santander SA']], columns = [0])\n",
    "df_bssa_with_topics = pd.concat([df_bssa_with_name, df_bssa_prefixed], ignore_index = True)\n",
    "df_final = pd.concat([df_final, df_bssa_with_topics], ignore_index=True)\n",
    "#Groupe BPCE\n",
    "df_bpce_with_name = pd.DataFrame([['BNP Paribas']], columns = [0])\n",
    "df_bpce_with_topics = pd.concat([df_bpce_with_name, df_bpce_prefixed], ignore_index = True)\n",
    "df_final = pd.concat([df_final, df_bpce_with_topics], ignore_index=True)\n",
    "\n",
    "\n",
    "excel_path = '/Users/houhiroshisakai/Desktop/Schulich/Term 2/MBAN 6090/ResultIntegration.xlsx'\n",
    "df_final.to_excel(excel_path, sheet_name='Bank_Topics', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
