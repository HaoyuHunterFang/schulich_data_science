{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RBCWealth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tweets_rbc = pd.read_csv('/Users/houhiroshisakai/Desktop/Schulich/Term 2/MBAN 6090/RBCwealth_since_2018_01_01.csv')\n",
    "\n",
    "texts_rbc = tweets_rbc['Tweet'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/houhiroshisakai/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/houhiroshisakai/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "#Define cleaning\n",
    "def clean_text(text):\n",
    "    text = str(text)\n",
    "    text = text.lower()  # convert to lower case\n",
    "    text = re.sub(r'\\d+', '', text)  # remove number\n",
    "    text = re.sub(r'http\\S+', '', text)  # remove url\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # remove punctuation mark\n",
    "    tokens = word_tokenize(text)  \n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]  # remove stop words\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "clean_texts_rbc = [clean_text(text) for text in texts_rbc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.1        4.09998158 1.09999021 ... 0.10005141 2.10003738 0.1       ]\n",
      " [0.10007079 0.1        0.1        ... 0.1        0.1        0.1       ]\n",
      " [0.10001903 0.10000307 0.1        ... 0.1        0.1        0.1       ]\n",
      " ...\n",
      " [0.1        0.1        0.1        ... 0.1        2.09999821 0.1       ]\n",
      " [2.0999966  0.10002996 0.10001177 ... 0.1        0.1        0.1       ]\n",
      " [3.09996042 0.1        1.09999802 ... 0.1        1.09980963 0.1       ]]\n"
     ]
    }
   ],
   "source": [
    "# Latent Dirichlet Allocation\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# ContextVectorize\n",
    "vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "X_rbc = vectorizer.fit_transform(clean_texts_rbc)\n",
    "\n",
    "# Apply LDA\n",
    "lda_rbc = LatentDirichletAllocation(n_components=10)\n",
    "lda_rbc.fit(X_rbc)\n",
    "\n",
    "print(lda_rbc.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names_rbc = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "learn rbc art help read communities wealth people canada artists\n",
      "Topic #1:\n",
      "learn read rbc market make financial career new clients women\n",
      "Topic #2:\n",
      "learn help read financial support impact teamrbc education work wealth\n",
      "Topic #3:\n",
      "global insight learn latest investors weekly economic read markets market\n",
      "Topic #4:\n",
      "watch wealthy barber chilton david executor learn estate family young\n",
      "Topic #5:\n",
      "learn wealth women financial rbc help business impact read investing\n",
      "Topic #6:\n",
      "rbc read jersey support wealth partnership management programme proud delighted\n",
      "Topic #7:\n",
      "read learn rbc indigenous digital help new world investing difference\n",
      "Topic #8:\n",
      "learn investors financial read year market markets protect health earnings\n",
      "Topic #9:\n",
      "learn wealth plan important family read financial planning consider future\n"
     ]
    }
   ],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "\n",
    "print_top_words(lda_rbc, feature_names_rbc, n_top_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top words per topic\n",
    "top_words_per_topic = []\n",
    "for topic_idx, topic in enumerate(lda_rbc.components_):\n",
    "    top_features_ind = topic.argsort()[:-11:-1]\n",
    "    top_features = [feature_names_rbc[i] for i in top_features_ind]\n",
    "    top_words_per_topic.append(top_features)\n",
    "\n",
    "# Create a DataFrame with the top words for each topic\n",
    "df_top_words_rbc = pd.DataFrame(top_words_per_topic)\n",
    "df_top_words_transposed_rbc = df_top_words_rbc.T \n",
    "\n",
    "# Save the DataFrame to an Excel file\n",
    "#excel_path = '/Users/houhiroshisakai/Desktop/Schulich/Term 2/MBAN 6090/ResultIntegration.xlsx'\n",
    "#df_top_words_transposed_rbc.to_excel(excel_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rbc_prefixed = df_top_words_transposed_rbc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_cibc = pd.read_csv('/Users/houhiroshisakai/Desktop/Schulich/Term 2/MBAN 6090/CIBCwealth_since_2018_01_01.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_cibc = tweets_cibc['Tweet'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_texts_cibc = [clean_text(text) for text in texts_cibc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.1        0.1        0.1        ... 0.1        2.10000535 0.1       ]\n",
      " [1.10000213 0.1        0.1        ... 2.19702399 0.10000192 2.10002401]\n",
      " [0.1        2.1000002  4.73427713 ... 1.19461948 2.37168403 0.1000047 ]\n",
      " ...\n",
      " [1.09999787 0.1        2.465384   ... 0.10001347 9.82824524 0.10015446]\n",
      " [0.1        1.0999998  0.1        ... 0.1        0.10003734 0.1       ]\n",
      " [0.1        0.1        0.1        ... 1.09994808 2.1000046  0.1       ]]\n"
     ]
    }
   ],
   "source": [
    "# ContextVectorize\n",
    "X_cibc = vectorizer.fit_transform(clean_texts_cibc)\n",
    "\n",
    "# Apply LDA\n",
    "lda_cibc = LatentDirichletAllocation(n_components=10)\n",
    "topics_cibc = lda_cibc.fit(X_cibc)\n",
    "\n",
    "print(lda_cibc.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names_cibc = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "canadian canada wishing day happy loved ones investors bank investing\n",
      "Topic #1:\n",
      "markets fed icymi benjamin tal tax planning learn experts hear\n",
      "Topic #2:\n",
      "tax jamie golombek cibcs federal cibc budget new market explains\n",
      "Topic #3:\n",
      "cibc today celebrate communities day inflation canada dyk indigenous learn\n",
      "Topic #4:\n",
      "oil recession cibcs perspectives year economy inflation selling markets says\n",
      "Topic #5:\n",
      "week investment available economic weekly roundup watch discuss markets reading\n",
      "Topic #6:\n",
      "team year market visit investment family ambitions information families leadership\n",
      "Topic #7:\n",
      "wealth wood gundy cibc advisors advisor women planning experience years\n",
      "Topic #8:\n",
      "inflation news rate rates markets end higher income says durantaye\n",
      "Topic #9:\n",
      "rate et pm register interestrates bank join cibcfamilyoffice canada economy\n"
     ]
    }
   ],
   "source": [
    "print_top_words(lda_cibc, feature_names_cibc, n_top_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top words per topic\n",
    "top_words_per_topic = []\n",
    "for topic_idx, topic in enumerate(lda_rbc.components_):\n",
    "    top_features_ind = topic.argsort()[:-11:-1]\n",
    "    top_features = [feature_names_rbc[i] for i in top_features_ind]\n",
    "    top_words_per_topic.append(top_features)\n",
    "\n",
    "# Create a DataFrame with the top words for each topic\n",
    "df_top_words_cibc = pd.DataFrame(top_words_per_topic)\n",
    "df_top_words_transposed_cibc = df_top_words_cibc.T \n",
    "\n",
    "df_cibc_prefixed = df_top_words_transposed_cibc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>learn</td>\n",
       "      <td>learn</td>\n",
       "      <td>learn</td>\n",
       "      <td>global</td>\n",
       "      <td>watch</td>\n",
       "      <td>learn</td>\n",
       "      <td>rbc</td>\n",
       "      <td>read</td>\n",
       "      <td>learn</td>\n",
       "      <td>learn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rbc</td>\n",
       "      <td>read</td>\n",
       "      <td>help</td>\n",
       "      <td>insight</td>\n",
       "      <td>wealthy</td>\n",
       "      <td>wealth</td>\n",
       "      <td>read</td>\n",
       "      <td>learn</td>\n",
       "      <td>investors</td>\n",
       "      <td>wealth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>art</td>\n",
       "      <td>rbc</td>\n",
       "      <td>read</td>\n",
       "      <td>learn</td>\n",
       "      <td>barber</td>\n",
       "      <td>women</td>\n",
       "      <td>jersey</td>\n",
       "      <td>rbc</td>\n",
       "      <td>financial</td>\n",
       "      <td>plan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>help</td>\n",
       "      <td>market</td>\n",
       "      <td>financial</td>\n",
       "      <td>latest</td>\n",
       "      <td>chilton</td>\n",
       "      <td>financial</td>\n",
       "      <td>support</td>\n",
       "      <td>indigenous</td>\n",
       "      <td>read</td>\n",
       "      <td>important</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>read</td>\n",
       "      <td>make</td>\n",
       "      <td>support</td>\n",
       "      <td>investors</td>\n",
       "      <td>david</td>\n",
       "      <td>rbc</td>\n",
       "      <td>wealth</td>\n",
       "      <td>digital</td>\n",
       "      <td>year</td>\n",
       "      <td>family</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>communities</td>\n",
       "      <td>financial</td>\n",
       "      <td>impact</td>\n",
       "      <td>weekly</td>\n",
       "      <td>executor</td>\n",
       "      <td>help</td>\n",
       "      <td>partnership</td>\n",
       "      <td>help</td>\n",
       "      <td>market</td>\n",
       "      <td>read</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>wealth</td>\n",
       "      <td>career</td>\n",
       "      <td>teamrbc</td>\n",
       "      <td>economic</td>\n",
       "      <td>learn</td>\n",
       "      <td>business</td>\n",
       "      <td>management</td>\n",
       "      <td>new</td>\n",
       "      <td>markets</td>\n",
       "      <td>financial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>people</td>\n",
       "      <td>new</td>\n",
       "      <td>education</td>\n",
       "      <td>read</td>\n",
       "      <td>estate</td>\n",
       "      <td>impact</td>\n",
       "      <td>programme</td>\n",
       "      <td>world</td>\n",
       "      <td>protect</td>\n",
       "      <td>planning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>canada</td>\n",
       "      <td>clients</td>\n",
       "      <td>work</td>\n",
       "      <td>markets</td>\n",
       "      <td>family</td>\n",
       "      <td>read</td>\n",
       "      <td>proud</td>\n",
       "      <td>investing</td>\n",
       "      <td>health</td>\n",
       "      <td>consider</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>artists</td>\n",
       "      <td>women</td>\n",
       "      <td>wealth</td>\n",
       "      <td>market</td>\n",
       "      <td>young</td>\n",
       "      <td>investing</td>\n",
       "      <td>delighted</td>\n",
       "      <td>difference</td>\n",
       "      <td>earnings</td>\n",
       "      <td>future</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0          1          2          3         4          5   \n",
       "0        learn      learn      learn     global     watch      learn  \\\n",
       "1          rbc       read       help    insight   wealthy     wealth   \n",
       "2          art        rbc       read      learn    barber      women   \n",
       "3         help     market  financial     latest   chilton  financial   \n",
       "4         read       make    support  investors     david        rbc   \n",
       "5  communities  financial     impact     weekly  executor       help   \n",
       "6       wealth     career    teamrbc   economic     learn   business   \n",
       "7       people        new  education       read    estate     impact   \n",
       "8       canada    clients       work    markets    family       read   \n",
       "9      artists      women     wealth     market     young  investing   \n",
       "\n",
       "             6           7          8          9  \n",
       "0          rbc        read      learn      learn  \n",
       "1         read       learn  investors     wealth  \n",
       "2       jersey         rbc  financial       plan  \n",
       "3      support  indigenous       read  important  \n",
       "4       wealth     digital       year     family  \n",
       "5  partnership        help     market       read  \n",
       "6   management         new    markets  financial  \n",
       "7    programme       world    protect   planning  \n",
       "8        proud   investing     health   consider  \n",
       "9    delighted  difference   earnings     future  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cibc_prefixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store RBC to excel file\n",
    "df_rbc_with_name = pd.DataFrame([['RBC']], columns=[0])  # Only a cell includes \"RBC\"\n",
    "df_rbc_with_topics = pd.concat([df_rbc_with_name, df_rbc_prefixed], ignore_index=True)\n",
    "\n",
    "\n",
    "df_final = pd.concat([df_final, df_rbc_with_topics], ignore_index=True)\n",
    "#Store CIBC to excel file\n",
    "df_cibc_with_name = pd.DataFrame([['CIBC']], columns=[0])  # 只有一个单元格包含\"CIBC\"\n",
    "df_cibc_with_topics = pd.concat([df_cibc_with_name, df_cibc_prefixed], ignore_index=True)\n",
    "\n",
    "excel_path = '/Users/houhiroshisakai/Desktop/Schulich/Term 2/MBAN 6090/ResultIntegration.xlsx'\n",
    "\n",
    "df_final = pd.concat([df_final, df_cibc_with_topics], ignore_index=True)\n",
    "df_final.to_excel(excel_path, sheet_name='Bank_Topics', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_bmo = pd.read_csv('/Users/houhiroshisakai/Desktop/Schulich/Term 2/MBAN 6090/BMO_since_2018_01_01.csv')\n",
    "\n",
    "texts_bmo = tweets_bmo['Tweet'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_texts_bmo = [clean_text(text) for text in texts_bmo]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.1         0.10000756  9.69730682 ...  0.1         0.1\n",
      "   0.1       ]\n",
      " [ 0.1         0.1000031   0.1000004  ...  0.1001267  10.09999501\n",
      "   0.10012454]\n",
      " [ 0.1        16.23299101  9.08592481 ...  0.1         0.10000054\n",
      "   0.65406565]\n",
      " ...\n",
      " [ 0.1         0.10004352 14.85448769 ...  0.1000048   0.1\n",
      "   1.54575797]\n",
      " [ 0.10000418  6.8385215   2.30359299 ...  0.1         0.10000162\n",
      "   0.1       ]\n",
      " [ 0.10000576  0.10000814  0.10000853 ...  0.1         0.10000269\n",
      "   0.10003865]]\n"
     ]
    }
   ],
   "source": [
    "# ContextVectorize\n",
    "X_bmo = vectorizer.fit_transform(clean_texts_bmo)\n",
    "\n",
    "# Apply LDA\n",
    "lda_bmo = LatentDirichletAllocation(n_components=10)\n",
    "topics_bmo = lda_bmo.fit_transform(X_bmo)\n",
    "\n",
    "print(lda_bmo.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names_bmo = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "send dm sorry learn hi thank hello like help message\n",
      "Topic #1:\n",
      "bmo financial learn business bmoforwomen join pm today new women\n",
      "Topic #2:\n",
      "working apologize possible patience wait times soon banking online hi\n",
      "Topic #3:\n",
      "rl im hello send message sorry help youre ryan private\n",
      "Topic #4:\n",
      "nc know thank investsmart wethenorth let help great ns northovereverything\n",
      "Topic #5:\n",
      "hi mf phone thanks number nc reach write review forward\n",
      "Topic #6:\n",
      "thank dm hi assist help hello reaching send ds gladly\n",
      "Topic #7:\n",
      "feedback thanks app bmo appreciate message good mobile hi thank\n",
      "Topic #8:\n",
      "wait send hi times apologize dm experienced feel message free\n",
      "Topic #9:\n",
      "branch bmo nous vous visit et bank bds branches je\n"
     ]
    }
   ],
   "source": [
    "print_top_words(lda_bmo, feature_names_bmo, n_top_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top words per topic\n",
    "top_words_per_topic = []\n",
    "for topic_idx, topic in enumerate(lda_bmo.components_):\n",
    "    top_features_ind = topic.argsort()[:-11:-1]\n",
    "    top_features = [feature_names_bmo[i] for i in top_features_ind]\n",
    "    top_words_per_topic.append(top_features)\n",
    "\n",
    "# Create a DataFrame with the top words for each topic\n",
    "df_top_words_bmo = pd.DataFrame(top_words_per_topic)\n",
    "df_top_words_transposed_bmo = df_top_words_bmo.T \n",
    "\n",
    "df_bmo_prefixed = df_top_words_transposed_bmo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scotiabank "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.1         0.1         0.10001233 ...  0.10000098  0.1\n",
      "   0.1       ]\n",
      " [ 0.1         0.1        14.0999311  ...  0.1         0.1\n",
      "   0.1       ]\n",
      " [ 0.1         4.10001711  0.10001351 ...  0.1         1.09988117\n",
      "   0.1       ]\n",
      " ...\n",
      " [ 0.1         0.1         0.10000117 ...  0.1         0.1\n",
      "   0.1       ]\n",
      " [ 2.10000161  1.09998782  0.1        ...  0.1         0.1\n",
      "   1.09993987]\n",
      " [ 1.09999212  0.1         0.10000232 ...  0.10000101  0.1\n",
      "   0.1       ]]\n"
     ]
    }
   ],
   "source": [
    "tweets_scotia = pd.read_csv('/Users/houhiroshisakai/Desktop/Schulich/Term 2/MBAN 6090/scotiabank_since_2018_01_01.csv')\n",
    "\n",
    "texts_scotia = tweets_scotia['Tweet'].tolist()\n",
    "\n",
    "clean_texts_scotia = [clean_text(text) for text in texts_scotia]\n",
    "\n",
    "# ContextVectorize\n",
    "vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "X_scotia = vectorizer.fit_transform(clean_texts_scotia)\n",
    "\n",
    "# Apply LDA\n",
    "lda_scotia = LatentDirichletAllocation(n_components=10)\n",
    "topics_scotia = lda_scotia.fit(X_scotia)\n",
    "\n",
    "print(lda_scotia.components_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names_scotia = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "financial good year new canada education fraud luck tonight edmontonoilers\n",
      "Topic #1:\n",
      "hockey hockeyforall game scotiabank accessible inclusive year canada diverse help\n",
      "Topic #2:\n",
      "scotiabank scene latest episode money earn support points read credit\n",
      "Topic #3:\n",
      "la les et des le que taux pour du en\n",
      "Topic #4:\n",
      "help learn work scotiabank best canadian canadians canada new like\n",
      "Topic #5:\n",
      "holiday financial season spending canada time advice plan growth according\n",
      "Topic #6:\n",
      "credit score money canada ces fund way scores macklem canadian\n",
      "Topic #7:\n",
      "rate inflation canada bank rates economy boc hikes scotiabank canadians\n",
      "Topic #8:\n",
      "home financial scotiabank scotia savings help account save advisor fhsa\n",
      "Topic #9:\n",
      "listen follow spotify podcasts apple latest rate indigenous chief economist\n"
     ]
    }
   ],
   "source": [
    "print_top_words(lda_scotia, feature_names_scotia, n_top_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top words per topic\n",
    "top_words_per_topic = []\n",
    "for topic_idx, topic in enumerate(lda_scotia.components_):\n",
    "    top_features_ind = topic.argsort()[:-11:-1]\n",
    "    top_features = [feature_names_scotia[i] for i in top_features_ind]\n",
    "    top_words_per_topic.append(top_features)\n",
    "\n",
    "# Create a DataFrame with the top words for each topic\n",
    "df_top_words_scotia = pd.DataFrame(top_words_per_topic)\n",
    "df_top_words_transposed_scotia = df_top_words_scotia.T \n",
    "\n",
    "df_scotia_prefixed = df_top_words_transposed_scotia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD_US_News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.10000569 0.1        0.1        ... 0.89393282 0.1        0.1       ]\n",
      " [3.09996456 0.1        0.1        ... 4.99788573 2.09998919 0.10000275]\n",
      " [0.10002963 0.1        0.1        ... 1.20304618 0.1        0.10000084]\n",
      " ...\n",
      " [0.1        1.09999803 0.1        ... 6.142038   0.10001225 0.10000967]\n",
      " [0.1        0.1        0.1        ... 3.74701698 0.1        0.1       ]\n",
      " [0.1        1.10000197 1.09999885 ... 0.10000988 0.1        0.1       ]]\n"
     ]
    }
   ],
   "source": [
    "tweets_tdus = pd.read_csv('/Users/houhiroshisakai/Desktop/Schulich/Term 2/MBAN 6090/TDNews_US_since_2018_01_01.csv')\n",
    "\n",
    "texts_tdus = tweets_tdus['Tweet'].tolist()\n",
    "\n",
    "clean_texts_tdus = [clean_text(text) for text in texts_tdus]\n",
    "\n",
    "# ContextVectorize\n",
    "X_tdus = vectorizer.fit_transform(clean_texts_tdus)\n",
    "\n",
    "# Apply LDA\n",
    "lda_tdus = LatentDirichletAllocation(n_components=10)\n",
    "topics_tdus = lda_tdus.fit(X_tdus)\n",
    "\n",
    "print(lda_tdus.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "td learn new year tds check survey taking look results\n",
      "Topic #1:\n",
      "td read covid colleagues story day bank financial head diversity\n",
      "Topic #2:\n",
      "tdbank_us head wealth financial survey provides td strategist investment chief\n",
      "Topic #3:\n",
      "money tdbank_us couples talk learn tdbank_uss survey benefits business role\n",
      "Topic #4:\n",
      "td ready new year challenge learn check help grant organizations\n",
      "Topic #5:\n",
      "tips check holiday moneymattersmonday tdbank_us retail spending card save make\n",
      "Topic #6:\n",
      "tdbank_us financial survey learn read best year data cited credit\n",
      "Topic #7:\n",
      "business td learn small community help tdbank_us owners smallbusiness home\n",
      "Topic #8:\n",
      "td colleagues customers learn bank communities president check new ceo\n",
      "Topic #9:\n",
      "td read tds business learn finance experience proud businesses community\n"
     ]
    }
   ],
   "source": [
    "feature_names_tdus = vectorizer.get_feature_names_out()\n",
    "\n",
    "print_top_words(lda_tdus, feature_names_tdus, n_top_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top words per topic\n",
    "top_words_per_topic = []\n",
    "for topic_idx, topic in enumerate(lda_tdus.components_):\n",
    "    top_features_ind = topic.argsort()[:-11:-1]\n",
    "    top_features = [feature_names_tdus[i] for i in top_features_ind]\n",
    "    top_words_per_topic.append(top_features)\n",
    "\n",
    "# Create a DataFrame with the top words for each topic\n",
    "df_top_words_tdus = pd.DataFrame(top_words_per_topic)\n",
    "df_top_words_transposed_tdus = df_top_words_tdus.T \n",
    "\n",
    "df_tdus_prefixed = df_top_words_transposed_tdus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_tdus1 = pd.read_csv('/Users/houhiroshisakai/Desktop/Schulich/Term 2/MBAN 6090/TDBank_US_since_2018_01_01.csv')\n",
    "\n",
    "texts_tdus1 = tweets_tdus1['Tweet'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_texts_tdus1 = [clean_text(text) for text in texts_tdus1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.00636298e-01 1.01299926e-01 1.00004868e-01 ... 1.00067875e-01\n",
      "  1.00000000e-01 1.00000000e-01]\n",
      " [1.00000001e-01 2.09818497e+00 5.27647957e+02 ... 1.22338687e+00\n",
      "  1.00000000e-01 1.00000000e-01]\n",
      " [1.00000001e-01 1.00000002e-01 1.10102802e+02 ... 1.00000000e-01\n",
      "  1.00026369e-01 1.00026369e-01]\n",
      " ...\n",
      " [1.00000001e-01 1.00000002e-01 2.69477796e+02 ... 1.00000001e-01\n",
      "  1.00000000e-01 1.00000000e-01]\n",
      " [1.00000001e-01 1.00000003e-01 1.00008747e-01 ... 1.97648524e+00\n",
      "  1.00000000e-01 1.00000000e-01]\n",
      " [2.09936369e+00 1.00451132e-01 9.57102180e+01 ... 1.00000001e-01\n",
      "  2.09989523e+00 2.09989523e+00]]\n"
     ]
    }
   ],
   "source": [
    "# ContextVectorize\n",
    "X_tdus1 = vectorizer.fit_transform(clean_texts_tdus1)\n",
    "\n",
    "# Apply LDA\n",
    "lda_tdus1 = LatentDirichletAllocation(n_components=10)\n",
    "topics_tdus1 = lda_tdus1.fit_transform(X_tdus1)\n",
    "\n",
    "print(lda_tdus1.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "like dm hey account numbers wed learn chat send details\n",
      "Topic #1:\n",
      "dm send account feel like saw numbers tweet learn details\n",
      "Topic #2:\n",
      "dm account send numbers details ask like kindly good thank\n",
      "Topic #3:\n",
      "apologize working experiencing time patience online hold issues inconvenience issue\n",
      "Topic #4:\n",
      "card td feedback store know service customer debit business bank\n",
      "Topic #5:\n",
      "td bank information account hope visit helps contact reach online\n",
      "Topic #6:\n",
      "dm account send numbers happy help hear feel understand hey\n",
      "Topic #7:\n",
      "dm like noaccts details send wed learn additional feel concerns\n",
      "Topic #8:\n",
      "dm accts hi sorry lw help plz hear assistance make\n",
      "Topic #9:\n",
      "hope great day thanks happy glad thank welcome hear weekend\n"
     ]
    }
   ],
   "source": [
    "feature_names_tdus1 = vectorizer.get_feature_names_out()\n",
    "\n",
    "print_top_words(lda_tdus1, feature_names_tdus1, n_top_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top words per topic\n",
    "top_words_per_topic = []\n",
    "for topic_idx, topic in enumerate(lda_tdus1.components_):\n",
    "    top_features_ind = topic.argsort()[:-11:-1]\n",
    "    top_features = [feature_names_bmo[i] for i in top_features_ind]\n",
    "    top_words_per_topic.append(top_features)\n",
    "\n",
    "# Create a DataFrame with the top words for each topic\n",
    "df_top_words_tdus1 = pd.DataFrame(top_words_per_topic)\n",
    "df_top_words_transposed_tdus1 = df_top_words_tdus1.T \n",
    "\n",
    "df_tdus1_prefixed = df_top_words_transposed_tdus1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD_Canada_New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_tdcanada = pd.read_csv('/Users/houhiroshisakai/Desktop/Schulich/Term 2/MBAN 6090/TDNews_Canada_since_2018_01_01.csv')\n",
    "\n",
    "texts_tdcanada = tweets_tdcanada['Tweet'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_texts_tdcanada = [clean_text(text) for text in texts_tdcanada]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.1        0.1        0.1        ... 0.1        3.10001432 0.1       ]\n",
      " [0.1        3.1        0.1        ... 2.1000004  5.10006016 0.1       ]\n",
      " [0.100001   0.1        0.10000202 ... 0.10001121 1.09997297 0.1       ]\n",
      " ...\n",
      " [0.1        0.1        0.1        ... 0.1        0.1        1.10000095]\n",
      " [0.1        0.1        0.1        ... 0.1        1.09990886 1.09999726]\n",
      " [0.10000101 0.1        1.09992148 ... 0.1        0.1        1.10000179]]\n"
     ]
    }
   ],
   "source": [
    "# ContextVectorize\n",
    "X_tdcanada = vectorizer.fit_transform(clean_texts_tdcanada)\n",
    "\n",
    "# Apply LDA\n",
    "lda_tdcanada = LatentDirichletAllocation(n_components=10)\n",
    "topics = lda_tdcanada.fit_transform(X_tdcanada)\n",
    "\n",
    "print(lda_tdcanada.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "help td canadians canada investing retirement tax new consider savings\n",
      "Topic #1:\n",
      "help tips fraud learn canadians canadian financial protect insurance bank\n",
      "Topic #2:\n",
      "td new support help colleagues customers communities lgbtq heres proud\n",
      "Topic #3:\n",
      "brucecooper_td tdassetmanagement weeks podcast marketperspectives mortgage td economy economic explains\n",
      "Topic #4:\n",
      "financial td tds canada learn help home indigenous things canadians\n",
      "Topic #5:\n",
      "td community heres help learn canadians scam money make protect\n",
      "Topic #6:\n",
      "financial tds planning help bank shares holiday td manager helping\n",
      "Topic #7:\n",
      "digital td customers experiences banking customer experience officer intelligence weve\n",
      "Topic #8:\n",
      "td ceo bharat masrani tds learn group read canadian black\n",
      "Topic #9:\n",
      "financial td canadians covid canadian new money youre tips finances\n"
     ]
    }
   ],
   "source": [
    "feature_names_tdcanada = vectorizer.get_feature_names_out()\n",
    "\n",
    "print_top_words(lda_tdcanada, feature_names_tdcanada, n_top_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top words per topic\n",
    "top_words_per_topic = []\n",
    "for topic_idx, topic in enumerate(lda_tdcanada.components_):\n",
    "    top_features_ind = topic.argsort()[:-11:-1]\n",
    "    top_features = [feature_names_tdcanada[i] for i in top_features_ind]\n",
    "    top_words_per_topic.append(top_features)\n",
    "\n",
    "# Create a DataFrame with the top words for each topic\n",
    "df_top_words_tdcanada = pd.DataFrame(top_words_per_topic)\n",
    "df_top_words_transposed_tdcanada = df_top_words_tdcanada.T \n",
    "\n",
    "df_tdcanada_prefixed = df_top_words_transposed_tdcanada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD Canada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.10001113 1.09999602 0.1        ... 0.1        0.1        0.1       ]\n",
      " [0.1        0.1        0.1        ... 1.09999496 0.1        3.09999845]\n",
      " [0.1        0.1        3.1        ... 0.1        0.1        0.1       ]\n",
      " ...\n",
      " [2.09998529 0.1        0.1        ... 0.1        7.94231811 0.1       ]\n",
      " [0.1        0.1        0.1        ... 3.10000504 0.10000579 0.1       ]\n",
      " [0.10000358 0.1        0.1        ... 0.1        0.1        0.1       ]]\n"
     ]
    }
   ],
   "source": [
    "tweets_tdcanada1 = pd.read_csv('/Users/houhiroshisakai/Desktop/Schulich/Term 2/MBAN 6090/TDNews_Canada_since_2018_01_01.csv')\n",
    "\n",
    "texts_tdcanada1 = tweets_tdcanada1['Tweet'].tolist()\n",
    "\n",
    "clean_texts_tdcanada1 = [clean_text(text) for text in texts_tdcanada]\n",
    "\n",
    "# ContextVectorize\n",
    "X_tdcanada1 = vectorizer.fit_transform(clean_texts_tdcanada1)\n",
    "\n",
    "# Apply LDA\n",
    "lda_tdcanada1 = LatentDirichletAllocation(n_components=10)\n",
    "topics_tdcanada1 = lda_tdcanada1.fit_transform(X_tdcanada1)\n",
    "\n",
    "print(lda_tdcanada1.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "td shares financial inclusive innovation help covid work meet colleagues\n",
      "Topic #1:\n",
      "td news tds month new dont community weve got worry\n",
      "Topic #2:\n",
      "brucecooper_td tdassetmanagement weeks podcast marketperspectives protect help canadians fraud learn\n",
      "Topic #3:\n",
      "td investing experience tips energy tds season canada direct holiday\n",
      "Topic #4:\n",
      "td financial help ai new bank canadian canadians app customers\n",
      "Topic #5:\n",
      "td help work canadian tds banking learn thereadycommitment future community\n",
      "Topic #6:\n",
      "td bharat ceo masrani canada tds financial list group economic\n",
      "Topic #7:\n",
      "td help financial canadians new retirement planning ways mortgage budget\n",
      "Topic #8:\n",
      "digital td money financial mortgage customers tds best bank learn\n",
      "Topic #9:\n",
      "td customers canadian financial support read indigenous banking help heres\n"
     ]
    }
   ],
   "source": [
    "feature_names_tdcanada1 = vectorizer.get_feature_names_out()\n",
    "\n",
    "print_top_words(lda_tdcanada1, feature_names_tdcanada1, n_top_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top words per topic\n",
    "top_words_per_topic = []\n",
    "for topic_idx, topic in enumerate(lda_tdcanada1.components_):\n",
    "    top_features_ind = topic.argsort()[:-11:-1]\n",
    "    top_features = [feature_names_tdcanada1[i] for i in top_features_ind]\n",
    "    top_words_per_topic.append(top_features)\n",
    "\n",
    "# Create a DataFrame with the top words for each topic\n",
    "df_top_words_tdcanada1 = pd.DataFrame(top_words_per_topic)\n",
    "df_top_words_transposed_tdcanada1 = df_top_words_tdcanada1.T \n",
    "\n",
    "df_tdcanada1_prefixed = df_top_words_transposed_tdcanada1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# US Company"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Morgan Stanley"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.10028971 0.1        1.09999759 ... 0.1        0.1        0.1       ]\n",
      " [0.10002185 0.10000609 0.1        ... 0.1        0.1        0.10001165]\n",
      " [2.09923498 0.1        0.1        ... 0.1        0.1        0.1       ]\n",
      " ...\n",
      " [0.10003105 0.10001429 0.1        ... 0.1        0.10000667 0.1       ]\n",
      " [0.1        0.1        0.1        ... 0.1        0.10000108 0.1       ]\n",
      " [0.10008442 0.1        0.1        ... 0.1        0.1        0.10000341]]\n"
     ]
    }
   ],
   "source": [
    "tweets_ms = pd.read_csv('/Users/houhiroshisakai/Desktop/Schulich/Term 2/MBAN 6090/MorganStanley_since_2018_01_01.csv')\n",
    "\n",
    "texts_ms = tweets_ms['Tweet'].tolist()\n",
    "\n",
    "clean_texts_ms = [clean_text(text) for text in texts_ms]\n",
    "\n",
    "# ContextVectorize\n",
    "X_ms = vectorizer.fit_transform(clean_texts_ms)\n",
    "\n",
    "# Apply LDA\n",
    "lda_ms = LatentDirichletAllocation(n_components=10)\n",
    "topics_ms = lda_ms.fit(X_ms)\n",
    "\n",
    "print(lda_ms.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "morgan msgivesback stanley employees program work london support learn summer\n",
      "Topic #1:\n",
      "sustainable investing learn help make financial best challenge change says\n",
      "Topic #2:\n",
      "investors investment markets growth market chief officer morgan wealth management\n",
      "Topic #3:\n",
      "sichallenge ceo team fund morgan stanley james opportunity access global\n",
      "Topic #4:\n",
      "morgan stanley billion net year ms management earnings revenues director\n",
      "Topic #5:\n",
      "multicultural innovation morgan learn stanley lab sustainability women entrepreneurs mcil\n",
      "Topic #6:\n",
      "policy eagleup public eagles year booktrustusa market michael plan zezas\n",
      "Topic #7:\n",
      "morgan stanley learn head financial help make diversity global stanleys\n",
      "Topic #8:\n",
      "global chief market markets equity strategist discusses team investors andrew\n",
      "Topic #9:\n",
      "morgan stanley new health learn mental investors investment global stanleys\n"
     ]
    }
   ],
   "source": [
    "feature_names_ms = vectorizer.get_feature_names_out()\n",
    "print_top_words(lda_ms, feature_names_ms, n_top_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top words per topic\n",
    "top_words_per_topic = []\n",
    "for topic_idx, topic in enumerate(lda_ms.components_):\n",
    "    top_features_ind = topic.argsort()[:-11:-1]\n",
    "    top_features = [feature_names_ms[i] for i in top_features_ind]\n",
    "    top_words_per_topic.append(top_features)\n",
    "\n",
    "# Create a DataFrame with the top words for each topic\n",
    "df_top_words_ms = pd.DataFrame(top_words_per_topic)\n",
    "df_top_words_transposed_ms = df_top_words_ms.T \n",
    "\n",
    "df_ms_prefixed = df_top_words_transposed_ms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UBS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.1        0.1        0.1        ... 0.1        0.1        7.99126146]\n",
      " [0.1        0.1        0.1        ... 0.1        1.09999024 0.10000456]\n",
      " [0.1        0.1        0.1        ... 0.10000935 0.1        0.1000147 ]\n",
      " ...\n",
      " [0.1000013  0.1        0.1        ... 0.1        0.10000963 5.44433962]\n",
      " [0.1        0.1        0.1        ... 2.09998981 1.10000014 1.21417386]\n",
      " [0.1        0.1        0.1000278  ... 0.1        0.1        5.13154582]]\n"
     ]
    }
   ],
   "source": [
    "tweets_ubs = pd.read_csv('/Users/houhiroshisakai/Desktop/Schulich/Term 2/MBAN 6090/UBS_since_2018_01_01.csv')\n",
    "\n",
    "texts_ubs = tweets_ubs['Tweet'].tolist()\n",
    "\n",
    "clean_texts_ubs = [clean_text(text) for text in texts_ubs]\n",
    "\n",
    "# ContextVectorize\n",
    "X_ubs = vectorizer.fit_transform(clean_texts_ubs)\n",
    "\n",
    "# Apply LDA\n",
    "lda_ubs = LatentDirichletAllocation(n_components=10)\n",
    "topics_ubs = lda_ubs.fit_transform(X_ubs)\n",
    "\n",
    "print(lda_ubs.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "shareubs best read private bank proud awards industry years ubsresearch\n",
      "Topic #1:\n",
      "shareubs investors ubsresearch survey research investor inflation ubsinvestorsentiment insights team\n",
      "Topic #2:\n",
      "results ubs ceo quarter live shareubs ermotti sergio starts group\n",
      "Topic #3:\n",
      "shareubs ubsresearch report ai latest business companies world billionaires data\n",
      "Topic #4:\n",
      "shareubs ubs future finance learn challenge foundation ubsinnovate ubsresearch ubss\n",
      "Topic #5:\n",
      "shareubs ubs global investment ubsresearch china market investors head ubsconf\n",
      "Topic #6:\n",
      "shareubs paul economist chief women ubs donovan report ownyourworth financial\n",
      "Topic #7:\n",
      "shareubs togetherband ubs thetogetherband conversation support iwd join sdg learn\n",
      "Topic #8:\n",
      "shareubs report family investors learn nobel nobelperspectives global new office\n",
      "Topic #9:\n",
      "shareubs impact sustainable future help make wef sustainability learn togetherband\n"
     ]
    }
   ],
   "source": [
    "feature_names_ubs = vectorizer.get_feature_names_out()\n",
    "print_top_words(lda_ubs, feature_names_ubs, n_top_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top words per topic\n",
    "top_words_per_topic = []\n",
    "for topic_idx, topic in enumerate(lda_ubs.components_):\n",
    "    top_features_ind = topic.argsort()[:-11:-1]\n",
    "    top_features = [feature_names_ubs[i] for i in top_features_ind]\n",
    "    top_words_per_topic.append(top_features)\n",
    "\n",
    "# Create a DataFrame with the top words for each topic\n",
    "df_top_words_ubs = pd.DataFrame(top_words_per_topic)\n",
    "df_top_words_transposed_ubs = df_top_words_ubs.T \n",
    "\n",
    "df_ubs_prefixed = df_top_words_transposed_ubs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Citi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.09999846 0.10003112 3.10003045 ... 0.10001294 0.1        0.1       ]\n",
      " [0.1        2.10000982 0.10000595 ... 0.1        0.10000921 0.1       ]\n",
      " [0.1        0.1        0.1        ... 0.1        0.10001859 0.10001769]\n",
      " ...\n",
      " [0.1        0.1        0.10018225 ... 0.1        0.10003095 0.1       ]\n",
      " [0.1        0.1        0.10000197 ... 3.09997332 0.10000787 0.1       ]\n",
      " [0.10000154 0.10001087 0.1        ... 0.1        2.09993338 0.1       ]]\n"
     ]
    }
   ],
   "source": [
    "tweets_citi = pd.read_csv('/Users/houhiroshisakai/Desktop/Schulich/Term 2/MBAN 6090/Citi_since_2018_01_01.csv')\n",
    "\n",
    "texts_citi = tweets_citi['Tweet'].tolist()\n",
    "\n",
    "clean_texts_citi = [clean_text(text) for text in texts_citi]\n",
    "\n",
    "# ContextVectorize\n",
    "X_citi = vectorizer.fit_transform(clean_texts_citi)\n",
    "\n",
    "# Apply LDA\n",
    "lda_citi = LatentDirichletAllocation(n_components=10)\n",
    "topics_citi = lda_citi.fit(X_citi)\n",
    "\n",
    "print(lda_citi.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "citi proud standforprogress support learn work diversity colleagues continue community\n",
      "Topic #1:\n",
      "citi community learn veterans new today day support committed families\n",
      "Topic #2:\n",
      "citi report read gps latest new global sustainability digital chief\n",
      "Topic #3:\n",
      "best teamciti bank proud named services people awards disabilities private\n",
      "Topic #4:\n",
      "ceo jane citi fraser corbat mike global information citis financial\n",
      "Topic #5:\n",
      "citi digital citis future banking innovation global money citidigimoney home\n",
      "Topic #6:\n",
      "experience citis new clients watch citi solutions digital global treasury\n",
      "Topic #7:\n",
      "citi learn global payments treasury clients new citis digital companies\n",
      "Topic #8:\n",
      "pathwaysprogress citi youth young foundation youthcolab entrepreneurs learn looking people\n",
      "Topic #9:\n",
      "citi support help learn community global foundation communities women proud\n"
     ]
    }
   ],
   "source": [
    "feature_names_citi = vectorizer.get_feature_names_out()\n",
    "\n",
    "print_top_words(lda_citi, feature_names_citi, n_top_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top words per topic\n",
    "top_words_per_topic = []\n",
    "for topic_idx, topic in enumerate(lda_citi.components_):\n",
    "    top_features_ind = topic.argsort()[:-11:-1]\n",
    "    top_features = [feature_names_citi[i] for i in top_features_ind]\n",
    "    top_words_per_topic.append(top_features)\n",
    "\n",
    "# Create a DataFrame with the top words for each topic\n",
    "df_top_words_citi = pd.DataFrame(top_words_per_topic)\n",
    "df_top_words_transposed_citi = df_top_words_citi.T \n",
    "\n",
    "df_citi_prefixed = df_top_words_transposed_citi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wells Fargo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.1         0.1         0.1000003  ...  0.10001227  0.1\n",
      "   0.10001452]\n",
      " [ 0.10000304  0.1         0.1        ...  0.10000049  0.1\n",
      "   0.10000279]\n",
      " [ 0.10003722  0.1         0.10000962 ...  0.10000369  0.1\n",
      "   0.1000005 ]\n",
      " ...\n",
      " [ 7.09981284  2.099915    6.09996    ...  0.10001761  0.10000974\n",
      "   8.38210148]\n",
      " [ 0.10001258  0.1         0.10000028 ...  0.10001093  0.10000223\n",
      "  25.51461551]\n",
      " [ 0.10012862  0.10007821  0.10002561 ...  0.10000136  6.09997831\n",
      "   0.10543643]]\n"
     ]
    }
   ],
   "source": [
    "tweets_wf = pd.read_csv('/Users/houhiroshisakai/Desktop/Schulich/Term 2/MBAN 6090/WellsFargo_since_2018_01_01.csv')\n",
    "\n",
    "texts_wf = tweets_wf['Tweet'].tolist()\n",
    "\n",
    "clean_texts_wf = [clean_text(text) for text in texts_wf]\n",
    "\n",
    "# ContextVectorize\n",
    "X_wf = vectorizer.fit_transform(clean_texts_wf)\n",
    "\n",
    "# Apply LDA\n",
    "lda_wf = LatentDirichletAllocation(n_components=10)\n",
    "topics_wf = lda_wf.fit(X_wf)\n",
    "\n",
    "print(lda_wf.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "account numbers help tell need tweet know specific like happened\n",
      "Topic #1:\n",
      "account make sure dm like numbers issues mind experience address\n",
      "Topic #2:\n",
      "working issue support happy technical gift apologize ddg dm wed\n",
      "Topic #3:\n",
      "speak banker assistance hi contact need visit information card thanks\n",
      "Topic #4:\n",
      "account dm details numbers help sorry best hello security thank\n",
      "Topic #5:\n",
      "wells fargo experience information address sure make provide account sorry\n",
      "Topic #6:\n",
      "online youre banking access able hi issues dm try inconvenience\n",
      "Topic #7:\n",
      "email learn visit thank hi receive information message customers forward\n",
      "Topic #8:\n",
      "dm number account phone send numbers like details address experience\n",
      "Topic #9:\n",
      "thank support thanks glad reach great chris time opportunity help\n"
     ]
    }
   ],
   "source": [
    "feature_names_wf = vectorizer.get_feature_names_out()\n",
    "print_top_words(lda_wf, feature_names_wf, n_top_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top words per topic\n",
    "top_words_per_topic = []\n",
    "for topic_idx, topic in enumerate(lda_wf.components_):\n",
    "    top_features_ind = topic.argsort()[:-11:-1]\n",
    "    top_features = [feature_names_wf[i] for i in top_features_ind]\n",
    "    top_words_per_topic.append(top_features)\n",
    "\n",
    "# Create a DataFrame with the top words for each topic\n",
    "df_top_words_wf = pd.DataFrame(top_words_per_topic)\n",
    "df_top_words_transposed_wf = df_top_words_wf.T \n",
    "\n",
    "df_wf_prefixed = df_top_words_transposed_wf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BOA (Bank of America)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.1        0.1        2.80826351 ... 0.1        1.12077359 0.1       ]\n",
      " [0.1        0.1        0.1        ... 0.1        0.1        0.10010396]\n",
      " [1.09998774 0.1        0.1        ... 0.10000065 0.1        0.1       ]\n",
      " ...\n",
      " [0.1        0.10000318 0.10000131 ... 0.10000033 2.07923594 0.1       ]\n",
      " [5.10001226 0.1        0.1        ... 0.1        0.1        0.1       ]\n",
      " [0.1        1.09997011 1.39172122 ... 0.1        0.1        0.1       ]]\n"
     ]
    }
   ],
   "source": [
    "tweets_boa = pd.read_csv('/Users/houhiroshisakai/Desktop/Schulich/Term 2/MBAN 6090/BankofAmerica_since_2018_01_01.csv')\n",
    "\n",
    "texts_boa = tweets_boa['Tweet'].tolist()\n",
    "\n",
    "clean_texts_boa = [clean_text(text) for text in texts_boa]\n",
    "\n",
    "# ContextVectorize\n",
    "X_boa = vectorizer.fit_transform(clean_texts_boa)\n",
    "\n",
    "# Apply LDA\n",
    "lda_boa = LatentDirichletAllocation(n_components=10)\n",
    "topics_boa = lda_boa.fit(X_boa)\n",
    "\n",
    "print(lda_boa.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "tools digital cantstopbanking fans bank america impressive learn weekend love\n",
      "Topic #1:\n",
      "bettermoneyhabits help financial life tips youre plan new spending money\n",
      "Topic #2:\n",
      "help thats weve thanks important young provide supporting women conversation\n",
      "Topic #3:\n",
      "business help like link know opportunity let small economic connect\n",
      "Topic #4:\n",
      "great time bofavolunteers like make congrats bofastudentleaders pay bofapride payments\n",
      "Topic #5:\n",
      "banking app mobile help link account connect send use hi\n",
      "Topic #6:\n",
      "thank communities work appreciate shout support rewards proud cash help\n",
      "Topic #7:\n",
      "women program thanks change positive creating partnership leaders conversation bitlyjevmow\n",
      "Topic #8:\n",
      "hear glad sharing thanks erica financial business student help virtual\n",
      "Topic #9:\n",
      "support happy help learn communities day year season proud need\n"
     ]
    }
   ],
   "source": [
    "feature_names_boa = vectorizer.get_feature_names_out()\n",
    "print_top_words(lda_boa, feature_names_boa, n_top_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top words per topic\n",
    "top_words_per_topic = []\n",
    "for topic_idx, topic in enumerate(lda_boa.components_):\n",
    "    top_features_ind = topic.argsort()[:-11:-1]\n",
    "    top_features = [feature_names_boa[i] for i in top_features_ind]\n",
    "    top_words_per_topic.append(top_features)\n",
    "\n",
    "# Create a DataFrame with the top words for each topic\n",
    "df_top_words_boa = pd.DataFrame(top_words_per_topic)\n",
    "df_top_words_transposed_boa = df_top_words_boa.T \n",
    "\n",
    "df_boa_prefixed = df_top_words_transposed_boa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JP Morgan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.10001151 0.1        0.1        ... 0.1        1.09999945 0.1       ]\n",
      " [2.09997044 0.10004384 1.09990422 ... 0.10000647 0.1        0.1       ]\n",
      " [0.1        0.1        1.61737598 ... 0.1        0.1        0.10000845]\n",
      " ...\n",
      " [0.1        0.1        0.1        ... 0.1        0.1        0.1       ]\n",
      " [0.1        2.09995616 0.10005822 ... 0.1        0.1        0.1       ]\n",
      " [0.1        0.1        2.10000186 ... 0.1        0.1        2.09999155]]\n"
     ]
    }
   ],
   "source": [
    "tweets_jp = pd.read_csv('/Users/houhiroshisakai/Desktop/Schulich/Term 2/MBAN 6090/jpmorgan_since_2018_01_01.csv')\n",
    "\n",
    "texts_jp = tweets_jp['Tweet'].tolist()\n",
    "\n",
    "clean_texts_jp = [clean_text(text) for text in texts_jp]\n",
    "\n",
    "# ContextVectorize\n",
    "X_jp = vectorizer.fit_transform(clean_texts_jp)\n",
    "\n",
    "# Apply LDA\n",
    "lda_jp = LatentDirichletAllocation(n_components=10)\n",
    "topics_jp = lda_jp.fit_transform(X_jp)\n",
    "\n",
    "print(lda_jp.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "global business payments market beer lori tech future jp morgans\n",
      "Topic #1:\n",
      "women business leadership leaders day career jpm advice share togetherwe\n",
      "Topic #2:\n",
      "payments business digital jpm erdoes management businesses years mary art\n",
      "Topic #3:\n",
      "ddftennis jpmcc day tennis clients jp dubai participants morgan congrats\n",
      "Topic #4:\n",
      "leaders tech jpm help program students industry business new support\n",
      "Topic #5:\n",
      "income jpm net eps usopen team reports curators global womens\n",
      "Topic #6:\n",
      "jp morgan new list summer research reading technology year ai\n",
      "Topic #7:\n",
      "jpmhc companies market jp tech capital healthcare industry trends investors\n",
      "Topic #8:\n",
      "jp global morgans markets outlook ceo economic head morgan market\n",
      "Topic #9:\n",
      "company investment asj false forex ai undersummit technology learning machine\n"
     ]
    }
   ],
   "source": [
    "feature_names_jp = vectorizer.get_feature_names_out()\n",
    "print_top_words(lda_jp, feature_names_jp, n_top_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top words per topic\n",
    "top_words_per_topic = []\n",
    "for topic_idx, topic in enumerate(lda_jp.components_):\n",
    "    top_features_ind = topic.argsort()[:-11:-1]\n",
    "    top_features = [feature_names_jp[i] for i in top_features_ind]\n",
    "    top_words_per_topic.append(top_features)\n",
    "\n",
    "# Create a DataFrame with the top words for each topic\n",
    "df_top_words_jp = pd.DataFrame(top_words_per_topic)\n",
    "df_top_words_transposed_jp = df_top_words_jp.T \n",
    "\n",
    "df_jp_prefixed = df_top_words_transposed_jp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raymond James"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.1         0.10000045  0.1        ...  0.10003177  7.09998969\n",
      "   0.1       ]\n",
      " [ 2.09987612  0.1         0.1        ...  0.1         0.1000071\n",
      "   0.1       ]\n",
      " [ 0.10012063 15.09999655 12.09999008 ...  6.0999386   0.1\n",
      "   0.10001844]\n",
      " ...\n",
      " [ 0.1         0.10000123  0.10000992 ...  0.10001123  0.10000026\n",
      "   0.1       ]\n",
      " [ 0.1         0.1         0.1        ...  0.1         0.10000216\n",
      "   0.1       ]\n",
      " [ 0.1         0.10000163  0.1        ...  0.1         0.1\n",
      "   0.1       ]]\n"
     ]
    }
   ],
   "source": [
    "tweets_rj = pd.read_csv('/Users/houhiroshisakai/Desktop/Schulich/Term 2/MBAN 6090/RaymondJames_since_2018_01_01.csv')\n",
    "\n",
    "texts_rj = tweets_rj['Tweet'].tolist()\n",
    "\n",
    "clean_texts_rj = [clean_text(text) for text in texts_rj]\n",
    "\n",
    "# ContextVectorize\n",
    "X_rj = vectorizer.fit_transform(clean_texts_rj)\n",
    "\n",
    "# Apply LDA\n",
    "lda_rj = LatentDirichletAllocation(n_components=10)\n",
    "topics_rj = lda_rj.fit_transform(X_rj)\n",
    "\n",
    "print(lda_rj.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "learn ones loved make protect time started years help heres\n",
      "Topic #1:\n",
      "time consider start tips new jobsreport tax help highlights home\n",
      "Topic #2:\n",
      "discuss tune et policy pm analyst mills ed change subject\n",
      "Topic #3:\n",
      "rjcares associates learn james advisors raymond financial month support communities\n",
      "Topic #4:\n",
      "financial cio larryadamrj plan future year investors markets consider new\n",
      "Topic #5:\n",
      "chief scott brown economist record raymond james says year make\n",
      "Topic #6:\n",
      "raymond james today happy retirement legacy life community learn help\n",
      "Topic #7:\n",
      "know heres dont market theres lower mean volatility doesnt like\n",
      "Topic #8:\n",
      "james raymond financial plan help learn planning rjf nyse data\n",
      "Topic #9:\n",
      "markets heres economy experts look whats investment market expect investors\n"
     ]
    }
   ],
   "source": [
    "feature_names_rj = vectorizer.get_feature_names_out()\n",
    "print_top_words(lda_rj, feature_names_rj, n_top_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top words per topic\n",
    "top_words_per_topic = []\n",
    "for topic_idx, topic in enumerate(lda_rj.components_):\n",
    "    top_features_ind = topic.argsort()[:-11:-1]\n",
    "    top_features = [feature_names_rj[i] for i in top_features_ind]\n",
    "    top_words_per_topic.append(top_features)\n",
    "\n",
    "# Create a DataFrame with the top words for each topic\n",
    "df_top_words_rj = pd.DataFrame(top_words_per_topic)\n",
    "df_top_words_transposed_rj = df_top_words_rj.T \n",
    "\n",
    "df_rj_prefixed = df_top_words_transposed_rj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Africa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quant Africa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.1        0.1        1.56835098 ... 2.07757643 0.1        1.09999342]\n",
      " [0.1        0.1        1.10000144 ... 0.12242357 5.09992224 0.1       ]\n",
      " [0.1        0.1        0.1        ... 0.1        1.09998457 1.10000086]\n",
      " ...\n",
      " [0.1        0.1        1.09998235 ... 0.1        0.1        0.1       ]\n",
      " [0.1        0.1        0.1        ... 0.1        0.1        1.10000572]\n",
      " [2.09999495 0.1        0.1        ... 0.1        0.1        0.1       ]]\n"
     ]
    }
   ],
   "source": [
    "tweets_qa = pd.read_csv('/Users/houhiroshisakai/Desktop/Schulich/Term 2/MBAN 6090/quantafrica_since_2018_01_01.csv')\n",
    "\n",
    "texts_qa = tweets_qa['Tweet'].tolist()\n",
    "\n",
    "clean_texts_qa = [clean_text(text) for text in texts_qa]\n",
    "\n",
    "# ContextVectorize\n",
    "X_qa = vectorizer.fit_transform(clean_texts_qa)\n",
    "\n",
    "# Apply LDA\n",
    "lda_qa = LatentDirichletAllocation(n_components=10)\n",
    "topics_qa = lda_qa.fit(X_qa)\n",
    "\n",
    "print(lda_qa.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "tech join today talentdev career time pm session miss space\n",
      "Topic #1:\n",
      "data tools analysis visualization powerful provides yes web software science\n",
      "Topic #2:\n",
      "learn used learning web olorunsheyi tech community quantaafrica markup google\n",
      "Topic #3:\n",
      "tech startup quanta africa session community idea talentdev offer dr\n",
      "Topic #4:\n",
      "quanta technology tech collaborate innovate new free alimosho quantaafrica techbros\n",
      "Topic #5:\n",
      "future cofounder tech quanta today innovation yes community way pertinencegroup\n",
      "Topic #6:\n",
      "session startup join tomorrow quanta idea day today pitch link\n",
      "Topic #7:\n",
      "website create official offers react creating library design allows interactive\n",
      "Topic #8:\n",
      "javascript web language programming covers used development google angular dont\n",
      "Topic #9:\n",
      "css year html quanta set amazing africa talentdev dont browser\n"
     ]
    }
   ],
   "source": [
    "feature_names_qa = vectorizer.get_feature_names_out()\n",
    "print_top_words(lda_qa, feature_names_qa, n_top_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top words per topic\n",
    "top_words_per_topic = []\n",
    "for topic_idx, topic in enumerate(lda_qa.components_):\n",
    "    top_features_ind = topic.argsort()[:-11:-1]\n",
    "    top_features = [feature_names_qa[i] for i in top_features_ind]\n",
    "    top_words_per_topic.append(top_features)\n",
    "\n",
    "# Create a DataFrame with the top words for each topic\n",
    "df_top_words_qa = pd.DataFrame(top_words_per_topic)\n",
    "df_top_words_transposed_qa = df_top_words_qa.T \n",
    "\n",
    "df_qa_prefixed = df_top_words_transposed_qa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.1        0.1        0.1        ... 0.10010024 0.1        0.1       ]\n",
      " [0.1        0.1        0.1        ... 0.10006001 0.10001029 0.1       ]\n",
      " [0.1        2.09998136 1.10000165 ... 0.1        0.1        0.1       ]\n",
      " ...\n",
      " [0.1        0.1        1.09998182 ... 0.1        0.1        0.1       ]\n",
      " [0.10007801 0.1        0.1        ... 0.1        1.099982   0.1       ]\n",
      " [0.1        0.1        0.1        ... 2.09983974 0.1        0.1       ]]\n"
     ]
    }
   ],
   "source": [
    "tweets_sb = pd.read_csv('/Users/houhiroshisakai/Desktop/Schulich/Term 2/MBAN 6090/StandardBankZA_since_2018_01_01.csv')\n",
    "\n",
    "texts_sb = tweets_sb['Tweet'].tolist()\n",
    "\n",
    "clean_texts_sb = [clean_text(text) for text in texts_sb]\n",
    "\n",
    "# ContextVectorize\n",
    "X_sb = vectorizer.fit_transform(clean_texts_sb)\n",
    "\n",
    "# Apply LDA\n",
    "lda_sb = LatentDirichletAllocation(n_components=10)\n",
    "topics_sb = lda_sb.fit_transform(X_sb)\n",
    "\n",
    "print(lda_sb.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "sblove instantmoneymondays money instant sblovessummer thats right wallet itcanbe answer\n",
      "Topic #1:\n",
      "account card hi banking funds app use need transaction link\n",
      "Topic #2:\n",
      "sblove sblovessummer welcome great youre thats love thanks wow glad\n",
      "Topic #3:\n",
      "thank hi getting team touch appreciate matter attention support assistance\n",
      "Topic #4:\n",
      "banking hi best hey say forward clients family experience makes\n",
      "Topic #5:\n",
      "app know let thank appreciate try love hi device banking\n",
      "Topic #6:\n",
      "dm details hi contact assist number send team like look\n",
      "Topic #7:\n",
      "hi beatthescam issues provide sblovessummer error app kindly help better\n",
      "Topic #8:\n",
      "team hi bank reach fraud standard payment address email provide\n",
      "Topic #9:\n",
      "sblove sblovessummer financial goals year savings like budget link youre\n"
     ]
    }
   ],
   "source": [
    "feature_names_sb = vectorizer.get_feature_names_out()\n",
    "print_top_words(lda_sb, feature_names_sb, n_top_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top words per topic\n",
    "top_words_per_topic = []\n",
    "for topic_idx, topic in enumerate(lda_sb.components_):\n",
    "    top_features_ind = topic.argsort()[:-11:-1]\n",
    "    top_features = [feature_names_sb[i] for i in top_features_ind]\n",
    "    top_words_per_topic.append(top_features)\n",
    "\n",
    "# Create a DataFrame with the top words for each topic\n",
    "df_top_words_sb = pd.DataFrame(top_words_per_topic)\n",
    "df_top_words_transposed_sb = df_top_words_sb.T \n",
    "\n",
    "df_sb_prefixed = df_top_words_transposed_sb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## North Thern Trust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.1        0.1        1.1000008  ... 0.1        0.1        0.10000726]\n",
      " [0.1        0.1        0.1        ... 1.3130838  4.09999417 0.1       ]\n",
      " [1.09967644 0.10002437 0.1        ... 0.10000947 0.1        0.10001047]\n",
      " ...\n",
      " [0.1        0.1        0.1        ... 2.57112763 1.09998915 5.09995854]\n",
      " [0.10030724 0.1        1.0999992  ... 0.1        0.1        0.1       ]\n",
      " [0.1        0.1        0.1        ... 0.1        3.10002106 0.1       ]]\n"
     ]
    }
   ],
   "source": [
    "tweets_nt = pd.read_csv('/Users/houhiroshisakai/Desktop/Schulich/Term 2/MBAN 6090/NTWealth_since_2018_01_01.csv')\n",
    "\n",
    "texts_nt = tweets_nt['Tweet'].tolist()\n",
    "\n",
    "clean_texts_nt = [clean_text(text) for text in texts_nt]\n",
    "\n",
    "# ContextVectorize\n",
    "X_nt = vectorizer.fit_transform(clean_texts_nt)\n",
    "\n",
    "# Apply LDA\n",
    "lda_nt = LatentDirichletAllocation(n_components=10)\n",
    "topics_nt = lda_nt.fit(X_nt)\n",
    "\n",
    "print(lda_nt.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "business wealth women region president family planning magazine learn elevating\n",
      "Topic #1:\n",
      "tax planning trust new northern learn business wealth event proud\n",
      "Topic #2:\n",
      "katie nixon inflation investors cio management wealth fed policy outlook\n",
      "Topic #3:\n",
      "global trust northern family expochicago dont jim inflation mcdonald paul\n",
      "Topic #4:\n",
      "nixon katie cio management wealth market art questions energy trust\n",
      "Topic #5:\n",
      "private bank northerntrust best art northern policy community digital expochicago\n",
      "Topic #6:\n",
      "northern trust chief officer expochicago fiduciary wealth announce management help\n",
      "Topic #7:\n",
      "wealth tax financialeducation learn retirement family charitable financial resources support\n",
      "Topic #8:\n",
      "financial plan wealth northern future trust learn american families explore\n",
      "Topic #9:\n",
      "wealth business northern learn art strategies hosted common trust explore\n"
     ]
    }
   ],
   "source": [
    "feature_names_nt = vectorizer.get_feature_names_out()\n",
    "print_top_words(lda_nt, feature_names_nt, n_top_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top words per topic\n",
    "top_words_per_topic = []\n",
    "for topic_idx, topic in enumerate(lda_nt.components_):\n",
    "    top_features_ind = topic.argsort()[:-11:-1]\n",
    "    top_features = [feature_names_nt[i] for i in top_features_ind]\n",
    "    top_words_per_topic.append(top_features)\n",
    "\n",
    "# Create a DataFrame with the top words for each topic\n",
    "df_top_words_nt = pd.DataFrame(top_words_per_topic)\n",
    "df_top_words_transposed_nt = df_top_words_nt.T \n",
    "\n",
    "df_nt_prefixed = df_top_words_transposed_nt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UBA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.10003267 0.10025843 0.1        ... 0.1        0.1        0.1       ]\n",
      " [0.1        0.1        0.1        ... 0.1        0.10009902 0.10005755]\n",
      " [0.1        0.1        0.1        ... 0.10000779 0.10002096 0.1001622 ]\n",
      " ...\n",
      " [0.1        0.1        0.1        ... 0.10000518 0.10008848 0.10001905]\n",
      " [1.09981844 0.1        2.09992242 ... 0.10000133 0.1        2.0997612 ]\n",
      " [0.1        0.1        0.1        ... 0.10000178 0.1        0.1       ]]\n"
     ]
    }
   ],
   "source": [
    "tweets_uba = pd.read_csv('/Users/houhiroshisakai/Desktop/Schulich/Term 2/MBAN 6090/UBAGroup_since_2018_01_01.csv')\n",
    "\n",
    "texts_uba = tweets_uba['Tweet'].tolist()\n",
    "\n",
    "clean_texts_uba = [clean_text(text) for text in texts_uba]\n",
    "\n",
    "# ContextVectorize\n",
    "X_uba = vectorizer.fit_transform(clean_texts_uba)\n",
    "\n",
    "# Apply LDA\n",
    "lda_uba = LatentDirichletAllocation(n_components=10)\n",
    "topics = lda_uba.fit_transform(X_uba)\n",
    "\n",
    "print(lda_uba.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "thank uba hello bank contacting africas global send link like\n",
      "Topic #1:\n",
      "africa africasglobalbank uba african tonyoelumelu bank group read ubaafricanentrepreneurs business\n",
      "Topic #2:\n",
      "africasglobalbank ubamarketplace love happy day today friday ubaat leorewards uba\n",
      "Topic #3:\n",
      "number account enable mobile kindly assist thank provide email address\n",
      "Topic #4:\n",
      "uba africasglobalbank ubaafricaday africa africaday ubaafricaconversations join group click tonyoelumelu\n",
      "Topic #5:\n",
      "informed hello response thank dm kindly provided bitlymlzbnp emanate uba\n",
      "Topic #6:\n",
      "africasglobalbank winners win uba draw time way nan ubacares ubaat\n",
      "Topic #7:\n",
      "africasglobalbank new ubaat make best ubaceoawards airtime win happy today\n",
      "Topic #8:\n",
      "thank hello kindly dm bitlymlzbnp link sharing avoid public click\n",
      "Topic #9:\n",
      "account hello leo open chat uba dial visit thank ubacares\n"
     ]
    }
   ],
   "source": [
    "feature_names_uba = vectorizer.get_feature_names_out()\n",
    "print_top_words(lda_uba, feature_names_uba, n_top_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top words per topic\n",
    "top_words_per_topic = []\n",
    "for topic_idx, topic in enumerate(lda_uba.components_):\n",
    "    top_features_ind = topic.argsort()[:-11:-1]\n",
    "    top_features = [feature_names_uba[i] for i in top_features_ind]\n",
    "    top_words_per_topic.append(top_features)\n",
    "\n",
    "# Create a DataFrame with the top words for each topic\n",
    "df_top_words_uba = pd.DataFrame(top_words_per_topic)\n",
    "df_top_words_transposed_uba = df_top_words_uba.T \n",
    "\n",
    "df_uba_prefixed = df_top_words_transposed_uba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Asia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HSBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.1        2.09997873 0.1        ... 0.1        3.09997939 0.1       ]\n",
      " [0.1        0.10000269 0.1        ... 1.09989396 0.10001897 0.1       ]\n",
      " [0.1        0.1        2.10002437 ... 0.1        0.1        0.1       ]\n",
      " ...\n",
      " [0.1        0.1        1.13442599 ... 0.10012791 0.1        1.09999601]\n",
      " [0.1        0.1        0.1        ... 1.09997813 0.1        0.1       ]\n",
      " [0.1        0.10001858 1.09997081 ... 0.1        0.1        1.10000399]]\n"
     ]
    }
   ],
   "source": [
    "tweets_hsbc = pd.read_csv('/Users/houhiroshisakai/Desktop/Schulich/Term 2/MBAN 6090/HSBC_since_2018_01_01.csv')\n",
    "\n",
    "texts_hsbc = tweets_hsbc['Tweet'].tolist()\n",
    "\n",
    "clean_texts_hsbc = [clean_text(text) for text in texts_hsbc]\n",
    "\n",
    "# ContextVectorize\n",
    "X_hsbc = vectorizer.fit_transform(clean_texts_hsbc)\n",
    "\n",
    "# Apply LDA\n",
    "lda_hsbc = LatentDirichletAllocation(n_components=10)\n",
    "topics = lda_hsbc.fit(X_hsbc)\n",
    "\n",
    "print(lda_hsbc.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "hsbc results read climate financial sustainable today transition hsbcresults change\n",
      "Topic #1:\n",
      "hsbc colleagues proud mental health support inclusive future role business\n",
      "Topic #2:\n",
      "hsbc help asset digital support covid management way banking bond\n",
      "Topic #3:\n",
      "hsbc john china ceo flint business ciie hsbcnavigator group international\n",
      "Topic #4:\n",
      "businesses hsbcnavigator hsbc global report asia read new sustainable research\n",
      "Topic #5:\n",
      "hsbc ceo noel quinn global support customers covid future world\n",
      "Topic #6:\n",
      "supply hsbc global chain chains asian new hsbcs chinese trade\n",
      "Topic #7:\n",
      "bank global hsbc trade finance banking technology customers year best\n",
      "Topic #8:\n",
      "businesses business solutions climate partnership future help global world new\n",
      "Topic #9:\n",
      "social cities netzero sustainability transition businesses global read green environmental\n"
     ]
    }
   ],
   "source": [
    "feature_names_hsbc = vectorizer.get_feature_names_out()\n",
    "print_top_words(lda_hsbc, feature_names_hsbc, n_top_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top words per topic\n",
    "top_words_per_topic = []\n",
    "for topic_idx, topic in enumerate(lda_hsbc.components_):\n",
    "    top_features_ind = topic.argsort()[:-11:-1]\n",
    "    top_features = [feature_names_hsbc[i] for i in top_features_ind]\n",
    "    top_words_per_topic.append(top_features)\n",
    "\n",
    "# Create a DataFrame with the top words for each topic\n",
    "df_top_words_hsbc = pd.DataFrame(top_words_per_topic)\n",
    "df_top_words_transposed_hsbc = df_top_words_hsbc.T \n",
    "\n",
    "df_hsbc_prefixed = df_top_words_transposed_hsbc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OCBC (Singapore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.10000923 0.1        0.1        ... 0.1        0.1        0.10000349]\n",
      " [4.99671433 0.1        2.09996787 ... 0.1        0.10011599 0.10001212]\n",
      " [0.1        0.10000119 1.0999994  ... 0.10046125 0.10007097 0.1       ]\n",
      " ...\n",
      " [0.1        0.1        0.1        ... 0.10001367 0.1        0.1       ]\n",
      " [0.1        0.1        0.1        ... 0.1        0.1        0.1       ]\n",
      " [0.10000483 0.10000709 0.1        ... 0.10008208 0.1        0.1       ]]\n"
     ]
    }
   ],
   "source": [
    "tweets_ocbc = pd.read_csv('/Users/houhiroshisakai/Desktop/Schulich/Term 2/MBAN 6090/OCBCBank_since_2018_01_01.csv')\n",
    "\n",
    "texts_ocbc = tweets_ocbc['Tweet'].tolist()\n",
    "\n",
    "clean_texts_ocbc = [clean_text(text) for text in texts_ocbc]\n",
    "\n",
    "# ContextVectorize\n",
    "X_ocbc = vectorizer.fit_transform(clean_texts_ocbc)\n",
    "\n",
    "# Apply LDA\n",
    "lda_ocbc = LatentDirichletAllocation(n_components=10)\n",
    "topics_ocbc = lda_ocbc.fit(X_ocbc)\n",
    "\n",
    "print(lda_ocbc.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "sh youre tweet hey came welcome hi transactions onetoken transaction\n",
      "Topic #1:\n",
      "thank dm hi xf contact share sorry details hear number\n",
      "Topic #2:\n",
      "account hi card sh xf access banking code pin clarify\n",
      "Topic #3:\n",
      "service banking dg hi executive able internet update online form\n",
      "Topic #4:\n",
      "sh app thanks mobile banking version know good phone morning\n",
      "Topic #5:\n",
      "ocbc hi singapore dg touch looking malaysia xf reached relevant\n",
      "Topic #6:\n",
      "hi try issue dg inconvenience thank app working caused apologise\n",
      "Topic #7:\n",
      "email hi secured send sh thank banking sent mail reference\n",
      "Topic #8:\n",
      "hi dm dg drop number able sh bank mobile assist\n",
      "Topic #9:\n",
      "sh account transfer ocbc hi cheque deposit pm cash overseas\n"
     ]
    }
   ],
   "source": [
    "feature_names_ocbc = vectorizer.get_feature_names_out()\n",
    "print_top_words(lda_ocbc, feature_names_ocbc, n_top_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top words per topic\n",
    "top_words_per_topic = []\n",
    "for topic_idx, topic in enumerate(lda_ocbc.components_):\n",
    "    top_features_ind = topic.argsort()[:-11:-1]\n",
    "    top_features = [feature_names_ocbc[i] for i in top_features_ind]\n",
    "    top_words_per_topic.append(top_features)\n",
    "\n",
    "# Create a DataFrame with the top words for each topic\n",
    "df_top_words_ocbc = pd.DataFrame(top_words_per_topic)\n",
    "df_top_words_transposed_ocbc = df_top_words_ocbc.T \n",
    "\n",
    "df_ocbc_prefixed = df_top_words_transposed_ocbc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bank of Singapore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.10000203 0.1        0.1        ... 1.10000253 0.10000824 0.1       ]\n",
      " [0.1        0.1        0.1        ... 1.09999747 1.55721679 0.1       ]\n",
      " [0.10005602 0.1000711  0.1        ... 0.1        0.10002339 0.1000418 ]\n",
      " ...\n",
      " [0.1        1.09996478 1.09999983 ... 0.1        0.1000431  0.10002427]\n",
      " [1.09999918 0.1        0.1        ... 0.1        0.1        0.1       ]\n",
      " [0.1        0.1        0.1        ... 0.1        3.73516201 0.1       ]]\n"
     ]
    }
   ],
   "source": [
    "tweets_bos = pd.read_csv('/Users/houhiroshisakai/Desktop/Schulich/Term 2/MBAN 6090/bankofSG_since_2018_01_01.csv')\n",
    "\n",
    "texts_bos = tweets_bos['Tweet'].tolist()\n",
    "\n",
    "clean_texts_bos = [clean_text(text) for text in texts_bos]\n",
    "\n",
    "# ContextVectorize\n",
    "X_bos = vectorizer.fit_transform(clean_texts_bos)\n",
    "\n",
    "# Apply LDA\n",
    "lda_bos = LatentDirichletAllocation(n_components=10)\n",
    "topics_bos = lda_bos.fit(X_bos)\n",
    "\n",
    "print(lda_bos.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "research global singapore management head chia jean outlook portfolio office\n",
      "Topic #1:\n",
      "market investment watch chief months markets global officer insights rajeev\n",
      "Topic #2:\n",
      "economist chief says jerram mansoor richard bank mohiuddin growth economy\n",
      "Topic #3:\n",
      "global social need sustainable market development investors says years covid\n",
      "Topic #4:\n",
      "investment lee head eli strategy says markets policy strategist ahead\n",
      "Topic #5:\n",
      "moh siong sim strategist currency says usd risk likely fx\n",
      "Topic #6:\n",
      "global head marc walle products van family new shares investors\n",
      "Topic #7:\n",
      "fed economist inflation chief mansoor mohiuddin rate federal rates expected\n",
      "Topic #8:\n",
      "says bank chief uk mohiuddin mansoor economist global business ceo\n",
      "Topic #9:\n",
      "private china bank market greater best wealth global management head\n"
     ]
    }
   ],
   "source": [
    "feature_names_bos = vectorizer.get_feature_names_out()\n",
    "print_top_words(lda_bos, feature_names_bos, n_top_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top words per topic\n",
    "top_words_per_topic = []\n",
    "for topic_idx, topic in enumerate(lda_bos.components_):\n",
    "    top_features_ind = topic.argsort()[:-11:-1]\n",
    "    top_features = [feature_names_bos[i] for i in top_features_ind]\n",
    "    top_words_per_topic.append(top_features)\n",
    "\n",
    "# Create a DataFrame with the top words for each topic\n",
    "df_top_words_bos = pd.DataFrame(top_words_per_topic)\n",
    "df_top_words_transposed_bos = df_top_words_bos.T \n",
    "\n",
    "df_bos_prefixed = df_top_words_transposed_bos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hana Bank (South Korea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.1         0.1         0.1         0.1        11.21419964  0.1\n",
      "   0.10000851  0.1         0.1         0.1         0.1         0.1\n",
      "   0.1         0.1         1.45714247  0.1         0.1         1.09999113\n",
      "   0.1        10.09999591  0.1         0.1         0.1        15.25902512\n",
      "   0.1         0.1         0.1         0.1         6.72208089  0.10002792\n",
      "  12.09999817  1.09995343  0.1         2.53811121  0.1         0.1\n",
      "   5.09998636  0.1         0.10000072  0.1         0.1         0.10000822\n",
      "   0.1         0.1000095   0.1         0.1         0.10000077  2.09993984\n",
      "   0.1         0.1        12.09996105  0.1         3.09998988 20.80710785\n",
      "   0.1         0.1         0.1         0.1         0.1        10.0999853\n",
      "   0.1         0.1         0.10005934  0.1         0.1         0.1\n",
      "   6.64064604  0.1         0.1         0.1         0.1         0.1\n",
      "   0.1        16.09997569 22.41458938  1.09995951  0.10001176  0.1\n",
      "   0.1         0.1         0.1         2.09999213  0.1         0.1\n",
      "   0.1         0.1         0.1         0.1         1.89978849  2.09999213\n",
      "   3.09996477  0.1         0.1        15.18968619  4.09994394  0.10000405\n",
      "   0.1        10.09999591  0.1         0.1       ]\n",
      " [ 0.1         2.1         2.10001617  0.1         0.1         0.1\n",
      "   0.1         0.1         1.10001084  0.1         0.1         0.1\n",
      "   0.1         0.1         0.1         0.1         1.10001264  0.1\n",
      "   0.1         0.1         1.10000802  0.1         0.1         0.1\n",
      "   0.1         1.09999824  0.1         0.1         0.1         0.1\n",
      "   0.1         0.1         1.09999721  0.1         1.09999846  0.1\n",
      "   0.1000565   0.1         2.0999876   0.1         0.1         0.1\n",
      "   0.1         0.1         0.1         0.1         0.1         0.1\n",
      "   1.10000043  0.1         0.1         2.10000576  0.1         0.1\n",
      "   2.1000414   0.1         1.10000406  0.1         0.1         0.1\n",
      "   0.1         0.1         0.1         0.1         1.09999758  0.1\n",
      "   0.1         1.0999995   0.1         0.1         0.1         1.10000573\n",
      "   0.1         0.1         0.1         0.1         0.1         1.10000119\n",
      "   0.1         1.10001692  0.1         0.1         0.1         1.09998691\n",
      "   1.10000426  0.1         0.1         1.09999736  0.1         0.1\n",
      "   0.1         0.1         0.1         0.1         0.1         0.1\n",
      "   1.10000573  0.1         0.1         1.099997  ]\n",
      " [ 0.1         0.1         0.1         0.1         0.10000198  0.1\n",
      "   4.59858859  0.1         0.1         0.1         0.1         0.1\n",
      "   4.73648488  0.1         4.74285182  0.1         0.1         0.1\n",
      "   0.1         0.10000408  0.1         0.1         1.27255697  5.94096632\n",
      "   0.1000067   0.1         0.1         0.1         9.47787408  3.09996554\n",
      "   0.10000183  0.1         0.1         2.66186351  0.1         0.1\n",
      "   0.10002645  0.1         0.1         1.09985029  0.1         5.09998613\n",
      "   0.1         0.10001981  0.1         0.1         0.1         0.10003092\n",
      "   0.1         0.1         0.10000338  0.1000067   0.10001012 17.9452629\n",
      "   0.1         0.1         0.1         0.1         0.1         0.10001099\n",
      "   0.1         0.1         4.09994065  1.09995208  0.1         1.09986525\n",
      "   9.05890401  0.1         0.1         0.1         0.1         0.1\n",
      "   0.1         0.10002202 16.785386    0.1         6.76106189  0.1\n",
      "   0.1         0.1         0.1         0.10000787  0.1         0.1\n",
      "   0.1         0.1         0.1         0.1         1.30008464  0.10000787\n",
      "   0.10003523  0.1         0.1         2.01031181  0.10001031 12.10005155\n",
      "   0.1         0.10000408  1.09999937  0.1       ]\n",
      " [ 0.1         0.1         0.10000209  0.1         3.98579837  0.1\n",
      "   0.1         0.1         0.1         3.09998869  0.1         0.10002422\n",
      "   0.1         0.1         0.1         0.1         0.1         0.1\n",
      "   1.09999995  0.1         0.1         0.10000342  0.1         1.10001352\n",
      "   0.1         0.10000209  0.10000342  1.099993    0.1         0.1\n",
      "   0.1         1.10004656  0.1         0.1         0.1         0.1\n",
      "   0.1         0.1         2.20966326  0.1         0.1         0.1\n",
      "   0.1         0.1         0.1         0.1         2.10006795  0.10009219\n",
      "   0.1         0.1         0.10006126  0.1         0.1         0.1\n",
      "   0.1         0.1         0.1         0.1         0.10003408  0.1\n",
      "   0.10000241  0.1         0.1         0.10000262  0.1         0.1\n",
      "   0.1         2.09987785  0.1         0.1         0.1         0.1\n",
      "   0.1         0.1         0.1         0.1         0.1         0.1\n",
      "   0.1         0.1         0.1         0.1         0.1         1.10001308\n",
      "   0.1         0.1         0.10001658  0.1         0.1         0.1\n",
      "   0.1         0.1         0.10000342  0.1         0.1         0.1\n",
      "   0.1         0.1         0.1         1.100003  ]\n",
      " [ 0.1         0.1         0.1         1.10001467  0.1         0.1\n",
      "   0.1         0.1         0.1         0.1         0.1         0.1\n",
      "   1.1000061   1.1000049   0.1         0.1         0.1         0.1\n",
      "   0.1         0.1         0.1         2.1000058   0.1         0.1\n",
      "   0.1         0.1         0.1         0.1         0.1         0.1\n",
      "   0.1         0.1         0.1         0.1         0.1         0.1\n",
      "   0.1         0.1         0.1         0.1         0.1         0.1\n",
      "   0.10000968  0.1         0.1         0.1         0.1         0.1\n",
      "   0.1         0.10002379  0.1         1.10001808  0.1         0.1\n",
      "   0.1         0.10013261  0.1         0.1         0.1         0.1\n",
      "   0.10001575  0.1         0.1         0.10000387  0.1         0.1\n",
      "   0.1         0.1         0.10001559  0.1         0.1         0.1\n",
      "   0.1         0.1         0.1         0.1         0.1         0.1\n",
      "   7.09998283  0.1         0.1         0.1         0.1         0.1\n",
      "   0.1         0.10004199  0.1         0.1         0.1         0.1\n",
      "   0.1         1.10002514  0.1         0.1         0.1         0.1\n",
      "   0.1         0.1         0.1         0.1       ]\n",
      " [ 0.1         0.1         1.12564444  1.09998533  0.1         3.1\n",
      "   0.1         1.10000839  1.09998915  0.1         2.1         0.10001303\n",
      "   0.1         2.0999951   0.1         1.09999199  0.1         0.1\n",
      "   0.1         0.1         1.09999197  0.10000184  0.1         0.1\n",
      "   1.09998921  1.12565494  0.10000184  0.1         0.1         0.1\n",
      "   0.1         0.1         0.1         0.1         0.1         1.09999474\n",
      "   0.1         0.1000151   2.30317957  0.1         0.1         0.1\n",
      "   0.10001212  0.10000152  0.1         3.1         1.09993859  0.1\n",
      "   1.09999957  5.09986222  0.1         1.09996195  0.1         0.1\n",
      "   1.09996927  2.09986738  0.1         2.1         2.09975484  0.1\n",
      "   0.10000676  1.09999406  0.1         5.0623339   0.1         1.10000903\n",
      "   0.1         0.10000986  0.10001953  0.1         1.0999911   1.09999427\n",
      "   0.10000245  0.1         0.1         0.1         0.1         0.1\n",
      "   0.10001002  1.10000001  1.09999789  0.1         3.1         0.1\n",
      "   0.1         3.09993386  2.09998361  0.1         0.1         0.1\n",
      "   0.1         0.1         0.10000184  0.1         0.1         1.09994439\n",
      "   1.09999427  0.1         1.10000062  0.1       ]\n",
      " [ 3.09999766  0.1         3.07433336  0.1         0.1         0.1\n",
      "   1.09997817  0.1         0.1         0.10001131  0.1         0.10022325\n",
      "   0.1         0.1         0.1         0.1         1.09998736  0.1\n",
      "   1.10000004  0.1         0.1         2.09998635  0.1         0.1\n",
      "   1.09998802  3.07434079  2.09999216  0.1         0.1         0.1\n",
      "   0.1         0.1         2.09999902  0.1         0.1         1.10000525\n",
      "   0.1         0.1         2.92820627  0.1         0.1         0.1\n",
      "   3.0999782   0.1         1.09999657  0.1         0.1         1.09989684\n",
      "   0.1         0.10000606  1.10000713  0.1         0.1         0.1\n",
      "   0.1         0.1         0.1         0.1         0.1001853   0.1\n",
      "   5.09997193  0.1         0.1         2.13770554  0.1         0.1\n",
      "   0.1         0.10009893  2.09996488  0.1         0.1         0.1\n",
      "   1.09997806  0.1         0.1         0.1         0.1         1.09999881\n",
      "   0.10000714  1.09998305  0.1         0.1         0.1         0.1\n",
      "   0.1         0.10002415  4.0999998   0.1         0.1         0.1\n",
      "   0.1         1.09997485  2.09999216  0.1         0.1         0.1\n",
      "   0.1         0.1         0.1         0.1       ]\n",
      " [ 0.1         0.1         0.1         0.1         0.1         0.1\n",
      "   1.19397643  0.1         0.1         0.1         0.1         0.1\n",
      "   0.1         0.1         0.1         1.100008    0.1         1.10000887\n",
      "   0.1         0.1         0.1         0.1         0.1         0.1\n",
      "   0.1         0.1         0.1         1.10000699  0.1         0.10000653\n",
      "   0.1         0.1         0.1         0.1000054   0.1         0.1\n",
      "   2.09993067  0.10011409  1.95895586  0.1         0.1         1.10000564\n",
      "   0.1         3.09996733  1.10000342  0.1         1.09999268  0.1\n",
      "   0.1         0.10004847  0.1         0.1         0.1         0.10000126\n",
      "   0.1         0.1         1.10000547  0.1         0.1         0.10000371\n",
      "   0.1         1.10000594  0.1         0.1         0.1         0.1\n",
      "   0.10000149  0.1         0.1         0.1         0.1         0.1\n",
      "   2.10001653  0.10000228  0.10000127  0.1         0.1         0.1\n",
      "   0.1         0.1         0.1         0.1         0.1         0.1\n",
      "   0.1         0.1         0.1         1.10000264  1.10002872  0.1\n",
      "   0.1         0.1         0.1         0.10000199  0.1         0.1\n",
      "   0.1         0.1         0.1         0.1       ]\n",
      " [ 0.1         0.1         0.1         0.1         0.1         0.1\n",
      "   1.50744829  0.1         0.1         0.1         0.1         0.1\n",
      "   3.4635052   0.1         0.1000057   0.1         0.1         0.1\n",
      "   0.1         0.1         0.1         0.1         1.92744302  0.1\n",
      "   2.10001607  0.1         0.1         0.1         0.10000268  0.1\n",
      "   0.1         0.1         0.1         1.09999889  0.1         0.1\n",
      "   0.1         2.0998708   0.10000279  0.10012571  0.1         0.1\n",
      "   0.1         0.10000184  0.1         0.1         0.1         1.10004019\n",
      "   0.1         0.10000783  0.1         2.10000751  0.1         3.54766818\n",
      "   1.09998932  0.1         1.09999047  0.1         0.1         0.1\n",
      "   0.1         0.1         0.1         0.1         0.1         0.10012571\n",
      "   3.60044845  0.1         0.1         2.1         0.1         0.1\n",
      "   0.10000297  0.1         0.10000148  0.1         4.43892634  0.1\n",
      "   0.1         0.1         1.1000021   0.1         0.1         0.1\n",
      "   1.09999573  0.1         0.1         0.1         0.10009814  0.1\n",
      "   0.1         0.1         0.1         0.1         5.10002002  0.1\n",
      "   0.1         0.1         0.1         0.1       ]\n",
      " [ 0.10000234  0.1         0.10000394  0.1         0.1         0.1\n",
      "   0.1         1.0999916   0.1         0.1         0.1         2.09973949\n",
      "   1.10000381  0.1         0.1         0.1         0.1         0.1\n",
      "   0.1         0.1         0.1         0.10000259  0.1         1.09999503\n",
      "   0.1         0.10000394  0.10000259  0.1         2.10004234  0.1\n",
      "   0.1         0.1         0.10000377  1.10002098  1.10000153  0.1\n",
      "   0.1         0.1         0.10000393  1.10002399  2.1         0.1\n",
      "   0.1         0.1         0.1         0.1         0.1         0.1\n",
      "   0.1         2.10005163  1.09996717  0.1         0.1         1.0999598\n",
      "   0.1         0.1         0.1         0.1         0.10002577  0.1\n",
      "   0.10000315  0.1         0.1         0.10000198  1.10000242  0.1\n",
      "   0.1         0.10001386  0.1         0.1         1.10000889  0.1\n",
      "   0.1         0.1         2.10002186  1.10004047  0.1         0.1\n",
      "   0.1         0.1         0.1         0.1         0.1         0.1\n",
      "   0.1         0.1         0.1         0.1         0.1         0.1\n",
      "   0.1         0.1         0.10000259  0.1         2.10002572  0.1\n",
      "   0.1         0.1         0.1         0.1       ]]\n"
     ]
    }
   ],
   "source": [
    "tweets_hana = pd.read_csv('/Users/houhiroshisakai/Desktop/Schulich/Term 2/MBAN 6090/HanaBank4expats_since_2018_01_01.csv')\n",
    "\n",
    "texts_hana = tweets_hana['Tweet'].tolist()\n",
    "\n",
    "clean_texts_hana = [clean_text(text) for text in texts_hana]\n",
    "\n",
    "# ContextVectorize\n",
    "X_hana = vectorizer.fit_transform(clean_texts_hana)\n",
    "\n",
    "# Apply LDA\n",
    "lda_hana = LatentDirichletAllocation(n_components=10)\n",
    "topics = lda_hana.fit(X_hana)\n",
    "\n",
    "print(lda_hana.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "rate krwusd range exchange week forecasts korean bank dollar wonus\n",
      "Topic #1:\n",
      "like app koreas annual hana st countries best dont win\n",
      "Topic #2:\n",
      "krwusd rate weeks forecast outlook report exchange heres complete check\n",
      "Topic #3:\n",
      "bank card hana keb overseas foreign exchange tell youll doesnt\n",
      "Topic #4:\n",
      "seoul easy visit koreas arent check coast lines tips korea\n",
      "Topic #5:\n",
      "korea new sunday july banking tips hana locations change coast\n",
      "Topic #6:\n",
      "money travel abroad home ez app hana new foreigners fast\n",
      "Topic #7:\n",
      "hope pyeongchang good hana banks tuesday day cool fly month\n",
      "Topic #8:\n",
      "weekly report outlook krwusd check expats koreas parent group english\n",
      "Topic #9:\n",
      "korea forecast weekly rate hard cheaper rates hapsmagazine fx picture\n"
     ]
    }
   ],
   "source": [
    "feature_names_hana = vectorizer.get_feature_names_out()\n",
    "print_top_words(lda_hana, feature_names_hana, n_top_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top words per topic\n",
    "top_words_per_topic = []\n",
    "for topic_idx, topic in enumerate(lda_hana.components_):\n",
    "    top_features_ind = topic.argsort()[:-11:-1]\n",
    "    top_features = [feature_names_hana[i] for i in top_features_ind]\n",
    "    top_words_per_topic.append(top_features)\n",
    "\n",
    "# Create a DataFrame with the top words for each topic\n",
    "df_top_words_hana = pd.DataFrame(top_words_per_topic)\n",
    "df_top_words_transposed_hana = df_top_words_hana.T \n",
    "\n",
    "df_hana_prefixed = df_top_words_transposed_hana"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UFJ (Japan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mizuho Bank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oceania"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Australia and New Zealand Banking Group Limited  (ANZ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.00067728e-01 4.33370129e+00 1.00000001e-01 ... 1.00020650e-01\n",
      "  1.00028194e-01 1.00069214e-01]\n",
      " [1.00000001e-01 7.53892798e+01 1.00000001e-01 ... 5.64640353e+00\n",
      "  1.00000000e-01 1.00001775e-01]\n",
      " [1.00000001e-01 2.73116440e+01 1.22052456e+00 ... 1.00013281e-01\n",
      "  9.09993915e+00 1.00041036e-01]\n",
      " ...\n",
      " [1.00053985e-01 4.97790851e+01 1.00002910e-01 ... 1.00000001e-01\n",
      "  1.00015751e-01 1.00017712e-01]\n",
      " [1.00000001e-01 4.16668725e+01 1.00000001e-01 ... 4.04158777e+00\n",
      "  1.00000000e-01 1.00000000e-01]\n",
      " [1.00000001e-01 1.46629923e+02 1.00000001e-01 ... 2.53832999e+00\n",
      "  1.00004102e-01 1.00000000e-01]]\n"
     ]
    }
   ],
   "source": [
    "tweets_anz = pd.read_csv('/Users/houhiroshisakai/Desktop/Schulich/Term 2/MBAN 6090/ANZ_AU_since_2018_01_01.csv')\n",
    "\n",
    "texts_anz = tweets_anz['Tweet'].tolist()\n",
    "\n",
    "clean_texts_anz = [clean_text(text) for text in texts_anz]\n",
    "\n",
    "# ContextVectorize\n",
    "X_anz = vectorizer.fit_transform(clean_texts_anz)\n",
    "\n",
    "# Apply LDA\n",
    "lda_anz = LatentDirichletAllocation(n_components=10)\n",
    "topics_anz = lda_anz.fit(X_anz)\n",
    "\n",
    "print(lda_anz.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "card credit anz hi account need access select check youre\n",
      "Topic #1:\n",
      "number contact dm post hi like code send postcode phone\n",
      "Topic #2:\n",
      "sorry hi team banking kindly internet anz thanks darren ampm\n",
      "Topic #3:\n",
      "hi account payments funds branch business sorry payment nishant atm\n",
      "Topic #4:\n",
      "thanks email hi hoax delete link sms hoaxcybersecurityanzcom click message\n",
      "Topic #5:\n",
      "hi dm sorry send help thanks hear message details look\n",
      "Topic #6:\n",
      "team thanks hi feedback pm aest pass customer contact ampm\n",
      "Topic #7:\n",
      "anz hi app pay new branch link banking available use\n",
      "Topic #8:\n",
      "issue hi app issues try sorry anz working inconvenience banking\n",
      "Topic #9:\n",
      "hi inconvenience banking internet caused app anz know let apologise\n"
     ]
    }
   ],
   "source": [
    "feature_names_anz = vectorizer.get_feature_names_out()\n",
    "print_top_words(lda_anz, feature_names_anz, n_top_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top words per topic\n",
    "top_words_per_topic = []\n",
    "for topic_idx, topic in enumerate(lda_anz.components_):\n",
    "    top_features_ind = topic.argsort()[:-11:-1]\n",
    "    top_features = [feature_names_anz[i] for i in top_features_ind]\n",
    "    top_words_per_topic.append(top_features)\n",
    "\n",
    "# Create a DataFrame with the top words for each topic\n",
    "df_top_words_anz = pd.DataFrame(top_words_per_topic)\n",
    "df_top_words_transposed_anz = df_top_words_anz.T \n",
    "\n",
    "df_anz_prefixed = df_top_words_transposed_anz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commonwealth Bank of Australia (CBA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.10000215 0.1        0.1        ... 0.1        2.95719437 1.41099343]\n",
      " [1.09997963 0.1        3.63515521 ... 0.1        0.1        1.55548059]\n",
      " [0.10000143 0.1        1.56482571 ... 0.1        0.1        0.1       ]\n",
      " ...\n",
      " [1.09998801 0.1        0.10000062 ... 0.1        0.10001849 0.1       ]\n",
      " [0.10000783 0.1        0.10000792 ... 1.10000096 1.31117473 0.10002546]\n",
      " [0.10000712 2.09999191 0.10000096 ... 0.1        0.10001478 0.1       ]]\n"
     ]
    }
   ],
   "source": [
    "tweets_cba = pd.read_csv('/Users/houhiroshisakai/Desktop/Schulich/Term 2/MBAN 6090/CommBank_since_2018_01_01.csv')\n",
    "\n",
    "texts_cba = tweets_cba['Tweet'].tolist()\n",
    "\n",
    "clean_texts_cba = [clean_text(text) for text in texts_cba]\n",
    "\n",
    "# ContextVectorize\n",
    "X_cba = vectorizer.fit_transform(clean_texts_cba)\n",
    "\n",
    "# Apply LDA\n",
    "lda_cba = LatentDirichletAllocation(n_components=10)\n",
    "topics_cba = lda_cba.fit_transform(X_cba)\n",
    "\n",
    "print(lda_cba.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "know thanks information report scams hoaxcbacomau let future security rina\n",
      "Topic #1:\n",
      "commbank app banking personal yello delete eligibility information click thanks\n",
      "Topic #2:\n",
      "check remember online look scams calls stop reject scam messages\n",
      "Topic #3:\n",
      "dm details hi like reach chris sorry thank help assistance\n",
      "Topic #4:\n",
      "customers hi message help scams branches genuine need funds atms\n",
      "Topic #5:\n",
      "send like hi message contact private number understand hear feedback\n",
      "Topic #6:\n",
      "dm send hi christine siji best sorry chat hello visit\n",
      "Topic #7:\n",
      "team need commbank hi account payid scam link thank transfers\n",
      "Topic #8:\n",
      "hi information help thanks team assist send enquiry understand dm\n",
      "Topic #9:\n",
      "card hi message details commbank anna thank assist credit dm\n"
     ]
    }
   ],
   "source": [
    "feature_names_cba = vectorizer.get_feature_names_out()\n",
    "print_top_words(lda_cba, feature_names_cba, n_top_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top words per topic\n",
    "top_words_per_topic = []\n",
    "for topic_idx, topic in enumerate(lda_cba.components_):\n",
    "    top_features_ind = topic.argsort()[:-11:-1]\n",
    "    top_features = [feature_names_cba[i] for i in top_features_ind]\n",
    "    top_words_per_topic.append(top_features)\n",
    "\n",
    "# Create a DataFrame with the top words for each topic\n",
    "df_top_words_cba = pd.DataFrame(top_words_per_topic)\n",
    "df_top_words_transposed_cba = df_top_words_cba.T \n",
    "\n",
    "df_cba_prefixed = df_top_words_transposed_cba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## National Australia Bank (NAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.1        4.93462299 0.1        ... 2.09998513 0.10000742 0.10016927]\n",
      " [0.10000152 0.10002018 0.1        ... 0.100015   0.1        0.1       ]\n",
      " [2.09992841 0.10001398 2.09998719 ... 0.1        0.10000107 0.1       ]\n",
      " ...\n",
      " [0.10000363 0.10003891 0.10000909 ... 0.1        0.1        0.1       ]\n",
      " [1.12515841 5.54515423 0.1        ... 0.1        0.1        0.1       ]\n",
      " [0.10000613 0.10001053 0.1        ... 0.10000814 3.09999151 0.1       ]]\n"
     ]
    }
   ],
   "source": [
    "tweets_nab = pd.read_csv('/Users/houhiroshisakai/Desktop/Schulich/Term 2/MBAN 6090/NAB_since_2018_01_01.csv')\n",
    "\n",
    "texts_nab = tweets_nab['Tweet'].tolist()\n",
    "\n",
    "clean_texts_nab = [clean_text(text) for text in texts_nab]\n",
    "\n",
    "# ContextVectorize\n",
    "X_nab = vectorizer.fit_transform(clean_texts_nab)\n",
    "\n",
    "# Apply LDA\n",
    "lda_nab = LatentDirichletAllocation(n_components=10)\n",
    "topics_nab = lda_nab.fit_transform(X_nab)\n",
    "\n",
    "print(lda_nab.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "dm send hi chat help like rg im id keen\n",
      "Topic #1:\n",
      "feedback team thanks understand appreciate like hi lodge thats wait\n",
      "Topic #2:\n",
      "banking app nab internet hi payments mobile issues working sorry\n",
      "Topic #3:\n",
      "need glad help rl hear youre worries pm team branch\n",
      "Topic #4:\n",
      "message delete hi thanks nab messages team security aware suspicious\n",
      "Topic #5:\n",
      "sorry hi im hear dm send able inconvenience working thanks\n",
      "Topic #6:\n",
      "thanks know hi team letting link ive ill rl click\n",
      "Topic #7:\n",
      "nab card hi business customers banking account new home loan\n",
      "Topic #8:\n",
      "know try pay hi let apple looking tc latest youre\n",
      "Topic #9:\n",
      "lh hey hear sorry im card rg ng hi account\n"
     ]
    }
   ],
   "source": [
    "feature_names_nab = vectorizer.get_feature_names_out()\n",
    "print_top_words(lda_nab, feature_names_nab, n_top_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top words per topic\n",
    "top_words_per_topic = []\n",
    "for topic_idx, topic in enumerate(lda_nab.components_):\n",
    "    top_features_ind = topic.argsort()[:-11:-1]\n",
    "    top_features = [feature_names_nab[i] for i in top_features_ind]\n",
    "    top_words_per_topic.append(top_features)\n",
    "\n",
    "# Create a DataFrame with the top words for each topic\n",
    "df_top_words_nab = pd.DataFrame(top_words_per_topic)\n",
    "df_top_words_transposed_nab = df_top_words_nab.T \n",
    "\n",
    "df_nab_prefixed = df_top_words_transposed_nab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Westpac Banking Corporation (WBC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5.05368795  0.1         4.0286193  ...  0.99267232  0.1\n",
      "   0.1       ]\n",
      " [ 0.1         0.10000157 16.85344738 ...  1.20725812  0.10001896\n",
      "   0.10002431]\n",
      " [ 0.1         0.1000011   0.10001475 ...  0.1         0.10007906\n",
      "   0.1       ]\n",
      " ...\n",
      " [ 0.1         0.10002816  0.10000561 ...  0.1         3.07233027\n",
      "   0.1       ]\n",
      " [ 0.1         0.1         0.10002982 ...  0.1         0.1\n",
      "   0.1       ]\n",
      " [ 0.14618015  0.10000926  2.60879172 ...  0.10006955  0.10005166\n",
      "   0.1       ]]\n"
     ]
    }
   ],
   "source": [
    "tweets_wbc = pd.read_csv('/Users/houhiroshisakai/Desktop/Schulich/Term 2/MBAN 6090/Westpac_since_2018_01_01.csv')\n",
    "\n",
    "texts_wbc = tweets_wbc['Tweet'].tolist()\n",
    "\n",
    "clean_texts_wbc = [clean_text(text) for text in texts_wbc]\n",
    "\n",
    "# ContextVectorize\n",
    "X_wbc = vectorizer.fit_transform(clean_texts_wbc)\n",
    "\n",
    "# Apply LDA\n",
    "lda_wbc = LatentDirichletAllocation(n_components=10)\n",
    "topics_wbc = lda_wbc.fit_transform(X_wbc)\n",
    "\n",
    "print(lda_wbc.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "dm send like concerns feedback ensure look hi thanks support\n",
      "Topic #1:\n",
      "team thanks hi branch know days help contact card pm\n",
      "Topic #2:\n",
      "dm send whats like information understanding better help best hi\n",
      "Topic #3:\n",
      "westpac hi help customers hope helps australia loan thanks home\n",
      "Topic #4:\n",
      "hi future open apple customers pay remain offering banking thanks\n",
      "Topic #5:\n",
      "message direct help information send hi complaints private right including\n",
      "Topic #6:\n",
      "hi know banking responded online issues inconvenience dm weve caused\n",
      "Topic #7:\n",
      "westpac pay help bank use australians apple payments customer cards\n",
      "Topic #8:\n",
      "sorry im hear hi help dm youve send way left\n",
      "Topic #9:\n",
      "hear dm hi send thanks sorry help im experience like\n"
     ]
    }
   ],
   "source": [
    "feature_names_wbc = vectorizer.get_feature_names_out()\n",
    "print_top_words(lda_wbc, feature_names_wbc, n_top_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top words per topic\n",
    "top_words_per_topic = []\n",
    "for topic_idx, topic in enumerate(lda_wbc.components_):\n",
    "    top_features_ind = topic.argsort()[:-11:-1]\n",
    "    top_features = [feature_names_wbc[i] for i in top_features_ind]\n",
    "    top_words_per_topic.append(top_features)\n",
    "\n",
    "# Create a DataFrame with the top words for each topic\n",
    "df_top_words_wbc = pd.DataFrame(top_words_per_topic)\n",
    "df_top_words_transposed_wbc = df_top_words_wbc.T \n",
    "\n",
    "df_wbc_prefixed = df_top_words_transposed_wbc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Europe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BNP Paribas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.1         4.36495021  0.1        ...  0.1         0.10000125\n",
      "   0.1       ]\n",
      " [ 0.1        30.53112384  0.1        ...  0.1         0.1\n",
      "   0.1       ]\n",
      " [ 0.1         0.10000065  0.1        ...  0.1         0.1\n",
      "   0.10000606]\n",
      " ...\n",
      " [ 0.1         0.100008    0.1        ...  0.1         0.10000034\n",
      "   0.10000306]\n",
      " [ 0.1         1.80231969  0.1        ...  0.1         0.1\n",
      "   0.1       ]\n",
      " [ 0.10010057  0.10000431  4.09999202 ...  2.1        11.09997778\n",
      "   6.09370476]]\n"
     ]
    }
   ],
   "source": [
    "tweets_bnp = pd.read_csv('/Users/houhiroshisakai/Desktop/Schulich/Term 2/MBAN 6090/BNPParibas_since_2018_01_01.csv')\n",
    "\n",
    "texts_bnp = tweets_bnp['Tweet'].tolist()\n",
    "\n",
    "clean_texts_bnp = [clean_text(text) for text in texts_bnp]\n",
    "\n",
    "# ContextVectorize\n",
    "X_bnp = vectorizer.fit_transform(clean_texts_bnp)\n",
    "\n",
    "# Apply LDA\n",
    "lda_bnp = LatentDirichletAllocation(n_components=10)\n",
    "topics_bnp = lda_bnp.fit(X_bnp)\n",
    "\n",
    "print(lda_bnp.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "positivebanking ceo bonnafé oyw group bnpparibas bank bnp paribas bnppcoalitions\n",
      "Topic #1:\n",
      "vivatech new mobility clients sustainable solutions bnppadvance data business support\n",
      "Topic #2:\n",
      "women vivatech wfgm tech positivebanking diversity meet womens_forum gender entrepreneurs\n",
      "Topic #3:\n",
      "vivatech startups positivebanking discover lab fondationbnpp innovation program bnppadvance digital\n",
      "Topic #4:\n",
      "paribas bnp vivatech support tennis young bnppcsr people impact discover\n",
      "Topic #5:\n",
      "clients wish day contact good answer dear account attended order\n",
      "Topic #6:\n",
      "jblefevre jbonnel ym bivwak xbond nicochan pierrecappelli fgraillot sebbourguignon atoucinho\n",
      "Topic #7:\n",
      "vous nous bonjour contacter dm afin bonne conseiller que par\n",
      "Topic #8:\n",
      "head positivebanking pff antoinesire vivatech ceo company engagement talk business\n",
      "Topic #9:\n",
      "la pour le et les des en nous du sur\n"
     ]
    }
   ],
   "source": [
    "feature_names_bnp = vectorizer.get_feature_names_out()\n",
    "print_top_words(lda_bnp, feature_names_bnp, n_top_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top words per topic\n",
    "top_words_per_topic = []\n",
    "for topic_idx, topic in enumerate(lda_bnp.components_):\n",
    "    top_features_ind = topic.argsort()[:-11:-1]\n",
    "    top_features = [feature_names_bnp[i] for i in top_features_ind]\n",
    "    top_words_per_topic.append(top_features)\n",
    "\n",
    "# Create a DataFrame with the top words for each topic\n",
    "df_top_words_bnp = pd.DataFrame(top_words_per_topic)\n",
    "df_top_words_transposed_bnp = df_top_words_bnp.T \n",
    "\n",
    "df_bnp_prefixed = df_top_words_transposed_bnp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BNP Asset Mgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.09996406 0.1        4.09996341 ... 2.09996159 7.10000161 0.1       ]\n",
      " [0.1        0.1        0.1        ... 0.1000202  0.10000039 0.1       ]\n",
      " [0.1        0.1        0.1        ... 0.1        0.1        0.1       ]\n",
      " ...\n",
      " [0.1        0.10000294 1.10000221 ... 6.09960653 1.09994041 0.1       ]\n",
      " [0.1        0.1        0.1000108  ... 0.1        0.1000064  0.10000842]\n",
      " [0.10000841 0.1        1.09995137 ... 0.10002204 0.1000074  0.1       ]]\n"
     ]
    }
   ],
   "source": [
    "tweets_bnp1 = pd.read_csv('/Users/houhiroshisakai/Desktop/Schulich/Term 2/MBAN 6090/BNPPAM_COM_since_2018_01_01.csv')\n",
    "\n",
    "texts_bnp1 = tweets_bnp1['Tweet'].tolist()\n",
    "\n",
    "clean_texts_bnp1 = [clean_text(text) for text in texts_bnp1]\n",
    "\n",
    "# ContextVectorize\n",
    "X_bnp1 = vectorizer.fit_transform(clean_texts_bnp1)\n",
    "\n",
    "# Apply LDA\n",
    "lda_bnp1 = LatentDirichletAllocation(n_components=10)\n",
    "topics_bnp1 = lda_bnp1.fit(X_bnp1)\n",
    "\n",
    "print(lda_bnp1.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "bnppam_com china investment senior equities tells head greaterchina chinas economist\n",
      "Topic #1:\n",
      "bnppam_com head plsainvest stand jambachtsheer real global sustainability announce appointment\n",
      "Topic #2:\n",
      "video solutions latest learn investors multiasset covid sustainability strategy investing\n",
      "Topic #3:\n",
      "market asset read article paribas bnp inflation podcast markets latest\n",
      "Topic #4:\n",
      "bnppam_com head investment debt esg apac investors tells stewardship wilsonotto\n",
      "Topic #5:\n",
      "bnppam_com paul head sandhu apac client maqs investors advisory tells\n",
      "Topic #6:\n",
      "awards bnppam_com year best fund asset esg manager sustainability equity\n",
      "Topic #7:\n",
      "climate sri change global investment sustainable discover people research sustainability\n",
      "Topic #8:\n",
      "learn thegreatinstability economic inflation investors chinas chinese rates usd markets\n",
      "Topic #9:\n",
      "read markets article investing latest equity equities sustainable growth future\n"
     ]
    }
   ],
   "source": [
    "feature_names_bnp1 = vectorizer.get_feature_names_out()\n",
    "print_top_words(lda_bnp1, feature_names_bnp1, n_top_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top words per topic\n",
    "top_words_per_topic = []\n",
    "for topic_idx, topic in enumerate(lda_bnp1.components_):\n",
    "    top_features_ind = topic.argsort()[:-11:-1]\n",
    "    top_features = [feature_names_bnp1[i] for i in top_features_ind]\n",
    "    top_words_per_topic.append(top_features)\n",
    "\n",
    "# Create a DataFrame with the top words for each topic\n",
    "df_top_words_bnp1 = pd.DataFrame(top_words_per_topic)\n",
    "df_top_words_transposed_bnp1 = df_top_words_bnp1.T \n",
    "\n",
    "df_bnp1_prefixed = df_top_words_transposed_bnp1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crédit Agricole Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.1        0.1        0.10005227 ... 0.1        0.10013138 0.10001695]\n",
      " [0.1        0.1        0.1        ... 0.1        2.81311784 0.1       ]\n",
      " [0.1        0.1        2.09994773 ... 0.1        1.10001044 0.1       ]\n",
      " ...\n",
      " [0.10001833 0.1        0.1        ... 1.10001179 0.1        0.1       ]\n",
      " [0.1        0.1        0.1        ... 0.1        0.10002414 0.10001977]\n",
      " [0.1        0.1        0.1        ... 0.1        0.1        2.09996328]]\n"
     ]
    }
   ],
   "source": [
    "tweets_cag = pd.read_csv('/Users/houhiroshisakai/Desktop/Schulich/Term 2/MBAN 6090/more_banks/Credit_Agricole_since_2018_01_01.csv')\n",
    "\n",
    "texts_cag = tweets_cag['Tweet'].tolist()\n",
    "\n",
    "clean_texts_cag = [clean_text(text) for text in texts_cag]\n",
    "\n",
    "# ContextVectorize\n",
    "X_cag = vectorizer.fit_transform(clean_texts_cag)\n",
    "\n",
    "# Apply LDA\n",
    "lda_cag = LatentDirichletAllocation(n_components=10)\n",
    "topics_cag = lda_cag.fit(X_cag)\n",
    "\n",
    "print(lda_cag.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "vivatech ca_cib cavivatech le nous sur stand notre aux avec\n",
      "Topic #1:\n",
      "group ukraine agricole banking credit teams million customer met fund\n",
      "Topic #2:\n",
      "russia companies financing russian hi war institutions soon ukraine begun\n",
      "Topic #3:\n",
      "pour vous la en nous et plus les des le\n",
      "Topic #4:\n",
      "le la les et des en pour dans du sur\n",
      "Topic #5:\n",
      "vous nous la le en que une pff et je\n",
      "Topic #6:\n",
      "du le crédit agricole et sa en directeur la général\n",
      "Topic #7:\n",
      "et du nos aca pour résultats en le est les\n",
      "Topic #8:\n",
      "la et les des le en du au pour avec\n",
      "Topic #9:\n",
      "la vous sur les dans pour des bonne le creditagricole\n"
     ]
    }
   ],
   "source": [
    "feature_names_cag = vectorizer.get_feature_names_out()\n",
    "print_top_words(lda_cag, feature_names_cag, n_top_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top words per topic\n",
    "top_words_per_topic = []\n",
    "for topic_idx, topic in enumerate(lda_cag.components_):\n",
    "    top_features_ind = topic.argsort()[:-11:-1]\n",
    "    top_features = [feature_names_cag[i] for i in top_features_ind]\n",
    "    top_words_per_topic.append(top_features)\n",
    "\n",
    "# Create a DataFrame with the top words for each topic\n",
    "df_top_words_cag = pd.DataFrame(top_words_per_topic)\n",
    "df_top_words_transposed_cag = df_top_words_cag.T \n",
    "\n",
    "df_cag_prefixed = df_top_words_transposed_cag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Barclays PLC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.94436966  0.1         0.10000569 ... 11.51836224  0.1\n",
      "   0.86748877]\n",
      " [ 1.20153368  0.1         0.10000127 ...  0.10000182  6.09998554\n",
      "   0.1       ]\n",
      " [ 1.24283358  1.10001608  9.09994601 ...  0.10001354  0.1\n",
      "   0.10000943]\n",
      " ...\n",
      " [39.53882524  0.1         0.10000125 ... 38.12746009  0.1\n",
      "   0.10002848]\n",
      " [ 6.48425319  0.1         0.1        ...  9.35933992  0.10000131\n",
      "   3.0264945 ]\n",
      " [ 2.62484127  0.1         0.1        ...  0.10008565  0.1\n",
      "   1.173503  ]]\n"
     ]
    }
   ],
   "source": [
    "tweets_barclays = pd.read_csv('/Users/houhiroshisakai/Desktop/Schulich/Term 2/MBAN 6090/more_banks/Barclays_since_2018_01_01.csv')\n",
    "\n",
    "texts_barclays = tweets_barclays['Tweet'].tolist()\n",
    "\n",
    "clean_texts_barclays = [clean_text(text) for text in texts_barclays]\n",
    "\n",
    "# ContextVectorize\n",
    "X_barclays = vectorizer.fit_transform(clean_texts_barclays)\n",
    "\n",
    "# Apply LDA\n",
    "lda_barclays = LatentDirichletAllocation(n_components=10)\n",
    "topics_barclays = lda_barclays.fit(X_barclays)\n",
    "\n",
    "print(lda_barclays.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "sorry try im whats today happened need help thank app\n",
      "Topic #1:\n",
      "colleagues customers happy barclays clients world uk bank new wish\n",
      "Topic #2:\n",
      "check app ensure need absolutely account pay youre working update\n",
      "Topic #3:\n",
      "complaint like log youd link complaints fully sure include make\n",
      "Topic #4:\n",
      "thanks hi available hope youve hey theyre youre help askbarclaysus\n",
      "Topic #5:\n",
      "barclays group read new venkatakrishnan cs proud results years colleagues\n",
      "Topic #6:\n",
      "banking post local office app community online weve use thank\n",
      "Topic #7:\n",
      "youre sorry im know thanks dm let hi help really\n",
      "Topic #8:\n",
      "dm number barclaysukhelp contact postcode pop send help chat support\n",
      "Topic #9:\n",
      "barclays rewards blue message community wimbledon today offer support day\n"
     ]
    }
   ],
   "source": [
    "feature_names_barclays = vectorizer.get_feature_names_out()\n",
    "print_top_words(lda_barclays, feature_names_barclays, n_top_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top words per topic\n",
    "top_words_per_topic = []\n",
    "for topic_idx, topic in enumerate(lda_barclays.components_):\n",
    "    top_features_ind = topic.argsort()[:-11:-1]\n",
    "    top_features = [feature_names_barclays[i] for i in top_features_ind]\n",
    "    top_words_per_topic.append(top_features)\n",
    "\n",
    "# Create a DataFrame with the top words for each topic\n",
    "df_top_words_barclays = pd.DataFrame(top_words_per_topic)\n",
    "df_top_words_transposed_barclays = df_top_words_barclays.T \n",
    "\n",
    "df_barclays_prefixed = df_top_words_transposed_barclays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Banco Santander SA (BSSA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.10001716 0.1        0.10000877 ... 0.1        0.1        0.1       ]\n",
      " [0.1        2.09991637 1.1000022  ... 0.1        0.1        0.1       ]\n",
      " [2.09999297 0.1        1.09998903 ... 0.1        0.1        0.1       ]\n",
      " ...\n",
      " [0.1        0.1        0.1        ... 0.1        0.1        0.1       ]\n",
      " [0.1        0.10001883 0.1        ... 0.1        0.1        0.1       ]\n",
      " [1.09996789 0.1        0.1        ... 0.1        0.1        0.1       ]]\n"
     ]
    }
   ],
   "source": [
    "tweets_bssa = pd.read_csv('/Users/houhiroshisakai/Desktop/Schulich/Term 2/MBAN 6090/more_banks/bancosantander_since_2018_01_01.csv')\n",
    "\n",
    "texts_bssa = tweets_bssa['Tweet'].tolist()\n",
    "\n",
    "clean_texts_bssa = [clean_text(text) for text in texts_bssa]\n",
    "\n",
    "# ContextVectorize\n",
    "X_bssa = vectorizer.fit_transform(clean_texts_bssa)\n",
    "\n",
    "# Apply LDA\n",
    "lda_bssa = LatentDirichletAllocation(n_components=10)\n",
    "topics_bssa = lda_bssa.fit(X_bssa)\n",
    "\n",
    "print(lda_bssa.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "la en el que para los más las por es\n",
      "Topic #1:\n",
      "santander start challenge day work make future ready blockchain dont\n",
      "Topic #2:\n",
      "en la del los el para santander resultados las accionistas\n",
      "Topic #3:\n",
      "santander millones en euros results beneficio el más bancosantander al\n",
      "Topic #4:\n",
      "en para una la más que santander tu los el\n",
      "Topic #5:\n",
      "help digital new financial know bank work want santander people\n",
      "Topic #6:\n",
      "por gracias hola favor en banco que te nuestros compañeros\n",
      "Topic #7:\n",
      "santander banking global know digital bank today anabotin best dont\n",
      "Topic #8:\n",
      "million profit customers really santander attributable share money santanders pay\n",
      "Topic #9:\n",
      "san madrid acción cierre bancosantander public presence dirigiendo contributed outlook\n"
     ]
    }
   ],
   "source": [
    "feature_names_bssa = vectorizer.get_feature_names_out()\n",
    "print_top_words(lda_bssa, feature_names_bssa, n_top_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top words per topic\n",
    "top_words_per_topic = []\n",
    "for topic_idx, topic in enumerate(lda_bssa.components_):\n",
    "    top_features_ind = topic.argsort()[:-11:-1]\n",
    "    top_features = [feature_names_bssa[i] for i in top_features_ind]\n",
    "    top_words_per_topic.append(top_features)\n",
    "\n",
    "# Create a DataFrame with the top words for each topic\n",
    "df_top_words_bssa = pd.DataFrame(top_words_per_topic)\n",
    "df_top_words_transposed_bssa = df_top_words_bssa.T \n",
    "\n",
    "df_bssa_prefixed = df_top_words_transposed_bssa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group BPCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.02583507 2.02583826 0.1        ... 2.10002695 0.1        0.1       ]\n",
      " [0.1        0.1        0.1        ... 3.22075618 0.1        0.1       ]\n",
      " [0.10000404 0.10001002 0.1        ... 1.91668401 0.1        0.1       ]\n",
      " ...\n",
      " [0.1        0.1        2.09980337 ... 0.10000534 2.0999528  0.10001037]\n",
      " [0.1        0.1        0.10001947 ... 4.77325404 0.1        0.1       ]\n",
      " [1.17416089 1.17413987 0.1        ... 3.76341116 0.1        0.1       ]]\n"
     ]
    }
   ],
   "source": [
    "tweets_bpce = pd.read_csv('/Users/houhiroshisakai/Desktop/Schulich/Term 2/MBAN 6090/more_banks/GroupeBPCE_since_2018_01_01.csv')\n",
    "\n",
    "texts_bpce = tweets_bpce['Tweet'].tolist()\n",
    "\n",
    "clean_texts_bpce = [clean_text(text) for text in texts_bpce]\n",
    "\n",
    "# ContextVectorize\n",
    "X_bpce = vectorizer.fit_transform(clean_texts_bpce)\n",
    "\n",
    "# Apply LDA\n",
    "lda_bpce = LatentDirichletAllocation(n_components=10)\n",
    "topics_bpce = lda_bpce.fit(X_bpce)\n",
    "\n",
    "print(lda_bpce.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "la en et le les par sur des une au\n",
      "Topic #1:\n",
      "des le la les par et en sur pour avec\n",
      "Topic #2:\n",
      "vous bien bonjour cordialement je dm le si votre souhaitez\n",
      "Topic #3:\n",
      "du la les laurent mignon des président directoire bpce groupe\n",
      "Topic #4:\n",
      "la et les du des pour le leur en bpce\n",
      "Topic #5:\n",
      "du bpce et groupe des les résultats la le sur\n",
      "Topic #6:\n",
      "le pour et en la une les voilebanquepop par des\n",
      "Topic #7:\n",
      "du le et groupe les bpce des en pour la\n",
      "Topic #8:\n",
      "et du les paris la groupe des le bpce pour\n",
      "Topic #9:\n",
      "la le et des du groupe les en bpce une\n"
     ]
    }
   ],
   "source": [
    "feature_names_bpce = vectorizer.get_feature_names_out()\n",
    "print_top_words(lda_bpce, feature_names_bpce, n_top_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top words per topic\n",
    "top_words_per_topic = []\n",
    "for topic_idx, topic in enumerate(lda_bpce.components_):\n",
    "    top_features_ind = topic.argsort()[:-11:-1]\n",
    "    top_features = [feature_names_bpce[i] for i in top_features_ind]\n",
    "    top_words_per_topic.append(top_features)\n",
    "\n",
    "# Create a DataFrame with the top words for each topic\n",
    "df_top_words_bpce = pd.DataFrame(top_words_per_topic)\n",
    "df_top_words_transposed_bpce = df_top_words_bpce.T \n",
    "\n",
    "df_bpce_prefixed = df_top_words_transposed_bpce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.DataFrame()\n",
    "#Canada\n",
    "df_canada = pd.DataFrame([['Canada']], columns = [0])\n",
    "#Store RBC to excel file\n",
    "df_rbc_with_name = pd.DataFrame([['RBC']], columns=[0])  # Only a cell includes \"RBC\"\n",
    "df_rbc_with_topics = pd.concat([df_canada, df_rbc_with_name, df_rbc_prefixed], ignore_index=True)\n",
    "\n",
    "\n",
    "df_final = pd.concat([df_final, df_rbc_with_topics], ignore_index=True)\n",
    "#Store CIBC to excel file\n",
    "df_cibc_with_name = pd.DataFrame([['CIBC']], columns=[0]) \n",
    "df_cibc_with_topics = pd.concat([df_cibc_with_name, df_cibc_prefixed], ignore_index=True)\n",
    "df_final = pd.concat([df_final, df_cibc_with_topics], ignore_index=True)\n",
    "\n",
    "#Store Scotiabank to excel file\n",
    "df_scotia_with_name = pd.DataFrame([['ScotiaBank']], columns =[0])\n",
    "df_scotia_with_topics = pd.concat([df_scotia_with_name,df_scotia_prefixed], ignore_index= True)\n",
    "df_final = pd.concat([df_final, df_scotia_with_topics], ignore_index=True)\n",
    "\n",
    "#Store TD Canada News to excel file\n",
    "df_tdcanada_with_name = pd.DataFrame([['TD_Canada_News']], columns = [0])\n",
    "df_tdcanada_with_topics = pd.concat([df_tdcanada_with_name, df_tdcanada_prefixed], ignore_index = True)\n",
    "df_final = pd.concat([df_final, df_tdcanada_with_topics], ignore_index=True)\n",
    "\n",
    "#Store TD Canada Official Bank to excel file\n",
    "df_tdcanada1_with_name = pd.DataFrame([['TD_Canada']], columns = [0])\n",
    "df_tdcanada1_with_topics = pd.concat([df_tdcanada1_with_name, df_tdcanada1_prefixed], ignore_index = True)\n",
    "df_final = pd.concat([df_final, df_tdcanada1_with_topics], ignore_index=True)\n",
    "\n",
    "#US Area\n",
    "df_us = pd.DataFrame([['US']], columns = [0])\n",
    "#Store TD US News to excel file\n",
    "df_tdus_with_name = pd.DataFrame([['TD_US_News']], columns = [0])\n",
    "df_tdus_with_topics = pd.concat([df_us, df_tdus_with_name, df_tdus_prefixed], ignore_index = True)\n",
    "df_final = pd.concat([df_final, df_tdus_with_topics], ignore_index=True)\n",
    "\n",
    "#Store TD US Official Account to excel file\n",
    "df_tdus1_with_name = pd.DataFrame([['TD_US']], columns = [0])\n",
    "df_tdus1_with_topics = pd.concat([df_tdus1_with_name, df_tdus1_prefixed], ignore_index = True)\n",
    "df_final = pd.concat([df_final, df_tdus1_with_topics], ignore_index=True)\n",
    "\n",
    "#Store Morgan Stanley to excel file\n",
    "df_ms_with_name = pd.DataFrame([['Morgan Stanley']], columns = [0])\n",
    "df_ms_with_topics = pd.concat([df_ms_with_name, df_ms_prefixed], ignore_index = True)\n",
    "df_final = pd.concat([df_final, df_ms_with_topics], ignore_index=True)\n",
    "\n",
    "#Store UBS\n",
    "df_ubs_with_name = pd.DataFrame([['UBS']], columns = [0])\n",
    "df_ubs_with_topics = pd.concat([df_ubs_with_name, df_ubs_prefixed], ignore_index = True)\n",
    "df_final = pd.concat([df_final, df_ubs_with_topics], ignore_index=True)\n",
    "#Citi\n",
    "df_citi_with_name = pd.DataFrame([['Citi']], columns = [0])\n",
    "df_citi_with_topics = pd.concat([df_citi_with_name, df_citi_prefixed], ignore_index = True)\n",
    "df_final = pd.concat([df_final, df_citi_with_topics], ignore_index=True)\n",
    "#Wells Fargo\n",
    "df_wf_with_name = pd.DataFrame([['Wells Fargo']], columns = [0])\n",
    "df_wf_with_topics = pd.concat([df_wf_with_name, df_wf_prefixed], ignore_index = True)\n",
    "df_final = pd.concat([df_final, df_wf_with_topics], ignore_index=True)\n",
    "#BOA (Bank of America)\n",
    "df_boa_with_name = pd.DataFrame([['Bank of America']], columns = [0])\n",
    "df_boa_with_topics = pd.concat([df_boa_with_name, df_boa_prefixed], ignore_index = True)\n",
    "df_final = pd.concat([df_final, df_boa_with_topics], ignore_index=True)\n",
    "#JP Morgan\n",
    "df_jp_with_name = pd.DataFrame([['JP Morgan']], columns = [0])\n",
    "df_jp_with_topics = pd.concat([df_jp_with_name, df_jp_prefixed], ignore_index = True)\n",
    "df_final = pd.concat([df_final, df_jp_with_topics], ignore_index=True)\n",
    "#Raymond James\n",
    "df_rj_with_name = pd.DataFrame([['Raymond James']], columns = [0])\n",
    "df_rj_with_topics = pd.concat([df_rj_with_name, df_rj_prefixed], ignore_index = True)\n",
    "df_final = pd.concat([df_final, df_rj_with_topics], ignore_index=True)\n",
    "\n",
    "#Africa\n",
    "df_africa = pd.DataFrame([['Africa']], columns = [0])\n",
    "#Quant Africa\n",
    "df_qa_with_name = pd.DataFrame([['Quant Africa']], columns = [0])\n",
    "df_qa_with_topics = pd.concat([df_africa, df_qa_with_name, df_qa_prefixed], ignore_index = True)\n",
    "df_final = pd.concat([df_final, df_qa_with_topics], ignore_index=True)\n",
    "#Standard Bank\n",
    "df_sb_with_name = pd.DataFrame([['Standard Bank']], columns = [0])\n",
    "df_sb_with_topics = pd.concat([df_sb_with_name, df_sb_prefixed], ignore_index = True)\n",
    "df_final = pd.concat([df_final, df_sb_with_topics], ignore_index=True)\n",
    "#Northern Trust\n",
    "df_nt_with_name = pd.DataFrame([['Standard Bank']], columns = [0])\n",
    "df_nt_with_topics = pd.concat([df_nt_with_name, df_nt_prefixed], ignore_index = True)\n",
    "df_final = pd.concat([df_final, df_nt_with_topics], ignore_index=True)\n",
    "#UBA\n",
    "df_uba_with_name = pd.DataFrame([['UBA']], columns = [0])\n",
    "df_uba_with_topics = pd.concat([df_uba_with_name, df_uba_prefixed], ignore_index = True)\n",
    "df_final = pd.concat([df_final, df_uba_with_topics], ignore_index=True)\n",
    "\n",
    "#Asia\n",
    "df_asia = pd.DataFrame([['Asia']], columns = [0])\n",
    "#HSBC\n",
    "df_hsbc_with_name = pd.DataFrame([['HSBC']], columns = [0])\n",
    "df_hsbc_with_topics = pd.concat([df_asia, df_hsbc_with_name, df_hsbc_prefixed], ignore_index = True)\n",
    "df_final = pd.concat([df_final, df_hsbc_with_topics], ignore_index=True)\n",
    "#OCBC Premier banking\n",
    "df_ocbc_with_name = pd.DataFrame([['OCBC Premier Banking']], columns = [0])\n",
    "df_ocbc_with_topics = pd.concat([df_ocbc_with_name, df_ocbc_prefixed], ignore_index = True)\n",
    "df_final = pd.concat([df_final, df_ocbc_with_topics], ignore_index=True)\n",
    "#Bank of Singapore\n",
    "df_bos_with_name = pd.DataFrame([['Bank of Singapore']], columns = [0])\n",
    "df_bos_with_topics = pd.concat([df_bos_with_name, df_bos_prefixed], ignore_index = True)\n",
    "df_final = pd.concat([df_final, df_bos_with_topics], ignore_index=True)\n",
    "#Hana Bank\n",
    "df_hana_with_name = pd.DataFrame([['Hana Bank']], columns = [0])\n",
    "df_hana_with_topics = pd.concat([df_hana_with_name, df_hana_prefixed], ignore_index = True)\n",
    "df_final = pd.concat([df_final, df_hana_with_topics], ignore_index=True)\n",
    "\n",
    "#Oceania\n",
    "df_oceania = pd.DataFrame([['Oceania']], columns = [0])\n",
    "#ANZ\n",
    "df_anz_with_name = pd.DataFrame([['ANZ']], columns = [0])\n",
    "df_anz_with_topics = pd.concat([df_oceania, df_anz_with_name, df_anz_prefixed], ignore_index = True)\n",
    "df_final = pd.concat([df_final, df_anz_with_topics], ignore_index=True)\n",
    "#CBA\n",
    "df_cba_with_name = pd.DataFrame([['CBA']], columns = [0])\n",
    "df_cba_with_topics = pd.concat([df_cba_with_name, df_cba_prefixed], ignore_index = True)\n",
    "df_final = pd.concat([df_final, df_cba_with_topics], ignore_index=True)\n",
    "#NAB\n",
    "df_nab_with_name = pd.DataFrame([['NAB']], columns = [0])\n",
    "df_nab_with_topics = pd.concat([df_nab_with_name, df_nab_prefixed], ignore_index = True)\n",
    "df_final = pd.concat([df_final, df_nab_with_topics], ignore_index=True)\n",
    "#WBC\n",
    "df_wbc_with_name = pd.DataFrame([['WBC']], columns = [0])\n",
    "df_wbc_with_topics = pd.concat([df_wbc_with_name, df_wbc_prefixed], ignore_index = True)\n",
    "df_final = pd.concat([df_final, df_wbc_with_topics], ignore_index=True)\n",
    "\n",
    "#Europe\n",
    "df_europe = pd.DataFrame([['Europe']], columns = [0])\n",
    "#BNP Paribas\n",
    "df_bnp_with_name = pd.DataFrame([['BNP Paribas']], columns = [0])\n",
    "df_bnp_with_topics = pd.concat([df_europe, df_bnp_with_name, df_bnp_prefixed], ignore_index = True)\n",
    "df_final = pd.concat([df_final, df_bnp_with_topics], ignore_index=True)\n",
    "#Crédit Agricole Group\n",
    "df_cag_with_name = pd.DataFrame([['Crédit Agricole Group']], columns = [0])\n",
    "df_cag_with_topics = pd.concat([df_cag_with_name, df_cag_prefixed], ignore_index = True)\n",
    "df_final = pd.concat([df_final, df_cag_with_topics], ignore_index=True)\n",
    "#Barclays PLC\n",
    "df_barclays_with_name = pd.DataFrame([['Barclays PLC']], columns = [0])\n",
    "df_barclays_with_topics = pd.concat([df_barclays_with_name, df_barclays_prefixed], ignore_index = True)\n",
    "df_final = pd.concat([df_final, df_barclays_with_topics], ignore_index=True)\n",
    "#Banco Santander SA\n",
    "df_bssa_with_name = pd.DataFrame([['Banco Santander SA']], columns = [0])\n",
    "df_bssa_with_topics = pd.concat([df_bssa_with_name, df_bssa_prefixed], ignore_index = True)\n",
    "df_final = pd.concat([df_final, df_bssa_with_topics], ignore_index=True)\n",
    "#Groupe BPCE\n",
    "df_bpce_with_name = pd.DataFrame([['BNP Paribas']], columns = [0])\n",
    "df_bpce_with_topics = pd.concat([df_bpce_with_name, df_bpce_prefixed], ignore_index = True)\n",
    "df_final = pd.concat([df_final, df_bpce_with_topics], ignore_index=True)\n",
    "\n",
    "\n",
    "excel_path = '/Users/houhiroshisakai/Desktop/Schulich/Term 2/MBAN 6090/ResultIntegration.xlsx'\n",
    "df_final.to_excel(excel_path, sheet_name='Bank_Topics', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
