{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/houhiroshisakai/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/houhiroshisakai/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Stanford Dogs dataset\n",
    "(raw_train, raw_validation, raw_test), metadata = tfds.load(\n",
    "    'stanford_dogs',\n",
    "    split=['train[:80%]', 'train[80%:90%]', 'train[90%:]'],\n",
    "    with_info=True,\n",
    "    as_supervised=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to resize and normalize images\n",
    "def preprocess(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = (image/255.0)  # Normalize pixel values\n",
    "    image = tf.image.resize(image, (224, 224))  # Resize images\n",
    "    return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to dataset\n",
    "train = raw_train.map(preprocess)\n",
    "validation = raw_validation.map(preprocess)\n",
    "test = raw_test.map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch and prefetch\n",
    "BATCH_SIZE = 32\n",
    "train_batches = train.shuffle(1000).batch(BATCH_SIZE).prefetch(1)\n",
    "validation_batches = validation.batch(BATCH_SIZE).prefetch(1)\n",
    "test_batches = test.batch(BATCH_SIZE).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment(image, label):\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_brightness(image, max_delta=0.5)\n",
    "    return image, label\n",
    "\n",
    "# Apply the `augment` function to each item in the training set using the map method\n",
    "train_batches = train_batches.map(augment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 222, 222, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 111, 111, 32)      0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 109, 109, 64)      18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 54, 54, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 52, 52, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 26, 26, 128)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 86528)             0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 86528)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               44302848  \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 120)               61560     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 44457656 (169.59 MB)\n",
      "Trainable params: 44457656 (169.59 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model Building\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "# Number of classes in the dataset\n",
    "num_classes = metadata.features['label'].num_classes\n",
    "\n",
    "# Building the CNN model\n",
    "model = Sequential([\n",
    "    # Convolutional Layer 1\n",
    "    Conv2D(32, (3,3), activation='relu', input_shape=(224, 224, 3)),\n",
    "    MaxPooling2D(2, 2),\n",
    "\n",
    "    # Convolutional Layer 2\n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "\n",
    "    # Convolutional Layer 3\n",
    "    Conv2D(128, (3,3), activation='relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "\n",
    "    # Flatten the results to feed into a Dense layer\n",
    "    Flatten(),\n",
    "    Dropout(0.5),\n",
    "\n",
    "    # 512 neuron hidden layer\n",
    "    Dense(512, activation='relu'),\n",
    "    Dense(num_classes, activation='softmax')  # Output layer\n",
    "])\n",
    "\n",
    "# Model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "300/300 [==============================] - 104s 344ms/step - loss: 4.4639 - accuracy: 0.0437 - val_loss: 4.5507 - val_accuracy: 0.0317\n",
      "Epoch 2/20\n",
      "300/300 [==============================] - 103s 342ms/step - loss: 3.6465 - accuracy: 0.1809 - val_loss: 5.2829 - val_accuracy: 0.0383\n",
      "Epoch 3/20\n",
      "300/300 [==============================] - 103s 342ms/step - loss: 1.7831 - accuracy: 0.5653 - val_loss: 7.5033 - val_accuracy: 0.0383\n",
      "Epoch 4/20\n",
      "300/300 [==============================] - 103s 342ms/step - loss: 0.7723 - accuracy: 0.8116 - val_loss: 9.2792 - val_accuracy: 0.0258\n"
     ]
    }
   ],
   "source": [
    "# Model Training and Optimization\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(\n",
    "    train_batches,\n",
    "    validation_data=validation_batches,\n",
    "    epochs=20,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=3, monitor='val_loss')]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 4s 100ms/step - loss: 9.4205 - accuracy: 0.0333\n",
      "Test accuracy: 0.03333333507180214\n",
      "38/38 [==============================] - 4s 95ms/step\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model\n",
    "test_loss, test_accuracy = model.evaluate(test_batches)\n",
    "print(f\"Test accuracy: {test_accuracy}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "y_pred = model.predict(test_batches)\n",
    "y_true = tf.concat([y for x, y in test_batches], axis=0)\n",
    "confusion_mtx = tf.math.confusion_matrix(y_true, tf.argmax(y_pred, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal'),\n",
    "    tf.keras.layers.experimental.preprocessing.RandomRotation(0.2),\n",
    "    tf.keras.layers.experimental.preprocessing.RandomZoom(0.2),\n",
    "    # Add more transformations as needed\n",
    "])\n",
    "\n",
    "# Apply data augmentation to the training dataset\n",
    "train_batches = train_batches.map(lambda x, y: (data_augmentation(x, training=True), y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# Adjust the model architecture\n",
    "model = Sequential([\n",
    "    # Add data augmentation as the first layer\n",
    "    data_augmentation,\n",
    "\n",
    "    # Convolutional Layer 1\n",
    "    Conv2D(32, (3,3), activation='relu', input_shape=(224, 224, 3), kernel_regularizer=l2(0.001)),\n",
    "    MaxPooling2D(2, 2),\n",
    "\n",
    "    # Add additional layers with regularization and dropout as needed\n",
    "\n",
    "    # Output layer with L2 Regularization\n",
    "    Dense(num_classes, activation='softmax', kernel_regularizer=l2(0.001))\n",
    "])\n",
    "\n",
    "# Compile the model again\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits defined at (most recent call last):\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/runpy.py\", line 87, in _run_code\n\n  File \"/Users/houhiroshisakai/Library/Python/3.9/lib/python/site-packages/ipykernel_launcher.py\", line 17, in <module>\n\n  File \"/Users/houhiroshisakai/Library/Python/3.9/lib/python/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n\n  File \"/Users/houhiroshisakai/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelapp.py\", line 725, in start\n\n  File \"/Users/houhiroshisakai/Library/Python/3.9/lib/python/site-packages/tornado/platform/asyncio.py\", line 195, in start\n\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py\", line 596, in run_forever\n\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py\", line 1890, in _run_once\n\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/events.py\", line 80, in _run\n\n  File \"/Users/houhiroshisakai/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelbase.py\", line 513, in dispatch_queue\n\n  File \"/Users/houhiroshisakai/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelbase.py\", line 502, in process_one\n\n  File \"/Users/houhiroshisakai/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelbase.py\", line 409, in dispatch_shell\n\n  File \"/Users/houhiroshisakai/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelbase.py\", line 729, in execute_request\n\n  File \"/Users/houhiroshisakai/Library/Python/3.9/lib/python/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n\n  File \"/Users/houhiroshisakai/Library/Python/3.9/lib/python/site-packages/ipykernel/zmqshell.py\", line 540, in run_cell\n\n  File \"/Users/houhiroshisakai/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3009, in run_cell\n\n  File \"/Users/houhiroshisakai/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3064, in _run_cell\n\n  File \"/Users/houhiroshisakai/Library/Python/3.9/lib/python/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"/Users/houhiroshisakai/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3269, in run_cell_async\n\n  File \"/Users/houhiroshisakai/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3448, in run_ast_nodes\n\n  File \"/Users/houhiroshisakai/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n\n  File \"/var/folders/n0/ybgqz5zd2cx4wbm8cv8vr3gr0000gn/T/ipykernel_33798/531325463.py\", line 5, in <module>\n\n  File \"/Users/houhiroshisakai/Library/Python/3.9/lib/python/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/Users/houhiroshisakai/Library/Python/3.9/lib/python/site-packages/keras/src/engine/training.py\", line 1783, in fit\n\n  File \"/Users/houhiroshisakai/Library/Python/3.9/lib/python/site-packages/keras/src/engine/training.py\", line 1377, in train_function\n\n  File \"/Users/houhiroshisakai/Library/Python/3.9/lib/python/site-packages/keras/src/engine/training.py\", line 1360, in step_function\n\n  File \"/Users/houhiroshisakai/Library/Python/3.9/lib/python/site-packages/keras/src/engine/training.py\", line 1349, in run_step\n\n  File \"/Users/houhiroshisakai/Library/Python/3.9/lib/python/site-packages/keras/src/engine/training.py\", line 1127, in train_step\n\n  File \"/Users/houhiroshisakai/Library/Python/3.9/lib/python/site-packages/keras/src/engine/training.py\", line 1185, in compute_loss\n\n  File \"/Users/houhiroshisakai/Library/Python/3.9/lib/python/site-packages/keras/src/engine/compile_utils.py\", line 277, in __call__\n\n  File \"/Users/houhiroshisakai/Library/Python/3.9/lib/python/site-packages/keras/src/losses.py\", line 143, in __call__\n\n  File \"/Users/houhiroshisakai/Library/Python/3.9/lib/python/site-packages/keras/src/losses.py\", line 270, in call\n\n  File \"/Users/houhiroshisakai/Library/Python/3.9/lib/python/site-packages/keras/src/losses.py\", line 2454, in sparse_categorical_crossentropy\n\n  File \"/Users/houhiroshisakai/Library/Python/3.9/lib/python/site-packages/keras/src/backend.py\", line 5777, in sparse_categorical_crossentropy\n\nlogits and labels must have the same first dimension, got logits shape [394272,120] and labels shape [32]\n\t [[{{node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]] [Op:__inference_train_function_12190]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m/HaoyuHunterFang/schulich_data_science/a1.ipynb Cell 12\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell://github/HaoyuHunterFang/schulich_data_science/a1.ipynb#X13sdnNjb2RlLXZmcw%3D%3D?line=1'>2</a>\u001b[0m early_stopping \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mcallbacks\u001b[39m.\u001b[39mEarlyStopping(patience\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, monitor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m'\u001b[39m, restore_best_weights\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell://github/HaoyuHunterFang/schulich_data_science/a1.ipynb#X13sdnNjb2RlLXZmcw%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://github/HaoyuHunterFang/schulich_data_science/a1.ipynb#X13sdnNjb2RlLXZmcw%3D%3D?line=4'>5</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m      <a href='vscode-notebook-cell://github/HaoyuHunterFang/schulich_data_science/a1.ipynb#X13sdnNjb2RlLXZmcw%3D%3D?line=5'>6</a>\u001b[0m     train_batches,\n\u001b[1;32m      <a href='vscode-notebook-cell://github/HaoyuHunterFang/schulich_data_science/a1.ipynb#X13sdnNjb2RlLXZmcw%3D%3D?line=6'>7</a>\u001b[0m     validation_data\u001b[39m=\u001b[39;49mvalidation_batches,\n\u001b[1;32m      <a href='vscode-notebook-cell://github/HaoyuHunterFang/schulich_data_science/a1.ipynb#X13sdnNjb2RlLXZmcw%3D%3D?line=7'>8</a>\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m,  \u001b[39m# Increase if necessary\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://github/HaoyuHunterFang/schulich_data_science/a1.ipynb#X13sdnNjb2RlLXZmcw%3D%3D?line=8'>9</a>\u001b[0m     callbacks\u001b[39m=\u001b[39;49m[early_stopping]\n\u001b[1;32m     <a href='vscode-notebook-cell://github/HaoyuHunterFang/schulich_data_science/a1.ipynb#X13sdnNjb2RlLXZmcw%3D%3D?line=9'>10</a>\u001b[0m )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/eager/execute.py:60\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m   \u001b[39m# Convert any objects of type core_types.Tensor to Tensor.\u001b[39;00m\n\u001b[1;32m     54\u001b[0m   inputs \u001b[39m=\u001b[39m [\n\u001b[1;32m     55\u001b[0m       tensor_conversion_registry\u001b[39m.\u001b[39mconvert(t)\n\u001b[1;32m     56\u001b[0m       \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(t, core_types\u001b[39m.\u001b[39mTensor)\n\u001b[1;32m     57\u001b[0m       \u001b[39melse\u001b[39;00m t\n\u001b[1;32m     58\u001b[0m       \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m inputs\n\u001b[1;32m     59\u001b[0m   ]\n\u001b[0;32m---> 60\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     61\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     62\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     63\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits defined at (most recent call last):\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/runpy.py\", line 87, in _run_code\n\n  File \"/Users/houhiroshisakai/Library/Python/3.9/lib/python/site-packages/ipykernel_launcher.py\", line 17, in <module>\n\n  File \"/Users/houhiroshisakai/Library/Python/3.9/lib/python/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n\n  File \"/Users/houhiroshisakai/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelapp.py\", line 725, in start\n\n  File \"/Users/houhiroshisakai/Library/Python/3.9/lib/python/site-packages/tornado/platform/asyncio.py\", line 195, in start\n\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py\", line 596, in run_forever\n\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py\", line 1890, in _run_once\n\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/events.py\", line 80, in _run\n\n  File \"/Users/houhiroshisakai/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelbase.py\", line 513, in dispatch_queue\n\n  File \"/Users/houhiroshisakai/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelbase.py\", line 502, in process_one\n\n  File \"/Users/houhiroshisakai/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelbase.py\", line 409, in dispatch_shell\n\n  File \"/Users/houhiroshisakai/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelbase.py\", line 729, in execute_request\n\n  File \"/Users/houhiroshisakai/Library/Python/3.9/lib/python/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n\n  File \"/Users/houhiroshisakai/Library/Python/3.9/lib/python/site-packages/ipykernel/zmqshell.py\", line 540, in run_cell\n\n  File \"/Users/houhiroshisakai/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3009, in run_cell\n\n  File \"/Users/houhiroshisakai/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3064, in _run_cell\n\n  File \"/Users/houhiroshisakai/Library/Python/3.9/lib/python/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"/Users/houhiroshisakai/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3269, in run_cell_async\n\n  File \"/Users/houhiroshisakai/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3448, in run_ast_nodes\n\n  File \"/Users/houhiroshisakai/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n\n  File \"/var/folders/n0/ybgqz5zd2cx4wbm8cv8vr3gr0000gn/T/ipykernel_33798/531325463.py\", line 5, in <module>\n\n  File \"/Users/houhiroshisakai/Library/Python/3.9/lib/python/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/Users/houhiroshisakai/Library/Python/3.9/lib/python/site-packages/keras/src/engine/training.py\", line 1783, in fit\n\n  File \"/Users/houhiroshisakai/Library/Python/3.9/lib/python/site-packages/keras/src/engine/training.py\", line 1377, in train_function\n\n  File \"/Users/houhiroshisakai/Library/Python/3.9/lib/python/site-packages/keras/src/engine/training.py\", line 1360, in step_function\n\n  File \"/Users/houhiroshisakai/Library/Python/3.9/lib/python/site-packages/keras/src/engine/training.py\", line 1349, in run_step\n\n  File \"/Users/houhiroshisakai/Library/Python/3.9/lib/python/site-packages/keras/src/engine/training.py\", line 1127, in train_step\n\n  File \"/Users/houhiroshisakai/Library/Python/3.9/lib/python/site-packages/keras/src/engine/training.py\", line 1185, in compute_loss\n\n  File \"/Users/houhiroshisakai/Library/Python/3.9/lib/python/site-packages/keras/src/engine/compile_utils.py\", line 277, in __call__\n\n  File \"/Users/houhiroshisakai/Library/Python/3.9/lib/python/site-packages/keras/src/losses.py\", line 143, in __call__\n\n  File \"/Users/houhiroshisakai/Library/Python/3.9/lib/python/site-packages/keras/src/losses.py\", line 270, in call\n\n  File \"/Users/houhiroshisakai/Library/Python/3.9/lib/python/site-packages/keras/src/losses.py\", line 2454, in sparse_categorical_crossentropy\n\n  File \"/Users/houhiroshisakai/Library/Python/3.9/lib/python/site-packages/keras/src/backend.py\", line 5777, in sparse_categorical_crossentropy\n\nlogits and labels must have the same first dimension, got logits shape [394272,120] and labels shape [32]\n\t [[{{node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]] [Op:__inference_train_function_12190]"
     ]
    }
   ],
   "source": [
    "# EarlyStopping callback to prevent overfitting\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=5, monitor='val_loss', restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_batches,\n",
    "    validation_data=validation_batches,\n",
    "    epochs=50,  # Increase if necessary\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
